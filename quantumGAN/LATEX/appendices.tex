\chapter{Més àlgebra lineal}
Ja he escrit bastants pàgines sobre àlgebra lineal, però aparentment no eren les suficients perquè estic content amb el treball\footnote{Hi ha moltes coses guais i interessants que vull explicar.}, així que aquí hi ha més àlgebra lineal.


\section{Procediment de Gram–Schmidt}\label{gram}
El procediment de Gram-Schmidt és un mètode utilitzat per produir bases per a espais vectorials\cite{QCandQI:GramSchmidt}. Per un espai $V$ amb producte interior de $d$ dimensions amb el set de vectors ${\ket{v_1}, \cdots, \ket{v_d}}$, podem definir una nova base de vectors ortonormals $\{\ket{u}\}$. El primer element d'aquest set és $\ket{u_1} = \ket{v_1}/\norm{\ket{v_1}}$, amb el següent element $\ket{v_{k+1}}$ sent:
$$
\ket{u_{k+1}} = \frac{\ket{v_{k+1}} - \sum_{i=1}^{k} \bra{u_i}\ket{v_{k+1}}\ket{u_i}}{\norm{\ket{v_{k+1}} - \sum_{i=1}^{k} \bra{u_i}\ket{v_{k+1}}\ket{u_i}}}
$$
Per $k$ en el interval $1 \leq k \leq d-1$.

Si seguim per $k$ en $1 \leq k \leq d-1$, obtenim el set de vectors ${\ket{u_1}, \cdots , \ket{u_d}}$ que és una base vàlida per l'espai ortonormal per l'estai $V$. Els vectors creats han de tindre el mateix \textit{span}\footnote{L'\textit{span} d'un set de vectors són totes les combinacions lineals possibles amb aquests vectors.} que el dels vectors que originalment eren la base per $V$:
$$
\text{span}(\{\ket{v}\}) = \text{span}(\{\ket{v}\}) = V
$$
Cal notar que l'\textit{span} de del set base és la definició del espai. En altres paraules, cada vector en $V$ pot ser representat per una combinació del vectors base. 

La prova de que és una base ortonormal és bastant simple:
Podem veure immediatament que els elements de $\{ \ket{u} \}$ són vectors unitaris perquè estan normalitzats (els vectors $\ket{v_{k+1}} - \sum_{i=1}^{k} \bra{u_i}\ket{v_{k+1}}\ket{u_i}$ estan dividits per la seva norma). També podem veure que són ortogonals els uns als altres mirant que el producte interior entre els dona 0:

Per $k=1$:
$$
\ket{u_2} = \frac{\ket{v_2} - \bra{u_1}\ket{v_2}\ket{u_1}}{\norm{\ket{v_2} - \bra{u_1}\ket{v_2}\ket{u_1}}}
$$
Per tant, el producte interior amb $\ket{v_1}$ és:
$$
\begin{aligned}
\bra{u_1}\ket{u_2} 
&= \bra{u_1} \left( \frac{\ket{v_2} - \bra{u_1}\ket{v_2}\ket{u_1}}{\norm{\ket{v_2} - \bra{u_1}\ket{v_2}\ket{u_1}}} \right) \\
&= \frac{\bra{u_1}\ket{v_2} - \bra{u_1}\ket{v_2}\bra{u_1}\ket{u_1}}{\norm{\ket{v_2} - \bra{u_1}\ket{v_2}\ket{u_1}}} \\
&= 0
\end{aligned}
$$
Per inducció podem veure que per $j \leq d$, amb $d$ sent la dimensió de l'espai vectorial:
$$
\begin{aligned}
	\bra{u_j}\ket{u_{n+1}} 
	& = \bra{u_j} \left( \frac{\ket{v_{n+1}} - \sum_{i=1}^{n} \bra{u_i}\ket{v_{n+1}}\ket{u_i}}{\norm{\ket{v_{n+1}} - \sum_{i=1}^{n} \bra{u_i}\ket{v_{n+1}}\ket{u_i}}} \right) \\
	& = \frac{\bra{u_j}\ket{v_{n+1}} - \sum_{i=1}^{n} \bra{u_i}\ket{v_{n+1}} \bra{u_j}\ket{u_i}}{\norm{\ket{v_{n+1}} - \sum_{i=1}^{n} \bra{u_i}\ket{v_{n+1}} \ket{u_i}}} \\
	& = \frac{\bra{u_j}\ket{v_{n+1}} - \sum_{i=1}^{n} \bra{u_i}\ket{v_{n+1}} \delta_{ij}}{\norm{\ket{v_{n+1}} - \sum_{i=1}^{n} \bra{u_i}\ket{v_{n+1}} \ket{u_i}}} \\
	& = \frac{\bra{u_j}\ket{v_{n+1}} - \bra{u_j}\ket{v_{n+1}}}{\norm{\ket{v_{n+1}} - \sum_{i=1}^{n} \bra{u_i}\ket{v_{n+1}} \ket{u_i}}} \\
	& = 0	\\
\end{aligned}
$$
Tot això no es veu molt clar al principi, però cal recordar que el producte interior de dos vectors ortonormals és zero, i que el producte interior entre el mateix vector unitari és un.

\section{Curs ràpid de la notació de Dirac}
A la taula següent hi ha un resum de conceptes matemàtics de l'àlgebra lineal importants expressats en la notació de Dirac\footnote{La notació utilitzada per un espai vectorial complex i l'espai dels nombres complex no són de la notació de Dirac estàndard, però les poso per explicar el que signifiquen.} \cite{QCandQI:dirac_notation}

\begin{tabular}{ p{2cm}|p{12cm} }
	\hline
	Notació & Descripció\\
	\hline
	\hline
	$z$ & Nombre complex \\
	$z^{*}$ & Conjugat complex d'un nombre complex $z$. $(a+ bi)^{*} = (a -bi)$\\
	$\ket{\psi} $ & Vector amb una etiqueta $\psi$. Conegut com a \textit{ket}\\
	$\ket{\psi}^T$ & Transposada d'un vector $\ket{\psi}$ \\
	$\ket{\psi}^\dag $ & Conjugat Hermitià d'un vector. $\ket{\psi}^\dag = (\ket{\psi}^T)^* $\\
	$\bra{\psi} $ & Vector dual a $\ket{\psi}$. $ \ket{\psi} = \bra{\psi}^{\dag}$ i $\bra{\psi} = \ket{\psi}^\dag$. Conegut com a \textit{bra}\\
	$ \bra{\varphi}\ket{\psi} $ & Producte interior dels vectors $\bra{\varphi}$ i $\ket{\psi}$ \\
	$ \ket{\varphi}\bra{\psi} $ & Producte exterior dels vectors $\bra{\varphi}$ i $\ket{\psi}$ \\
	$ \ket{\psi}\otimes\ket{\varphi}$ & Producte tensorial dels vectors $\ket{\varphi}$ i $\ket{\psi}$ \\
	$ 0 $ & Vector zero i operador zero \\
	$ \mathbb{I}_n $ & Matriu identitat de dimensions $n\times n$ \\
	$ \mathbb{C}_n $ & Espai vectorial complex de dimensió $n$ \\
	$ \mathbb{C}_1$ o $\mathbb{C} $ & Espai dels nombres complexos \\
	
\end{tabular}




\chapter{Computació Quàntica vs Mecànica Quàntica}
En la introducció havia mencionat que una de les raons per les quals havia començat a aprendre i recercar sobre computació quàntica era perquè era fàcil, en aquest apèndix explicaré exactament a què em refereixo per això amb un exemple pràctic. Dic que és fàcil, perquè si ho és, si es compara la computació quàntica amb la mecànica quàntica. Presentaré un exemple pràctic per il·lustrar-lo.

En mecànica quàntica la manera més usual de representar els estats quàntics, com els orbitals d'un àtom d'hidrogen, és a través de \textit{wavefunctions} o funcions d'ona, no es solen representar mitjançant vectors d'estat. Aquestes funcions d'ona són molt útils i poden representar casos més generals que els vectors d'estat. No obstant això, treballar amb elles és molt més complicat, ja que són funcions que depenen del temps, no són vectors. Això implica que s'han d'utilitzar altres operacions per normalitzar les funcions, determinar com evolucionen en el temps o per fer mesures. A continuació faré una comparació entre com es preserva la normalització d'una funció d'ona i la d'un vector d'estat quan aquests objectes evolucionen en el temps, per mostrar un exemple de com la mecànica quàntica més bàsica em sembla més complicada que la computació quàntica més bàsica.

\section{Normalitzar}
A causa de la interpretació probabilística dels vectors d'estat i de les funcions d'ona, aquests objectes han de ser normalitzats perquè la suma de les probabilitats dels possibles estats de mesura sigui $1$.

Per una funció d'ona $\Psi(x,t)$ que representa una partícula, la probabilitat de trobar aquesta partícula en un punt $x$ és $\abs{\Psi(x,t)}^2$. Per tant, la funció d'ona ha de ser normalitza seguint la fórmula:
\begin{equation}
\int_{-\infty}^{+\infty}\abs{\Psi(x,t)}^2\, dx\ = 1
\label{eq:norm_wave}
\end{equation}

Ja que la funció d'ona evoluciona a través del temps d'acord amb l'equació de Schrödinger, veure la figura \ref{fig:schro}, qualsevol solució d'aquesta equació ha d'estar també normalitzada. En altres paraules, aquesta equació ha de preservar la normalització de les funcions d'ona \cite{IntroQM:normalizing}.
\begin{figure}[H]
	$$
	i\hslash \pdv{\Psi}{t} = -\frac{\hslash^2}{2m}\pdv[2]{\Psi}{x}+V\Psi
	$$
	\caption{\textbf{Equació de Schrödinger}. On $\hslash$ és $h/2\pi$, i $V$ és una funció potencial d'energia.}
	\label{fig:schro}
\end{figure}

Podem provar que aquesta equació preserva la fórmula \ref{eq:norm_wave}, començant per la igualtat trivial:
$$
\dv{t} \int_{-\infty}^{+\infty}\abs{\Psi(x,t)}^2\, dx\ = \pdv{t} \int_{-\infty}^{+\infty}\abs{\Psi(x,t)}^2\, dx
$$
Per la regla del producte tenim que \footnote{A partir d'ara escriuré  $\Psi(x,t)$ simplement com $\Psi$ per no fer les equacions tan enrevessades.}:
$$
\pdv{t}\abs{\Psi}^2 = \pdv{t}(\Psi\Psi^*) = \Psi^*\pdv{\Psi}{t} + \pdv{\Psi^*}{t}\Psi
$$
Ara l'equació de Schrödinger diu que
$$
\pdv{\Psi}{t} = \frac{i\hslash}{2m}\pdv[2]{\Psi}{x} -  \frac{i}{\hslash}V\Psi
$$
després calculant el complex conjugat tenim que 
$$
\pdv{\Psi^*}{t} = - \frac{i\hslash}{2m}\pdv[2]{\Psi^*}{x} + \frac{i}{\hslash}V\Psi^*
$$
per tant
$$
\pdv{t}\abs{\Psi}^2 = \frac{i\hslash}{2m} \left( \Psi^*\pdv[2]{\Psi}{x} - \pdv[2]{\Psi^*}{x}\Psi \right) = \pdv{x} \left[ \frac{i\hslash}{2m} \left(\Psi^*\pdv{\Psi}{x} - \pdv{\Psi^*}{x}\Psi\right)\right]
$$
finalment podem avaluar l'integral del principi:
$$
\dv{t} \int_{-\infty}^{+\infty}\abs{\Psi(x,t)}^2\, dx\ = \frac{i\hslash}{2m} \left( \Psi^*\pdv{\Psi}{x} - \pdv{\Psi^*}{x}\Psi \right) \Big|_{-\infty}^{+\infty}
$$
Degut a que $\Psi(x,t)$ ha de convergir a zero quan $x$ va cap a infinit, es veritat que:
$$
\dv{t} \int_{-\infty}^{+\infty}\abs{\Psi(x,t)}^2\, dx\ = 0
$$
Es pot veure que l'integral és constant i, per tant, quan $\Psi$ és normalitzada a $t=0$, es queda d'aquesta manera per qualsevol $t$ (positiu és clar), i la normalització d'una funció d'ona és constant si aquesta evoluciona d'acord amb l'equació de Schrödinger.

Mentre que amb un vector d'estat és molt més simple: al evolucionar a partir de matrius unitàries les quals preserven la norma del vector a la qual s'apliquen; si el vector al qual s'aplica una porta quàntica té una norma de $1$, aquesta norma es preservarà.

\chapter{Polarització d'un fotó}
\label{appendix:optics}
En l'equació \ref{eq:photon_state} he exclòs el concepte de fase, que determina el tipus de polarització que té un fotó. Hi ha 3 tipus:
\begin{enumerate}
	\item \textbf{Linear:}
	Un fotó té polarització lineal quan els angles de la fase $\alpha_x$, $\alpha_y$ en els estats base $\ket{x},\ket{y}$ són iguals:
	\begin{align*}
		\ket{\nearrow} &= \cos(\theta) e^{i\alpha_x}\ket{x} + \cos(\theta)e^{i\alpha_y}\ket{y} \\
		&= [\cos(\theta)\ket{x} + \sin(\theta)\ket{y}]e^{i\alpha}
	\end{align*}
	On $\alpha=\alpha_x=\alpha_y$.
	\item \textbf{Circular:}
	Quan els angles $\alpha_x$, $\alpha_y$ són separats per exactament $\frac{\pi}{2}$ i l'amplitud per les dues bases és la mateixa:
	\begin{align*}
		\ket{\nearrow} &= \frac{1}{\sqrt{2}}\cos(\theta)e^{i\alpha_x}\ket{x} \pm i\frac{1}{\sqrt{2}}\sin(\theta)e^{i\alpha_y}\ket{y} \\
					   &= [\cos(\theta)e^{i\alpha_x}\ket{x} \pm i\sin(\theta)e^{i\alpha_y}\ket{y}]\frac{1}{\sqrt{2}}
	\end{align*}
	On el signe $\pm$ indica la diferència entre la polarització circular cap a la dreta o l'esquerra, amb $+$ i $-$, respectivament.
	\item \textbf{El·líptica:}
	On els angles $\alpha_x$, $\alpha_y$ són diferents per una quantitat arbitraria\footnote{Però que no sigui la quantitat que dona a terme la polarització circular.}:
	$$
	\ket{\nearrow} = \cos(\theta)e^{i\alpha_x}\ket{x} + \sin(\theta)e^{i\alpha_y}\ket{y}
	$$
	Aquest és el cas més general.
	
\end{enumerate}

\chapter{Complexitat i algoritmes quàntics}
\label{complexity}
En ciències de la computació existeix el concepte de \textit{Big-O Notation}, una forma de quantificar l'eficiència dels algoritmes, la qual s'anomena la complexitat algorítmica. Bàsicament, és una forma de classificar-los segon la rapidesa que tenen en fer la tasca que els correspon, aquesta rapidesa no és mesura en segons, ja que aquesta mètrica pot variar d'ordinador a ordinador per les diferències en hardware que aquests poden tindre. En canvi, es mesura segons nombre d'operacions o temps directament, però sense unitats.

La \textit{Big-O Notation} consisteix a definir el temps màxim que necessita un algoritme, es denota com $O(\cdot)$ on l'argument usualment depèn de $n$ que és la mida de l'input a l'algoritme, per exemple un algoritme de cerca ha de cercar a través de $n$ coses. Com a un exemple més concret tenim que un l'algoritme de cerca de cadenes binàries corre en un temps $O(\log_{2} n)$, on $n$ és el nombre de cadenes entre les quals ha de cercar. Recorda que la notació $O(\cdot)$ és el màxim, és a dir, és \textit{upper bounded}, això significa que $\log_{2} n$ és la quantitat de temps més gran en la que es troba la cadena, també és possible que es trobi a la primera comprovació que es va\footnote{Que la primera cadena que se cerca, és la que s'ha de trobar.}, llavors l'algoritme acabaria en un temps $O(1)$. Simplement, és una manera de mirar lo eficients que són els algoritmes amb relació a la mida de l'input que tenen.

Amb aquesta notació tenim una manera de comparar l'eficiència que tenen els algoritmes quàntics amb la dels clàssics que tenen la mateixa funció. Per il·lustrar aquest avantatge en eficiència que presenten els algoritmes quàntics, presentaré a continuació els dos algoritmes quàntics més famosos, el de Grover, i el de Shor. Que serveixen per a la cerca d'un element en una llista desordenada, i per la factorització en nombres primers respectivament.

\section{Algoritme de Grover}
En 1996, Lov Grover va presentar un algoritme quàntic per cercar en dades desordenades \cite{Grover_96} (e.g. cercar el número de telèfon d'una persona en una llista desordenada). Per aquest problema un algoritme clàssic té una complexitat de $O(N)$ cerques\footnote{Una cerca és quan es verifica si un element de la llista és l'element que se cerca.}, mentre que l'algoritme de Grover té una complexitat de $O(\sqrt{N})$, sent substancialment més eficient. En les paraules de Grover \cite{Grover_96} (adaptades), un ordinador clàssic per tindre una probabilitat de $\frac{1}{2}$ de trobar el número de telèfon d'una persona en una llista desordenada necessita mirar a un mínim de $\frac{N}{2}$ números, mentre que amb el seu s'obté el número de telèfon en només $O(\sqrt{N})$ passos\footnote{Per passos entenc que es refereix al nombre de vegades que es mira a l'oracle, és a dir, el nombre que de vegades que es verifica si s'ha trobat l'element que se cerca.}.

L'algoritme funciona de la següent manera:
Agafar un sistema de $n$ qubits en l'estat $\ket{0}$ que resulten en una combinació de $N = 2^n$ estats. Aplicar una distribució uniforme als qubits, és a dir, aplicar portes Hadamard a tots els qubits, tenint al final un estat resultant $\ket{s}$:
\begin{equation*}
	\ket{s} = \frac{1}{\sqrt{N}} \sum_{x=0}^{2^n-1} \ket{x}
\end{equation*}
Repetir $\approx \frac{\pi}{4}\sqrt{N}$ vegades, el operador de Grover $G$, que consisteix en el següent conjunt d'instruccions:
\begin{algorithmic}[1]
	\State{Aplicar el \textit{Oracle} $U_{\omega}$}
	\State{Aplicar portes Hadamard a tots els qubits}
	\State{Aplicar el \textit{Grover diffusion operator} $U_{s} = 2\ket{s}\bra{s} - I$}
	\State{Aplicar una altre vegada les portes Hadamard}
\end{algorithmic}


El \textit{oracle} és un tipus de funció que s'utilitza en els algoritmes de cerca, té la finalitat de reconèixer si un element és l'element que s'està cercant. En els algoritmes de cerca es construeixen al voltant de l'\textit{oracle}, diversos algoritmes tenen diverses formes de consultar a l'oracle. A més a més, la quantitat de consultes a l'\textit{oracle} serveixen per quantificar la complexitat de l'algoritme, a més consultes, més ineficient és l'algoritme.

Al veure el procediment de l'algoritme es pot veure que l\textit{oracle} es consulta aproximadament $\frac{\pi}{4}\sqrt{N}$, vegades, d'aquesta manera resultat en una complexitat aproximada de $O(\sqrt{N})$, on $N=2^{n}$ per $n$ qubits, com ja he esmentat anteriorment.

No entraré més en profunditat sobre aquest algoritme, perquè si no hauré d'introduir més conceptes de computació quàntica com ara la fase d'un estat, o començar a utilitzar notació més complicada que també hauré d'explicar. Em sap greu, perquè és un algoritme que funciona d'una manera bonica de veure. També em sembla el millor algoritme per poder explicar una de les millors característiques de la computació quàntica, tenir algoritmes que et descarten automàticament els estats dolents dintre d'una combinació d'estats, d'aquesta manera lliurant un estat concret desitjat d'entre la combinació. En el cas de l'algoritme de Grover aquest estat desitjat, és l'estat que està cercant l'\textit{oracle}.

\section{Algoritme de Shor}
No explicaré aquest algoritme en profunditat perquè és molt complex\footnote{Hauria d'introduir nous conceptes com la fase d'un qubit i com efectuar la transformada de Fourier inversa en un circuit quàntic.}, no surt ni en els llibres de texts als quals tinc accés. Simplement, em limitaré a explicar quina és la seva utilitat i el compararé amb les seves alternatives clàssiques.

L'algoritme de Shor per la factorització en nombres primers \cite{Shor_97}, és probablement l'algoritme quàntic més famós i més rellevant que hi ha, degut a la gran diferencia en complexitat que té en comparació als seus algoritmes anàlegs clàssics, i a causa del gran impacte que tindria la seva implementació a gran escala. Aquest últim punt és el més interessant i cridaner, per tant, és el que desenvoluparé en major profunditat.

Actualment, totes les dades encriptades que s'envien a través d'internet són encriptades mitjançant la família de protocols RSA\footnote{RSA són les sigles dels creadors d'aquests protocols; Ron Rivest, Adi Shamir i Leonard Adleman, que en 1977 van publicar els resultats de la seva investigació \cite{RSA}.}. Aquests protocols són lleugerament complicats, no els explicaré, l'únic fet que hem de tenir en compte és que per desencriptar un missatge s'ha de trobar els factors prims d'un nombre $n$, és a dir, dos nombres primers $p$ i $q$ que multiplicats donin $n$. Amb aquests nombres es poden saber les claus públiques i privades per encriptar i desencriptar missatges. $n$ seria la clau pública, un nombre que tothom pot conèixer i amb el qual es poden encriptar missatges. Mentre que a partir de $p$ i $q$ es pot trobar la clau privada que correspon a la clau pública, amb la qual es pot desencriptar un missatge encriptar amb $n$, per aquesta raó es mantenen els nombres $p$ i $q$ secrets. Es pot entendre la clau pública com un cadenat que només s'obre amb la clau privada. Cal mencionar que a cada clau privada correspon una clau pública, són paralles úniques.

En encriptació es busca el que diu \textit{one way operations}, és a dir, una operació que si és fàcil de fer, però que la seva inversa requereixi molts recursos computacionals per donar-la a terme. L'operació $n = pq$ compleix aquest requisit, és fàcil multiplicar els nombres primers $p$ i $q$ i trobar $n$, però és molt difícil trobar els nombres $p$ i $q$ sabent $n$, ja que s'ha de realitzar una factorització de $n$ en nombres primers. Els millors algoritmes per donar a terme aquesta tasca requereixen moltíssim temps computacional en els ordinadors clàssics. Per aquesta raó el protocol RSA s'utilitza àmpliament, és molt segur perquè per saber la clau privada a partir de $n$, es requereix molt de temps. Amb $n$ sent un nombre de 2048 bits, amb la tecnologia actual no és possible trobar els nombres primers $p$ i $q$ en un temps raonable. Dic raonable perquè es pot arribar a trobar, però es tardaria milers d'anys inclús amb els ordinadors més potents del planeta.

Ara és quan l'algoritme de Shor juga la seva part, com ja he dit s'utilitza per a la factorització de nombres primers, i a diferència dels seus anàlegs clàssics, és extremadament eficient en comparació. En termes de complexitat, aquest algoritme pot factoritzar un nombre de $N$ dígit en un temps polinomi de $O(\log N)$ \footnote{Concretament la seva complexitat mesurada mitjançant la quantitat de portes quàntiques seria de $O((\log n)^2 (\log \log n) (\log \log \log n))$, per factoritzar un nombre $n$ amb una longitud de $\log n$ bits.}. Mentre que el millor algoritme clàssic per factoritzar en nombres primers té una complexitat de $O(e^{c(\log N)^\frac{1}{3} (\log \log N)^\frac{2}{3}})$ per una constant $c$. És a dir, que clàssicament es pot factoritzar en un temps exponencial \cite{Shor_97}, o com a mínim en un temps sub-exponencial. Un algoritme que s'executa en temps sub-exponencial segueix sent més ineficient que un que s'executa en temps polinomi, però significativament més eficient que un que s'executa en temps exponencial. De totes maneres només cal tenir en compte que l'algoritme de Shor és molt més eficient que el millor algoritme clàssic que dona a terme la mateixa tasca.

S'estima que amb un ordinador de quàntic prou avançat es podrien arribar a factoritzar nombres els quals tenen una mida que actualment utilitzem\footnote{D'uns 2048 dígits per exemple.}, però amb la tecnologia actual aquesta fita és impossible. El record pel nombre més gran factoritzat per un ordinador quàntic en l'actualitat el posseeix un equip de la Universitat de Bristol al Regne Unit. En 2021 van aconseguir factoritzar 21 amb un ordinador quàntic que funciona a partir d'un sistema fotònic \cite{21_factoritzation}.

\section{Ordinadors quàntics actuals}
El només ser capaços de poder factoritzar 21 il·lustra lo nova que és la tecnologia que està al darrere dels ordinadors quàntics. És un camp que té pocs anys de vida, i en el qual és difícil fer grans avanços. Al moment només es pot utilitzar el millor algoritme que ofereix la computació quàntica per poder fer una tasca extremadament simple\footnote{$3 \times 7 = 21$}.

Això és a causa de la quantitat de qubits dels quals disposen els ordinadors quàntics actuals i als errors que aquests donen a terme a l'hora d'aplicar operacions. Quan un ordinador quàntic aplica una rotació en l'estat d'un qubit físic, aquest ho fa amb una certa imprecisió. El mateix passa a l'hora d'aplicar portes a dos qubits, com per exemple una $\mathrm{CNOT}$. Els ordinadors quàntics actuals només són utilitzats per fer experiments, com provar nous algoritmes i valorar els mateixos ordinadors. Pràcticament, no existeixen situacions pràctiques en les quals els ordinadors quàntics actuals siguin més útils que els clàssics. Tanmateix, és una tecnologia molt prometedora, en la qual grans companyies com Google, IBM, Amazon estan invertint per investigar-la, igual que altres companyies més petites i nombrosos equips d'investigació al llarg del món. S'espera que un futur els ordinadors quàntics ens permetran dur a terme tasques que són impossibles de realitzar en ordinadors clàssics, sobretot en tenir en compte que s'espera que començarem a arribar a limitacions físiques si continuem millorant els processadors d'ordinadors actuals.


\chapter{Codi}
En l'apèndix actual presentaré el codi que he utilitzat al llarg del treball. Està organitzat segons el moment en el qual he referenciat el codi en el text. 

IMPORTANT: Si hi ha text en català dintre del codi (n'hi ha en forma de comentaris), no porta accents perquè els paquets que utilitzo per formatejar el codi d'aquesta manera dintre del document no els accepta. Però si es mira el codi en el \href{https://github.com/tomiock/qGAN}{repositori del treball} es pot veure que sí que té accents. No em deixa posar ni accents, ni la ce trencada, ni l'ele geminada. L'error diu que no són caràcters que estan en Unicode UTF-8, quan sí que estan allà, per fet que és un codi que suposadament està internacionalitzat per tots els llenguatges que es fan servir.

\section{Part I}
\subsection{Capítol 3}

\paragraph{Regressió lienal}
\label{lst:linear_regression}
Codi per efectuar una regressió lineal a dades que es generen a l'atzar en el mateix arxiu, l'utilitzo per poder generar un gràfic per il·lustrar un exemple de regressió lienal. Aquest tros de codi l'he tret de GitHub\footnote{Gist realitzat per l'usuari \textit{jimimvp}: \href{https://gist.github.com/jimimvp/05ece11fec25d5c8c009af9ba469d6c2}{enllaç}}.

\begin{lstlisting}[language=Python, caption=Regressió lineal]
	import numpy as np
	from matplotlib import pyplot as plt
	import matplotlib
	
	font = {'family' : 'Helvetica',
		'size'   : 18}
	
	matplotlib.rc('font', **font)
	
	# generate the data
	np.random.seed(222)
	X = np.random.normal(0,1, (200, 1))
	w_target = np.random.normal(0,1, (1,1))
	# data + white noise
	y = X@w_target + np.random.normal(0, 1, (200,1))
	
	# least squares
	w_estimate = np.linalg.inv(X.T@X)@X.T@y
	y_estimate = X@w_estimate
	
	# plot the data
	plt.figure(figsize=(15,10))
	plt.scatter(X.flat, y_estimate.flat, label="Prediccio")
	plt.scatter(X.flat, y.flat, color='red', alpha=0.4, label="Dades")
	plt.tight_layout()
	plt.title("Regressio per diferencia de quadrats")
	plt.legend()
	plt.savefig("least_squares.png")
	plt.show()
\end{lstlisting}

\section{Part II}
\subsection{Capítol 6}

\paragraph{Codi original per la xarxa neuronal clàssica}
\label{lst:disc_original}
Està extret del \href{https://github.com/mnielsen/neural-networks-and-deep-learning}{repositori de GitHub de Micheal Nielsen}, concretament del arxiu \code{network.py}.


\begin{lstlisting}[language=Python, caption=Codi original per la xarxa neuronal clàssica]
"""
network.py
~~~~~~~~~~
A module to implement the stochastic gradient descent learning
algorithm for a feedforward neural network.  Gradients are calculated
using backpropagation.  Note that I have focused on making the code
simple, easily readable, and easily modifiable.  It is not optimized,
and omits many desirable features.
"""

#### Libraries
# Standard library
import random

# Third-party libraries
import numpy as np

class Network(object):

	def __init__(self, sizes):
		"""The list ``sizes`` contains the number of neurons in the
		respective layers of the network.  For example, if the list
		was [2, 3, 1] then it would be a three-layer network, with the
		first layer containing 2 neurons, the second layer 3 neurons,
		and the third layer 1 neuron.  The biases and weights for the
		network are initialized randomly, using a Gaussian
		distribution with mean 0, and variance 1.  Note that the first
		layer is assumed to be an input layer, and by convention we
		won't set any biases for those neurons, since biases are only
		ever used in computing the outputs from later layers."""
		self.num_layers = len(sizes)
		self.sizes = sizes
		self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
		self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]

	def feedforward(self, a):
		"""Return the output of the network if ``a`` is input."""
		for b, w in zip(self.biases, self.weights):
			a = sigmoid(np.dot(w, a)+b)
		return a

	def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):
		"""Train the neural network using mini-batch stochastic
		gradient descent.  The ``training_data`` is a list of tuples
		``(x, y)`` representing the training inputs and the desired
		outputs.  The other non-optional parameters are
		self-explanatory.  If ``test_data`` is provided then the
		network will be evaluated against the test data after each
		epoch, and partial progress printed out.  This is useful for
		tracking progress, but slows things down substantially."""
		if test_data: n_test = len(test_data)
		n = len(training_data)
		for j in xrange(epochs):
			random.shuffle(training_data)
			mini_batches = [training_data[k:k+mini_batch_size] for k in xrange(0, n, mini_batch_size)]
			for mini_batch in mini_batches:
				self.update_mini_batch(mini_batch, eta)
			if test_data:
				print "Epoch {0}: {1} / {2}".format(j, self.evaluate(test_data), n_test)
			else:
				print "Epoch {0} complete".format(j)

	def update_mini_batch(self, mini_batch, eta):
		"""Update the network's weights and biases by applying
		gradient descent using backpropagation to a single mini batch.
		The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``
		is the learning rate."""
		nabla_b = [np.zeros(b.shape) for b in self.biases]
		nabla_w = [np.zeros(w.shape) for w in self.weights]
		for x, y in mini_batch:
			delta_nabla_b, delta_nabla_w = self.backprop(x, y)
			nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
			nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
		self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]
		self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]

	def backprop(self, x, y):
		"""Return a tuple ``(nabla_b, nabla_w)`` representing the
		gradient for the cost function C_x.  ``nabla_b`` and
		``nabla_w`` are layer-by-layer lists of numpy arrays, similar
		to ``self.biases`` and ``self.weights``."""
		
		nabla_b = [np.zeros(b.shape) for b in self.biases]
		nabla_w = [np.zeros(w.shape) for w in self.weights]
		# feedforward
		activation = x
		activations = [x] # list to store all the activations, layer by layer
		zs = [] # list to store all the z vectors, layer by layer
		for b, w in zip(self.biases, self.weights):
			z = np.dot(w, activation)+b
			zs.append(z)
			activation = sigmoid(z)
			activations.append(activation)
			
		# backward pass
		delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])
		nabla_b[-1] = delta
		nabla_w[-1] = np.dot(delta, activations[-2].transpose())
		# Note that the variable l in the loop below is used a little
		# differently to the notation in Chapter 2 of the book.  Here,
		# l = 1 means the last layer of neurons, l = 2 is the
		# second-last layer, and so on.  It's a renumbering of the
		# scheme in the book, used here to take advantage of the fact
		# that Python can use negative indices in lists.
		for l in xrange(2, self.num_layers):
			z = zs[-l]
			sp = sigmoid_prime(z)
			delta = np.dot(self.weights[-l+1].transpose(), delta) * sp
			nabla_b[-l] = delta
			nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())
		return (nabla_b, nabla_w)

	def evaluate(self, test_data):
		"""Return the number of test inputs for which the neural
		network outputs the correct result. Note that the neural
		network's output is assumed to be the index of whichever
		neuron in the final layer has the highest activation."""
		test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]
		return sum(int(x == y) for (x, y) in test_results)

	def cost_derivative(self, output_activations, y):
		"""Return the vector of partial derivatives \partial C_x /
		\partial a for the output activations."""
		return (output_activations-y)

#### Miscellaneous functions
def sigmoid(z):
	"""The sigmoid function."""
	return 1.0/(1.0+np.exp(-z))

def sigmoid_prime(z):
	"""Derivative of the sigmoid function."""
	return sigmoid(z)*(1-sigmoid(z))
\end{lstlisting}

\paragraph{Codi final per el discriminador}
\label{lst:disc_final}
Aquest codi es pot trobar al \href{https://github.com/tomiock/qGAN}{repositori del treball} en l'arxiu \code{discriminator.py}. 

\begin{lstlisting}[language=Python, caption=Codi final pel discriminador]
"""DISCRIMINATOR"""
import json
from typing import Dict, List

import numpy as np

from quantumGAN.functions import BCE_derivative, minimax_derivative_fake, minimax_derivative_real, sigmoid, sigmoid_prime


def load(filename):
	f = open(filename, "r")
	data = json.load(f)
	f.close()
	# cost = getattr(sys.modules[__name__], data["cost"])
	net = ClassicalDiscriminator_that_works(data["sizes"], data["loss"])
	net.weights = [np.array(w) for w in data["weights"]]
	net.biases = [np.array(b) for b in data["biases"]]
	return net


class ClassicalDiscriminator:

	def __init__(	self,
					sizes: List[int],
					type_loss: str) -> None:

		self.num_layers = len(sizes)
		self.sizes = sizes
		self.type_loss = type_loss
		self.data_loss = {"real": [], "fake": []}
		self.ret: Dict[str, any] = {"loss": [],
			"label real": [],
			"label fake": [],
			"label fake time": [],
			"label real time": []}
		self.biases = [np.random.randn(y, ) for y in sizes[1:]]
		self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]

	def feedforward(self, a):
		"""Return the output of the network if ``a`` is input."""
		for b, w in zip(self.biases, self.weights):
		a = sigmoid(np.dot(w, a) + b)
		return a

	def predict(self, x):
		# feedforward
		activation = x
		zs = []  # list to store all the z vectors, layer by layer
		for b, w in zip(self.biases, self.weights):
			z = np.dot(w, activation) + b
			zs.append(z)
			activation = sigmoid(z)
		return activation

	def evaluate(self, test_data):
		test_results = [(np.argmax(self.feedforward(x)), y)
		for (x, y) in test_data]
		return sum(int(x == y) for (x, y) in test_results)


	def forwardprop(self, x: np.ndarray):
		activation = x
		activations = [x]  # list to store all the activations, layer by layer
		zs = []  # list to store all the z vectors, layer by layer
		for b, w in zip(self.biases, self.weights):
			z = np.dot(w, activation) + b
			zs.append(z)
			activation = sigmoid(z)
			activations.append(activation)
		return activation, activations, zs

	def backprop_bce(self, image, label):
		"""Return a tuple ``(nabla_b, nabla_w)`` representing the
		gradient for the cost function C_x.  ``nabla_b`` and
		``nabla_w`` are layer-by-layer lists of numpy arrays, similar
		to ``self.biases`` and ``self.weights``."""
		nabla_b = [np.zeros(b.shape) for b in self.biases]
		nabla_w = [np.zeros(w.shape) for w in self.weights]

		# feedforward and back error calculation depending on type of image
		activation, activations, zs = self.forwardprop(image)
		delta = BCE_derivative(activations[-1], label) * sigmoid_prime(zs[-1])

		# backward pass
		nabla_b[-1] = delta
		nabla_w[-1] = np.dot(delta, activations[-2].reshape(1, activations[-2].shape[0]))

		for l in range(2, self.num_layers):
			z = zs[-l]
			delta = np.dot(self.weights[-l + 1].transpose(), delta) * sigmoid_prime(z)
			nabla_b[-l] = delta
			nabla_w[-l] = np.dot(delta.reshape(delta.shape[0], 1), activations[-l - 1].reshape(1, activations[-l - 1].shape[0]))
		return nabla_b, nabla_w, activations[-1]

	def backprop_minimax(self, real_image, fake_image, is_real):
		"""Return a tuple ``(nabla_b, nabla_w)`` representing the
		gradient for the cost function C_x.  ``nabla_b`` and
		``nabla_w`` are layer-by-layer lists of numpy arrays, similar
		to ``self.biases`` and ``self.weights``."""
		nabla_b = [np.zeros(b.shape) for b in self.biases]
		nabla_w = [np.zeros(w.shape) for w in self.weights]

		# feedforward and back error calculation depending on type of image
		activation_real, activations_real, zs_real = self.forwardprop(real_image)
		activation_fake, activations_fake, zs_fake = self.forwardprop(fake_image)

		if is_real:
			delta = minimax_derivative_real(activations_real[-1]) * sigmoid_prime(zs_real[-1])
			activations, zs = activations_real, zs_real
		else:
			delta = minimax_derivative_fake(activations_fake[-1]) * sigmoid_prime(zs_fake[-1])
			activations, zs = activations_fake, zs_fake

		# backward pass
		nabla_b[-1] = delta
		nabla_w[-1] = np.dot(delta, activations[-2].reshape(1, activations[-2].shape[0]))

		for l in range(2, self.num_layers):
			z = zs[-l]
			delta = np.dot(self.weights[-l + 1].transpose(), delta) * sigmoid_prime(z)
			nabla_b[-l] = delta
			nabla_w[-l] = np.dot(delta.reshape(delta.shape[0], 1),
			activations[-l - 1].reshape(1, activations[-l - 1].shape[0]))
		return nabla_b, nabla_w, activations[-1]

	def train_mini_batch(self, mini_batch, learning_rate):
		global label_real, label_fake
		nabla_b = [np.zeros(b.shape) for b in self.biases]
		nabla_w = [np.zeros(w.shape) for w in self.weights]

		if self.type_loss == "binary cross entropy":
			for real_image, fake_image in mini_batch:
				delta_nabla_b, delta_nabla_w, label_real = self.backprop_bce(real_image, np.array([1.]))
				nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
				nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]

				delta_nabla_b, delta_nabla_w, label_fake = self.backprop_bce(fake_image, np.array([0.]))
				nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
				nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]

		elif self.type_loss == "minimax":
			for real_image, fake_image in mini_batch:
				delta_nabla_b, delta_nabla_w, label_real = self.backprop_minimax(real_image, fake_image, True)
				nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
				nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]

				delta_nabla_b, delta_nabla_w, label_fake = self.backprop_minimax(real_image, fake_image, False)
				nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
				nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
			else:
				raise Exception("type of loss function not valid")

			# gradient descent
			# nabla_w and nabla_b are multiplied by the learning rate
			# and taken the mean of (dividing by the mini batch size)
			self.weights = [w - (learning_rate / len(mini_batch)) * nw
							for w, nw in zip(self.weights, nabla_w)]
			self.biases = [b - (learning_rate / len(mini_batch)) * nb
						   for b, nb in zip(self.biases, nabla_b)]
\end{lstlisting}

\paragraph{Codi per el generador}
\label{lst:gen_final}
Aquest codi es pot trobar al \href{https://github.com/tomiock/qGAN}{repositori del treball} en l'arxiu \code{generador.py}. 

\begin{lstlisting}[language=Python, caption=Codi final pel generador]
"""QUANTUM GENERATOR"""

from typing import Any, Dict, Optional, cast

import numpy as np
import qiskit
from qiskit import ClassicalRegister, QuantumRegister
from qiskit.circuit import QuantumCircuit
from qiskit.circuit.library import TwoLocal
from qiskit.providers.aer import AerSimulator

from quantumGAN.functions import create_entangler_map, create_real_keys, minimax_generator


class QuantumGenerator:

	def __init__(
			self,
			shots: int,
			num_qubits: int,
			num_qubits_ancilla: int,
			generator_circuit: Optional[QuantumCircuit] = None,
			snapshot_dir: Optional[str] = None
	) -> None:

		super().__init__()
		# passar els arguments de la classe a metodes en de classes
		# d'aquesta manera son accessibles per qualsevol funcio dintre de la classe
		self.num_qubits_total = num_qubits
		self.num_qubits_ancilla = num_qubits_ancilla
		self.generator_circuit = generator_circuit
		self.snapshot_dir = snapshot_dir
		self.shots = shots
		self.discriminator = None
		self.ret: Dict[str, Any] = {"loss": []}
		self.simulator = AerSimulator()

	def init_parameters(self):
		"""Inicia els parametres inicial i crea el circuit al qual se li posen els parametres"""
		# iniciacia dels parametres inicials i dels circuits al qual posar aquests parametres
		self.generator_circuit = self.construct_circuit(latent_space_noise=None, to_measure=False)
		# s'ha de crear primer el circuit perque d'aquesta manera es pot saber el nombre de parametres que es necessiten
		self.parameter_values = np.random.normal(np.pi / 2, .1, self.generator_circuit.num_parameters)

	def construct_circuit(self,
						latent_space_noise,
						to_measure: bool):
		"""Crea el circuit quantic des de zero a partir de diversos registres de qubits"""
		if self.num_qubits_ancilla is 0:
			qr = QuantumRegister(self.num_qubits_total, 'q')
			cr = ClassicalRegister(self.num_qubits_total, 'c')
			qc = QuantumCircuit(qr, cr)
		else:
			qr = QuantumRegister(self.num_qubits_total - self.num_qubits_ancilla, 'q')
			anc = QuantumRegister(self.num_qubits_ancilla, 'ancilla')
			cr = ClassicalRegister(self.num_qubits_total - self.num_qubits_ancilla, 'c')
			qc = QuantumCircuit(anc, qr, cr)

		# creacia de la part del circuit que conte la implantacio dels parametres d'input. En cas que no es donin aquests parametres es creen automaticament
		if latent_space_noise is None:
			randoms = np.random.normal(-np.pi * .01, np.pi * .01, self.num_qubits_total)
			init_dist = qiskit.QuantumCircuit(self.num_qubits_total)

			# es col.loca una porta RY en cada qubits i amb un parametre diferent cadascuna
			for index in range(self.num_qubits_total):
				init_dist.ry(randoms[index], index)
		else:
			init_dist = qiskit.QuantumCircuit(self.num_qubits_total)

			for index in range(self.num_qubits_total):
				init_dist.ry(latent_space_noise[index], index)

		# la funcio create_entagler_map crea les parelles de qubits a les qual col.locar les portes CZ
		# en funcio del nombre de qubits
		if self.num_qubits_ancilla is 0:
			entangler_map = create_entangler_map(self.num_qubits_total)
		else:
			entangler_map = create_entangler_map(self.num_qubits_total - self.num_qubits_ancilla)

		# creacio final dels circuits a partir una funcio integrada a Qiskit que va repetint les operacions
		# que se li especifiquen
		ansatz = TwoLocal(int(self.num_qubits_total), 'ry', 'cz', entanglement=entangler_map, reps=1, insert_barriers=True)

		# aqui s'ajunten el circuit que funciona com a input amb el circuit que consisteix en la repeticio
		# de les portes RY i CZ
		qc = qc.compose(init_dist, front=True)
		qc = qc.compose(ansatz, front=False)

		if to_measure:
			qc.measure(qr, cr)

		return qc

	def set_discriminator(self, discriminator) -> None:
		self.discriminator = discriminator

	def get_output(
					self,
					latent_space_noise,
					parameters: Optional[np.ndarray] = None
					):
		"""Retorna un output del generador quan se li dona un estat d'input i opcionalment uns parametres en especific. Els pixels estan compostos per la probabilitat que un qubit resulti en ket_0 en cada base. Per tant, els pixels de l'imatge estan normalitzats amb la norma l-1."""
		real_keys_set, real_keys_list = create_real_keys(self.num_qubits_total - self.num_qubits_ancilla)

		# en cas de que no es donin parametres com a input, es treuen els parametres de la variable
		# self.parameter_values. Es a dir els parametres que es creen automaticament al principi i que es van
		# actualitzant al mateix temps que el model s'optimitza
		if parameters is None:
			parameters = cast(np.ndarray, self.parameter_values)

		qc = self.construct_circuit(latent_space_noise, True)

		parameter_binds = {parameter_id: parameter_value for parameter_id, parameter_value in zip(qc.parameters, parameters)}

		# el metode bind_parametres del circuit quantic
		qc = qc.bind_parameters(parameter_binds)

		# Simulacio dels circuits mitjancant el simulador Aer de Qiskit. El nivell d'optimitzacio es zero, perque al ser circuits petits i que simulen una vegada, no es necessari. Al optimitzar el proces acaba sent mes lent.
		result_ideal = qiskit.execute(experiments=qc,
									  backend=self.simulator,
									  shots=self.shots,
									  optimization_level=0).result()
		counts = result_ideal.get_counts()

		try:
			# creacio de l'imatge resultant
			pixels = np.array([counts[index] for index in list(real_keys_list)])

		except KeyError:
			# aquesta excepcio sorgeix quan en el diccionari dels resultats no estan totes les keys pel fet que qiskit, en cas de que no hi hagi un mesurament en una base, no inclou aquesta base en el diccionari
			keys = counts.keys()
			missing_keys = real_keys_set.difference(keys)
			# s'utilitza un la resta entre dos sets per poder veure quina es la key que falta en el diccionari
			for key_missing in missing_keys:
				counts[key_missing] = 0

			# una vegada es troba les keys que faltaven es crea l'imatge resultant
			pixels = np.array([counts[index] for index in list(real_keys_list)])

		pixels = pixels / self.shots
		return pixels

	def get_output_pixels(
					 	  self,
						  latent_space_noise,
						  params: Optional[np.ndarray] = None
						  ):
		"""Retorna un output del generador quan se li dona un estat d'input i opcionalment uns parametres en especific. Cada pixel es la probabilitat de que un qubits resulti en l'estat ket_0, per tant, els valors cada pixel (que son independents entre si) es troba en l'interval (0, 1) """
		qc = QuantumCircuit(self.num_qubits_total)

		init_dist = qiskit.QuantumCircuit(self.num_qubits_total)
		assert latent_space_noise.shape[0] == self.num_qubits_total

		for num_qubit in range(self.num_qubits_total):
			init_dist.ry(latent_space_noise[num_qubit], num_qubit)

		if params is None:
			params = cast(np.ndarray, self.parameter_values)

		qc.assign_parameters(params)

		# comptes de simular els valors que donara cada qubits, es simula l'estat final del circuit i d'aquest s'extreuen els valors que es mesuraran per a cada qubit
		state_vector = qiskit.quantum_info.Statevector.from_instruction(qc)
		pixels = []
		for qubit in range(self.num_qubits_total):
			# per treure la probabilitat s'utilitza una funcio implementada en Qiskit
			pixels.append(state_vector.probabilities([qubit])[0])

		# creacio de l'imatge resultada a partir de la llista que conte el valor per a cada pixel
		generated_samples = np.array(pixels)
		generated_samples.flatten()

		return generated_samples

	def train_mini_batch(self, mini_batch, learning_rate):
		"""Optimitzacio del generador per una batch d'imatges. Retorna una batch de les imatges generades amb unes imatges reals que poder donar com a input al generador. """
	 	nabla_theta = np.zero_like(self.parameter_values.shape)
		new_images = []

		for _, noise in mini_batch:
			for index, _ in enumerate(self.parameter_values):
				perturbation_vector = np.zeros_like(self.parameter_values)
				perturbation_vector[index] = 1

				# Creacio dels parametres per generar les dues imatges
				pos_params = self.parameter_values + (np.pi / 4) * perturbation_vector
				neg_params = self.parameter_values - (np.pi / 4) * perturbation_vector

				pos_result = self.get_output(noise, parameters=pos_params)  # Generacio imatges
				neg_result = self.get_output(noise, parameters=neg_params)

				pos_result = self.discriminator.predict(pos_result)  # Assignacio de les etiquetes
				neg_result = self.discriminator.predict(neg_result)

				# Diferencia entre les avaluacions de la funcio de perdua entre les dues etiquetes
				gradient = minimax_generator(pos_result) - minimax_generator(neg_result)
				nabla_theta[index] += gradient  # Afegir derivada d'un parametre al gradient
			new_images.append(self.get_output(noise))

		for index, _ in enumerate(self.parameter_values):
			# Actualitzacio dels parametres a traves del gradient
			self.parameter_values[index] += (learning_rate / len(mini_batch)) * nabla_theta[index]

		# Creacio de la batch d'imatges a retornar
		mini_batch = [(datapoint[0], fake_image) for datapoint, fake_image in zip(mini_batch, new_images)]
		return mini_batch
\end{lstlisting}


\paragraph{Definició del model}
\label{lst:qgan}
Aquest codi es pot trobar al \href{https://github.com/tomiock/qGAN}{repositori del treball} en l'arxiu \code{qgan.py}. 
\begin{lstlisting}[language=Python, caption=Codi per la definició del model]
import glob
import os
import json
import random
from datetime import datetime
from typing import List

import imageio
import matplotlib.pyplot as plt
import numpy as np

from quantumGAN.discriminator import ClassicalDiscriminator
from quantumGAN.functions import fechet_distance, minimax, images_to_distribution, images_to_scatter
from quantumGAN.quantum_generator import QuantumGenerator


class Quantum_GAN:

	def __init__(self,
				 generator: QuantumGenerator,
				 discriminator: ClassicalDiscriminator
				 ):

		self.last_batch = None
		now = datetime.now()
		init_time = now.strftime("%d_%m_%Y__%H_%M_%S")
		self.path = "data/run{}".format(init_time)
		self.path_images = self.path + "/images"
		self.filename = "run.txt"

		if not os.path.exists(self.path):
			os.makedirs(self.path)

		if not os.path.exists(self.path_images):
			os.makedirs(self.path_images)

		with open(os.path.join(self.path, self.filename), "w") as file:
			file.write("RUN {} \n".format(init_time))
			file.close()

		self.generator = generator
		self.discriminator = discriminator
		self.loss_series, self.label_real_series, self.label_fake_series, self.FD_score = [], [], [], []

		self.generator.init_parameters()
		self.example_g_circuit = self.generator.construct_circuit(latent_space_noise=None,
									 	 to_measure=False)
		self.generator.set_discriminator(self.discriminator)

	def __repr__(self):
		return "Discriminator Architecture: {} \n Generator Example Circuit: \n{}" \
		.format(self.discriminator.sizes, self.example_g_circuit)

	def store_info(self, epoch, loss, real_label, fake_label):
		file = open(os.path.join(self.path, self.filename), "a")
		file.write("{} epoch LOSS {} Parameters {} REAL {} FAKE {} \n"
		.format(epoch,
				loss,
				self.generator.parameter_values,
				real_label,
				fake_label))
		file.close()

	def plot(self):
		# save data for plotting
		fake_images, real_images = [], []
		for image_batch in self.last_batch:
			fake_images.append(image_batch[1])
			real_images.append(image_batch[0])

		keys, average_result = images_to_distribution(fake_images)
		print(average_result)
		if self.generator.num_qubits_ancilla != 0:
			plt.title(f"Distribucio d'una imatge generada \n(ancilla, epoch={self.num_epochs})")
		else:
			plt.title(f"Distribucio d'una imatge generada \n(epoch={self.num_epochs})")
		plt.ylim(0., 1.)
		plt.bar(keys, average_result)
		plt.savefig(self.path + "/fake_distribution.png")
		plt.clf()

		keys_real, average_result_real = images_to_distribution(real_images)
		print(average_result_real)
		if self.generator.num_qubits_ancilla != 0:
			plt.title(f"Distribucio d'una imatge real \n(ancilla, epoch={self.num_epochs})")
		else:
			plt.title(f"Distribucio d'una imatge real \n(epoch={self.num_epochs})")
		plt.ylim(0., 1.)
		plt.bar(keys_real, average_result_real)
		plt.savefig(self.path + "/real_distribution.png")
		plt.clf()

		y_axis, x_axis = images_to_scatter(fake_images)
		y_axis_real, x_axis_real = images_to_scatter(real_images)
		if self.generator.num_qubits_ancilla != 0:
			plt.title(f"Comparacio entre les imatges reals con les imatges falses \n(ancilla, epoch={self.num_epochs})")
		else:
			plt.title(f"Comparacio entre les imatges reals con les imatges falses \n(epoch={self.num_epochs})")
		plt.ylim(0. - .1, 1. + .1)
		plt.scatter(y_axis, x_axis, label='Valors per les imatges falses',
				 	color='indigo',
					linewidth=.1,
					alpha=.2)
		plt.scatter(y_axis_real, x_axis_real, label='Valors per les imatges reals',
					color='limegreen',
					linewidth=.1,
					alpha=.2)
		plt.savefig(self.path + "/scatter_plot.png")
		plt.clf()

		t_steps = np.arange(self.num_epochs)
		plt.figure(figsize=(6, 5))
		if self.generator.num_qubits_ancilla != 0:
			plt.title(f"Progres de la funcio de perdua \n(ancilla, epoch={self.num_epochs})")
		else:
			plt.title(f"Progres de la funcio de perdua \n(epoch={self.num_epochs})")
		plt.plot(t_steps, self.loss_series, label='Funcio de perdua del discriminador', color='rebeccapurple', linewidth=2)
		plt.grid()
		plt.legend(loc='best')
		plt.xlabel('Iteracions')
		plt.ylabel('Funcio de perdua')
		plt.savefig(self.path + "/loss_plot.png")
		plt.clf()

		t_steps = np.arange(self.num_epochs)
		plt.figure(figsize=(6, 5))
		if self.generator.num_qubits_ancilla != 0:
			plt.title(f"Progres de les etiquetes \n(ancilla, epoch={self.num_epochs})")
		else:
			plt.title(f"Progres de les etiquetes \n(epoch={self.num_epochs})")
		plt.scatter(t_steps, self.label_real_series, label='Etiquetes per les imatges reals',
					color='mediumvioletred',
					linewidth=.1)
		plt.scatter(t_steps, self.label_fake_series, label='Etiquetes per les imatges generades',
					color='rebeccapurple',
					linewidth=.1)
		plt.grid()
		plt.ylim(0. - .1, 1. + .1)
		plt.legend(loc='best')
		plt.xlabel('Iteracions')
		plt.ylabel('Valor de les etiquetes')
		plt.savefig(self.path + "/labels_plot.png")
		plt.clf()

		plt.figure(figsize=(6, 5))
		if self.generator.num_qubits_ancilla != 0:
			plt.title(f"Puntuacio de a partir de la distancia de Frechet \n(ancilla, epoch={self.num_epochs})")
		else:
			plt.title(f"Puntuacio de a partir de la distancia de Frechet \n(epoch={self.num_epochs})")
		plt.plot(t_steps, self.FD_score, label='Valor de la puntuacio de Frechet', color='rebeccapurple', linewidth=2)
		plt.grid()
		plt.legend(loc='best')
		plt.ylim(0., .28)
		plt.xlabel('Iteracions')
		plt.ylabel('Valor de la puntuacio')
		plt.savefig(self.path + "/FD_score.png")
		plt.clf()

	def train(self,
			  num_epochs: int,
			  training_data: List,
		 	  batch_size: int,
			  generator_learning_rate: float,
			  discriminator_learning_rate: float,
			  is_save_images: bool):

		self.num_epochs = num_epochs
		self.training_data = training_data
		self.batch_size = batch_size
		self.batch_size = batch_size
		self.generator_lr = generator_learning_rate
		self.discriminator_lr = discriminator_learning_rate
		self.is_save_images = is_save_images

		noise = self.training_data[0][1]
		time_init = datetime.now()
		for o in range(self.num_epochs):
			mini_batches = create_mini_batches(self.training_data, self.batch_size)
			output_fake = self.generator.get_output(latent_space_noise=mini_batches[0][0][1], parameters=None)

			for mini_batch in mini_batches:
				self.last_batch = self.generator.train_mini_batch(mini_batch, self.generator_lr)
				self.discriminator.train_mini_batch(self.last_batch, self.discriminator_lr)

			output_real = mini_batches[0][0][0]
			if is_save_images:
				self.save_images(self.generator.get_output(latent_space_noise=noise, parameters=None), o)

			self.FD_score.append(fechet_distance(output_real, output_fake))
			label_real, label_fake = self.discriminator.predict(output_real), self.discriminator.predict(output_fake)
			loss_final = 1 / 2 * (minimax(label_real, label_fake) + minimax(label_real, label_fake))

			self.loss_series.append(loss_final)
			self.label_real_series.append(label_real)
			self.label_fake_series.append(label_fake)

			print("Epoch {}: Loss: {}".format(o, loss_final), output_real, output_fake)
			print(label_real[-1], label_fake[-1])
			self.store_info(o, loss_final, label_real, label_fake)

		time_now = datetime.now()
		print((time_now - time_init).total_seconds(), "seconds")

	def save_images(self, image, epoch):
		image_shape = int(image.shape[0] / 2)
		image = image.reshape(image_shape, image_shape)

		plt.imshow(image, cmap='gray', vmax=1., vmin=0.)
		plt.axis('off')
		plt.savefig(self.path_images + '/image_at_epoch_{:04d}.png'.format(epoch))
		plt.clf()

	def create_gif(self):
		anim_file = self.path + '/dcgan.gif'

		with imageio.get_writer(anim_file, mode='I') as writer:
			filenames = glob.glob(self.path_images + '/image*.png')
			filenames = sorted(filenames)

			for filename in filenames:
				image = imageio.imread(filename)
				writer.append_data(image)

			image = imageio.imread(filename)
			writer.append_data(image)

	def save(self):
		"""Save the neural network to the file ``filename``."""
		data = {"batch_size": self.batch_size,
				"D_sizes": self.discriminator.sizes,
				"D_weights": [w.tolist() for w in self.discriminator.weights],
				"D_biases": [b.tolist() for b in self.discriminator.biases],
				"D_loss": self.discriminator.type_loss,
				"Q_parameters": [theta for theta in self.generator.parameter_values],
				"Q_shots": self.generator.shots,
				"Q_num_qubits": self.generator.num_qubits_total,
				"Q_num_qubits_ancilla": self.generator.num_qubits_ancilla,
				"real_labels": self.label_real_series,
				"fake_labels": self.label_fake_series,
				"loss_series": self.loss_series
				}
		f = open(os.path.join(self.path, "data.txt"), "a")
		json.dump(data, f)
		f.close()


	def create_mini_batches(training_data, mini_batch_size):
		n = len(training_data)
		random.shuffle(training_data)
		mini_batches = [
			training_data[k:k + mini_batch_size]
			for k in range(0, n, mini_batch_size)]
		return [mini_batches[0]]


	def load_gan(filename):
		f = open(filename, "r")
		data = json.load(f)
		f.close()
		discriminator = ClassicalDiscriminator(data["D_sizes"], data["D_loss"])

		generator = QuantumGenerator(num_qubits=data["Q_num_qubits"],
									 generator_circuit=None,
									 num_qubits_ancilla=data["Q_num_qubits_ancilla"],
									 shots=data["Q_shots"])

		quantum_gan = Quantum_GAN(generator, discriminator)

		quantum_gan.discriminator.weights = [np.array(w) for w in data["D_weights"]]
		quantum_gan.discriminator.biases = [np.array(b) for b in data["D_biases"]]
		quantum_gan.generator.parameter_values = np.array(data["Q_parameters"])

		1return quantum_gan, data["batch_size"]
	
	
\end{lstlisting}

\paragraph{Execució del model}
\label{lst:main}
Aquest codi es pot trobar al \href{https://github.com/tomiock/qGAN}{repositori del treball} en l'arxiu \code{main.py}. 

\begin{lstlisting}[language=Python, caption=Codi final pel generador]
import numpy as np

from quantumGAN.discriminator import ClassicalDiscriminator
from quantumGAN.qgan import Quantum_GAN
from quantumGAN.quantum_generator import QuantumGenerator

num_qubits: int = 3

# Set number of training epochs
num_epochs = 150
# Batch size
batch_size = 10

train_data = []
for _ in range(800):
	x2 = np.random.uniform(.55, .46, (2,))
	fake_datapoint = np.random.uniform(-np.pi * .01, np.pi * .01, (num_qubits,))
	real_datapoint = np.array([x2[0], 0, x2[0], 0])
	train_data.append((real_datapoint, fake_datapoint))

discriminator = ClassicalDiscriminator(sizes=[4, 16, 8, 1],
									   type_loss="minimax")
generator = QuantumGenerator(num_qubits=num_qubits,
							 generator_circuit=None,
							 num_qubits_ancilla=1,
							 shots=4096)

quantum_gan = Quantum_GAN(generator, discriminator)
print(quantum_gan)
print(num_epochs)
quantum_gan.generator.get_output(fake_datapoint[0], None)
quantum_gan.train(num_epochs, train_data, batch_size, .1, .1, True)

quantum_gan.plot()
quantum_gan.create_gif()
quantum_gan.save()

\end{lstlisting}



\paragraph{Funcions extres}
Hi han funcions en el codi que utilitzo en diversos arxius, aquestes les defineixo en l'arxiu \code{functions.py}, que es pot trobar al \href{https://github.com/tomiock/qGAN}{repositori del treball}. En aquest arxiu també hi ha una funció que dona a terme una traça parcial a una matriu donada, malgrat que no la he arribat a utilitzar. 
\begin{lstlisting}[language=Python, caption=Funcions varies]
import itertools
import math

import numpy as np
from matplotlib import pyplot as plt


# DATA PROCESSING
from scipy import linalg


def save_images_color(image, epoch):
	plt.imshow(image.reshape(int(image.shape[0] / 3), 1, 3))
	plt.axis('off')
	plt.savefig('images/image_at_epoch_{:04d}.png'.format(epoch))


def create_real_keys(num_qubits):
	lst = [[str(a) for a in i] for i in itertools.product([0, 1], repeat=num_qubits)]
	new_lst = []
	for element in lst:
		word = str()
		for number in element:
			word = word + number
			new_lst.append(word)
	return set(new_lst), new_lst


def create_entangler_map(num_qubits: int):
	lst = [list(i) for i in itertools.combinations(range(num_qubits), 2)]
	index = 0
	entangler_map = []
	for i in reversed(range(num_qubits)):
		try:
			entangler_map.append(lst[index])
			index += i
		except IndexError:
			return entangler_map


def images_to_distribution(batch_image):
	num_images = len(batch_image)
	sum_result = 0
	for image in batch_image:
		sum_result += image
	average_result = sum_result / num_images
	keys = create_real_keys(int(math.sqrt(batch_image[0].shape[0])))[1]
	return keys, average_result


def images_to_scatter(batch_image):
	keys = create_real_keys(int(math.sqrt(batch_image[0].shape[0])))[1]
	x_axis = []
	y_axis = []

	for image in batch_image:
		pixel_count = 0
		for pixel in image:
			x_axis.append(pixel)
			y_axis.append(keys[pixel_count])
			pixel_count += 1

	return y_axis, x_axis

def fechet_distance(image1: np.array, 
					image2: np.array):
	assert image1.shape == image2.shape
	y = np.arange(0, image1.flatten().shape[0])

	matrix_a_cov = np.cov(np.stack((y, image1.flatten()), axis=0))
	matrix_b_cov = np.cov(np.stack((y, image2.flatten()), axis=0))

	to_trace = matrix_a_cov + matrix_b_cov - 2*(linalg.fractional_matrix_power(np.dot(matrix_a_cov, matrix_b_cov), .5))
	return np.abs(image1.mean() - image2.mean())**2 + np.trace(to_trace)


# ACTIVATION FUNCTIONS

def sigmoid(z):
	"""The sigmoid function."""
	return 1.0 / (1.0 + np.exp(-z))


def sigmoid_prime(z):
	"""Derivative of the sigmoid function."""
	return sigmoid(z) * (1 - sigmoid(z))


def relu(z):
	return np.maximum(0, z)


def relu_prime(z):
	z[z <= 0] = 0
	z[z > 0] = 1
	return z


# LOSSES
def MSE_derivative(prediction, y):
	return 2 * (y - prediction)


def MSE(prediction, y):
	return (y - prediction) ** 2


def BCE_derivative(prediction, target):
	# return prediction - target
	return -target / prediction + (1 - target) / (1 - prediction)


def BCE(predictions: np.ndarray, targets: np.ndarray) -> np.ndarray:
	return targets * np.log(predictions) + (1 - targets) * np.log(1 - predictions).mean()


def minimax_derivative_real(real_prediction):
	real_prediction = np.array(real_prediction)
	return np.nan_to_num((-1) * (1 / real_prediction))


def minimax_derivative_fake(fake_prediction):
	fake_prediction = np.array(fake_prediction)
	return np.nan_to_num(1 / (1 - fake_prediction))


def minimax(real_prediction, fake_prediction):
	return np.nan_to_num(np.log(real_prediction) + np.log(1 - fake_prediction))


def minimax_generator(prediction_fake):
	return (-1) * np.log(1 - prediction_fake)


# QUANTUM FUNCTIONS

class Partial_Trace:
	def __init__(self, state: np.array, qubits_out: int, side: str):

		self.state = state
		self.qubits_out = qubits_out
		self.side = side

		if self.state.ndim == 1:
			self.state = np.outer(self.state, self.state)

		self.total_dim = self.state.shape[0]

		self.num_qubits = int(np.log2(self.total_dim))
		self.a_dim = 2 ** (self.num_qubits - self.qubits_out)
		self.b_dim = 2 ** self.qubits_out

		# if self.side == "bot":
		self.basis_b = [_ for _ in np.identity(int(self.b_dim))]
		self.basis_a = [_ for _ in np.identity(int(self.a_dim))]

		# elif self.side == "top":
		#	self.basis_a = [_ for _ in np.identity(int(self.b_dim))]
		#	self.basis_b = [_ for _ in np.identity(int(self.a_dim))]
		#
		# else:
		#	raise NameError("invalid side argument, enter \"bot\" or \"top\"")
		print(self.basis_a, self.basis_b)

	def get_entry(self, index_i, index_j):
		sigma = 0

		if self.side == "bot":
			for k in range(self.qubits_out + 1):
				ab_l = np.kron(self.basis_a[index_i],
						   	   self.basis_b[k])
				ab_r = np.kron(self.basis_a[index_j],
						   	   self.basis_b[k])

				print(ab_r, ab_l)

				right_side = np.dot(self.state, ab_r)
				sigma += np.inner(ab_l, right_side)

		if self.side == "top":
			for k in range(self.qubits_out + 1):
				ba_l = np.kron(self.basis_b[index_i],
							   self.basis_a[k])
				ba_r = np.kron(self.basis_b[index_j],
							   self.basis_a[k])

				print(ba_r, ba_l)

				right_side = np.dot(self.state, ba_r)
				sigma += np.inner(ba_l, right_side)

		return sigma

	def compute_matrix(self):
		a = [_ for _ in range(self.a_dim)]
		b = [__ for __ in range(self.a_dim)]

		entries_pre = [(x, y) for x in a for y in b]
		entries = []

		for i_index, j_index in entries_pre:
			entries.append(self.get_entry(i_index, j_index))

		entries = np.array(entries)
		return entries.reshape(self.a_dim, self.a_dim)

\end{lstlisting}

\subsection{Capítol 7}
\paragraph{Multiprocessament}
Per poder ser més eficient a l'hora d'executar els models, vaig decidir utilitzar una característica molt útil de Python, el \textit{multiprocessing}. Usualment, una instància de Python, és a dir; un arxiu executant-se, només es fa servir un nucli del processador a la vegada, però amb \textit{multiprocessing} pots emprar múltiples nuclis amb un mateix arxiu, d'aquesta manera executant diverses línies de codi al mateix temps. Per poder executar diversos models a la mateixa vegada i d'una forma simple, vaig fer servir aquest mòdul de Python per executar els models dels quals he agafat les dades que he presentat en aquest treball. Aquest codi es pot trobar al \href{https://github.com/tomiock/qGAN}{repositori del treball} en l'arxiu \code{test\_multi.py}.
\begin{lstlisting}[language=Python, caption=Executar els models amb multiprocessing]
	import multiprocessing
	import time
	import numpy as np
	
	from quantumGAN.discriminator import ClassicalDiscriminator
	from quantumGAN.qgan import Quantum_GAN
	from quantumGAN.quantum_generator import QuantumGenerator
	
	batch_size = 10
	
	train_data = []
	# generacio de les dades
	for _ in range(800):
		x2 = np.random.uniform(.55, .46, (2,))
		fake_datapoint = np.random.uniform(-np.pi * .01, np.pi * .01, (4,))
		real_datapoint = np.array([x2[0], 0, x2[0], 0])
		train_data.append((real_datapoint, fake_datapoint))
	list_epochs = [550, 550, 550]
	
	example_generator_ancilla = QuantumGenerator(num_qubits=4,
												 generator_circuit=None,
												 num_qubits_ancilla=2,
												 shots=4096)
	example_generator = QuantumGenerator(num_qubits=2,
										 generator_circuit=None,
										 num_qubits_ancilla=0,
										 shots=4096)
	
	# generem els parametres inicials d'acord amb el circuit amb un mayor nombre de parametres
	circuit_ancilla = example_generator_ancilla.construct_circuit(None, False)
	circuit_not_ancilla = example_generator_ancilla.construct_circuit(None, False)
	
	init_parameters_ancilla = np.random.normal(np.pi / 2, .1, circuit_ancilla.num_parameters)
	
	# canviem les dimensions dels parametres perque coincideixin amb les dimensions necessaries pel circuit
	# sense qubits ancilla
	not_ancilla_list = init_parameters_ancilla.tolist()[
	circuit_ancilla.num_parameters - circuit_not_ancilla.num_parameters:]
	init_parameters_not_ancilla = np.array(not_ancilla_list)
	
	example_discriminator = discriminator_ancilla = \
		ClassicalDiscriminator(sizes=[4, 16, 8, 1], type_loss="minimax")
	
	init_biases_discriminator, init_weights_discriminator = example_discriminator.init_parameters()
	
	
	def to_train(num_epochs):
		# definicio d'un discriminador pel model amb la funcio no-lineal i una altre pel model que no la te
		discriminator_ancilla = \
			ClassicalDiscriminator(sizes=[4, 16, 8, 1], type_loss="minimax")
		discriminator_not_ancilla = \
			ClassicalDiscriminator(sizes=[4, 16, 8, 1], type_loss="minimax")
	
		# es defineix un generador que te la funcio no-lienal integrada en el circuit
		generator_ancilla = \
			QuantumGenerator(num_qubits=4,
						  	 generator_circuit=None,
							 num_qubits_ancilla=2,  # IMPORTANT
						 	 shots=4096)
		# i un altre generador que no te la funcio no-lineal
		generator_not_ancilla = \
			QuantumGenerator(num_qubits=2,
	                         generator_circuit=None,
							 num_qubits_ancilla=0,  # IMPORTANT
							 shots=4096)
	
		quantum_gan_ancilla = \
			Quantum_GAN(generator_ancilla, discriminator_ancilla)
		time.sleep(1)
		quantum_gan_not_ancilla = \
			Quantum_GAN(generator_not_ancilla, discriminator_not_ancilla)
		print(quantum_gan_not_ancilla.path)
		print(quantum_gan_ancilla.path)
	
		# abans de comencar a entrenar el model s'ha de canviar els parametres als definits anteriorment
		quantum_gan_ancilla.generator.parameter_values = init_parameters_ancilla
		quantum_gan_ancilla.discriminator.weights = init_weights_discriminator
		quantum_gan_ancilla.discriminator.biases = init_biases_discriminator
		quantum_gan_ancilla.train(num_epochs, train_data, batch_size, .1, .1, False)
	
		quantum_gan_not_ancilla.generator.parameter_values = init_parameters_not_ancilla
		quantum_gan_not_ancilla.discriminator.weights = init_weights_discriminator
		quantum_gan_not_ancilla.discriminator.biases = init_biases_discriminator
		quantum_gan_not_ancilla.train(num_epochs, train_data, batch_size, .1, .1, False)
	
		# una vegada ha acabat l'optimitzacio es creen els grafics per compara l'eficiencia del models
		quantum_gan_not_ancilla.plot()
	quantum_gan_ancilla.plot()
	
	
	def main():
		# Crear una queue para compartir les dades entre els processos
		# Crear i iniciar el proces de simulacio
		jobs = []
		for epoch in list_epochs:
			simulate = multiprocessing.Process(None, to_train, args=(epoch,))
			jobs.append(simulate)
			time.sleep(2)
			simulate.start()
	
	
	if __name__ == '__main__':
		main()
\end{lstlisting}
