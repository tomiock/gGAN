\chapter{Més àlgebra lineal}
Ja he escrit bastants pàgines sobre àlgebra lineal, però aparentment no eren les suficients perquè estic content amb el treball\footnote{Hi han moltes coses guays i interessants que vull explicar.}, així que aquí hi ha més àlgebra lineal.


\section{Procediment de Gram–Schmidt}\label{gram}
El procediment de Gram-Schmidt és un mètode utilitzat per produir bases per a espais vectorials\cite{QCandQI:GramSchmidt}. Per un espai $V$ amb producte interior de $d$ dimensions amb el set de vectors ${\ket{v_1}, \cdots, \ket{v_d}}$, podem definir una nova base de vectors ortonormals $\{\ket{u}\}$. El primer element d'aquest set és $\ket{u_1} = \ket{v_1}/\norm{\ket{v_1}}$, amb el següent element $\ket{v_{k+1}}$ sent:
$$
\ket{u_{k+1}} = \frac{\ket{v_{k+1}} - \sum_{i=1}^{k} \bra{u_i}\ket{v_{k+1}}\ket{u_i}}{\norm{\ket{v_{k+1}} - \sum_{i=1}^{k} \bra{u_i}\ket{v_{k+1}}\ket{u_i}}}
$$
Per $k$ en el interval $1 \leq k \leq d-1$.

Si seguim per $k$ en $1 \leq k \leq d-1$, obtenim el set de vectors ${\ket{u_1}, \cdots , \ket{u_d}}$ que es una base vàlida per l'espai ortonormal per l'estai $V$. Els vectors creats han de tindre el mateix span\footnote{L'span d'un set de vectors són totes les combinacions lineals possibles amb aquests vectors.} que el dels vectors que originalment eren la base per $V$:
$$
\text{span}(\{\ket{v}\}) = \text{span}(\{\ket{v}\}) = V
$$
Cal notar que l'span de del set base és la definició del espai. En altres paraules, cada vector en $V$ pot ser representat per una combinació del vectors base. 

La prova de que és una base ortonormal és bastant simple:
Podem veure immediatament que els elements  de $\{ \ket{u} \}$ són vectors unitaris perquè estan normalitzats (els vectors $\ket{v_{k+1}} - \sum_{i=1}^{k} \bra{u_i}\ket{v_{k+1}}\ket{u_i}$ estan dividits per la seva norma). També podem veure que són ortogonals els uns als altres mirant que el producte interior entre els doni 0:

Per $k=1$:
$$
\ket{u_2} = \frac{\ket{v_2} - \bra{u_1}\ket{v_2}\ket{u_1}}{\norm{\ket{v_2} - \bra{u_1}\ket{v_2}\ket{u_1}}}
$$
Per tant el producte interior amb $\ket{v_1}$ és:
$$
\begin{aligned}
\bra{u_1}\ket{u_2} 
&= \bra{u_1} \left( \frac{\ket{v_2} - \bra{u_1}\ket{v_2}\ket{u_1}}{\norm{\ket{v_2} - \bra{u_1}\ket{v_2}\ket{u_1}}} \right) \\
&= \frac{\bra{u_1}\ket{v_2} - \bra{u_1}\ket{v_2}\bra{u_1}\ket{u_1}}{\norm{\ket{v_2} - \bra{u_1}\ket{v_2}\ket{u_1}}} \\
&= 0
\end{aligned}
$$
Per inducció podem veure que per $j \leq d$, amb $d$ sent la dimensió del espai vectorial:
$$
\begin{aligned}
	\bra{u_j}\ket{u_{n+1}} 
	& = \bra{u_j} \left( \frac{\ket{v_{n+1}} - \sum_{i=1}^{n} \bra{u_i}\ket{v_{n+1}}\ket{u_i}}{\norm{\ket{v_{n+1}} - \sum_{i=1}^{n} \bra{u_i}\ket{v_{n+1}}\ket{u_i}}} \right) \\
	& = \frac{\bra{u_j}\ket{v_{n+1}} - \sum_{i=1}^{n} \bra{u_i}\ket{v_{n+1}} \bra{u_j}\ket{u_i}}{\norm{\ket{v_{n+1}} - \sum_{i=1}^{n} \bra{u_i}\ket{v_{n+1}} \ket{u_i}}} \\
	& = \frac{\bra{u_j}\ket{v_{n+1}} - \sum_{i=1}^{n} \bra{u_i}\ket{v_{n+1}} \delta_{ij}}{\norm{\ket{v_{n+1}} - \sum_{i=1}^{n} \bra{u_i}\ket{v_{n+1}} \ket{u_i}}} \\
	& = \frac{\bra{u_j}\ket{v_{n+1}} - \bra{u_j}\ket{v_{n+1}}}{\norm{\ket{v_{n+1}} - \sum_{i=1}^{n} \bra{u_i}\ket{v_{n+1}} \ket{u_i}}} \\
	& = 0	\\
\end{aligned}
$$
Tot això no és veu molt clar al principi però cal recordar que el producte interior de dos vectors ortonormals és zero, i que el producte interior entre el mateix vector unitari és un.

\section{Curs ràpid de la notació de Dirac}
A la taula següent hi ha un resum de conceptes matemàtics de l'àlgebra lineal importants expressats en la notació de Dirac\footnote{La notació utilitzada per un espai vectorial complex i l'espai dels nombres complex no són de la notació de Dirac estàndard, però les poso per explicar el que signifiquen.} \cite{QCandQI:dirac_notation}

\begin{tabular}{ p{2cm}|p{12cm} }
	\hline
	Notació & Descripció\\
	\hline
	\hline
	$z$ & Nombre complex    \\
	$z^{*}$ & Conjugat complex d'un nombre complex $z$. $(a+ bi)^{*} = (a -bi)$\\
	$\ket{\psi} $ & Vector amb una etiqueta $\psi$. Conegut com \textit{ket}\\
	$\ket{\psi}^T$ & Transposada d'un vector $\ket{\psi}$ \\
	$\ket{\psi}^\dag $ &  Conjugat Hermitià d'un vector. $\ket{\psi}^\dag = (\ket{\psi}^T)^* $\\
	$\bra{\psi} $ & Vector dual a $\ket{\psi}$. $ \ket{\psi} = \bra{\psi}^{\dag}$ i $\bra{\psi} = \ket{\psi}^\dag$. Conegut com \textit{bra}\\
	$ \bra{\varphi}\ket{\psi} $ & Producte interior dels vectors $\bra{\varphi}$ i $\ket{\psi}$ \\
	$ \ket{\varphi}\bra{\psi} $ & Producte exterior del vectors $\bra{\varphi}$ i $\ket{\psi}$ \\
	$ \ket{\psi}\otimes\ket{\varphi}$ & Producte tensorial del vectors $\ket{\varphi}$ i $\ket{\psi}$ \\
	$ 0 $ & Vector zero i operador zero \\
	$ \mathbb{I}_n $ & Matriu identitat de dimensions $n\times n$ \\
	$ \mathbb{C}_n $ & Espai vectorial complex de dimensió $n$ \\
	$ \mathbb{C}_1$ o $\mathbb{C} $ & Espai del nombres complexos \\
	
\end{tabular}
\section{Més on la traça parcial}



\chapter{Computació Quàntica vs Mecànica Quàntica}
En la introducció havia mencionat que una de les raons per les quals havia començat a aprendre i recercar sobre computació quàntica era perquè era fàcil, en aquest apèndix explicaré exactament a que em refereixo per això amb un exemple pràctic. Dic que és fàcil, perquè si ho és, si es compara la computació quàntica amb la mecànica quàntica. Presentaré un exemple pràctic per il·lutrar-lo.  

En mecànica quàntica la manera més usual de representar els estats quàntics, com els orbitals d'un àtom d'hidrogen, és a través de \textit{wavefunctions} o funcions d'ona, no es solen representar mitjançant vectors d'estat. Aquestes funcions d'ona són molt útils i poden representar casos més generals que els vectors d'estat. No obstant, treballar amb elles és molt més complicat degut a que són funcions que depenen del temps, no son vectors. Això implica que s'han d'utilitzar altres operacions per normalitzar les funcions, determinar com evolucionen en el temps o per fer mesures. A continuació faré una comparació entre normalitzar una funció d'ona i un vectors d'estat.

\section{Normalitzar}
Degut a l'interpretació probabilística dels vectors d'estat i de les funcions d'ona, aquests objectes han de ser normalitzats perquè la suma de la probabilitats del possibles estats de mesura sigui $1$. 

Per una funció d'ona $\Psi(x,t)$ que representa una partícula, la probabilitat de trobar aquesta partícula en un punt $x$ és $\abs{\Psi(x,t)}^2$. Per tant, la funció d'ona ha de ser normalitza seguint la fórmula: 
\begin{equation}
\int_{-\infty}^{+\infty}\abs{\Psi(x,t)}^2\, dx\ = 1
\label{eq:norm_wave}
\end{equation}

Degut a que la funció d'ona evoluciona a través del temps d'acord amb l'equació de Schrödinger, veure la figura \ref{fig:schro}, qualsevol solució d'aquesta equació ha d'estar també normalitzada. En altres paraules, aquesta equació ha de preservar la normalització de les funcions d'ona \cite{IntroQM:normalizing}.
\begin{figure}
	$$
	i\hslash \pdv{\Psi}{t} = -\frac{\hslash^2}{2m}\pdv[2]{\Psi}{x}+V\Psi
	$$
	\caption{\textbf{Equació de Schrödinger}. On $\hslash$ és $h/2\pi$, i $V$ és una funció potencial d'energia.}
	\label{fig:schro}
\end{figure}

Podem provar que aquesta equació preserva la fórmula \ref{eq:norm_wave}, començant per la igualtat trivial:
$$
\dv{t} \int_{-\infty}^{+\infty}\abs{\Psi(x,t)}^2\, dx\ = \pdv{t} \int_{-\infty}^{+\infty}\abs{\Psi(x,t)}^2\, dx
$$
Per la regla del producte tenim que \footnote{A partir d'ara escriuré  $\Psi(x,t)$ simplement com $\Psi$ per no fer les equacions tan enrevessades.}:
$$
\pdv{t}\abs{\Psi}^2 = \pdv{t}(\Psi\Psi^*) = \Psi^*\pdv{\Psi}{t} + \pdv{\Psi^*}{t}\Psi
$$
Ara l'equació de Schrödinger diu que
$$
\pdv{\Psi}{t} = \frac{i\hslash}{2m}\pdv[2]{\Psi}{x} -  \frac{i}{\hslash}V\Psi
$$
després calculant el complex conjugat tenim que 
$$
\pdv{\Psi^*}{t} = - \frac{i\hslash}{2m}\pdv[2]{\Psi^*}{x} + \frac{i}{\hslash}V\Psi^*
$$
per tant
$$
\pdv{t}\abs{\Psi}^2 = \frac{i\hslash}{2m} \left( \Psi^*\pdv[2]{\Psi}{x} - \pdv[2]{\Psi^*}{x}\Psi \right) = \pdv{x} \left[ \frac{i\hslash}{2m} \left(\Psi^*\pdv{\Psi}{x} - \pdv{\Psi^*}{x}\Psi\right)\right]
$$
finalment podem avaluar l'integral del principi:
$$
\dv{t} \int_{-\infty}^{+\infty}\abs{\Psi(x,t)}^2\, dx\ = \frac{i\hslash}{2m} \left( \Psi^*\pdv{\Psi}{x} - \pdv{\Psi^*}{x}\Psi \right) \Big|_{-\infty}^{+\infty}
$$
Degut a que $\Psi(x,t)$ ha de convergir a zero quan $x$ va cap a infinit, es veritat que:
$$
\dv{t} \int_{-\infty}^{+\infty}\abs{\Psi(x,t)}^2\, dx\ = 0
$$
Es pot veure que l'integral es constant i per tant quan $\Psi$ es normalitzada a $t=0$, es queda d'aquesta manera per qualsevol $t$ (positiu es clar). 

\chapter{Polarització d'un fotó}
\label{appendix:optics}
En l'equació \ref{eq:photon_state} he excluit el concepte de fase, que determina el tipus de polarització que té un fotó. Hi han 3 tipus:
\begin{enumerate}
	\item \textbf{Linear:}
	Un fotó té polarització lineal quan els angles de la fase $\alpha_x$, $\alpha_y$ en els estats base $\ket{x},\ket{y}$ són iguals:
	\begin{align*}
		\ket{\nearrow} &= \cos(\theta) e^{i\alpha_x}\ket{x} + \cos(\theta)e^{i\alpha_y}\ket{y} \\
		&= [\cos(\theta)\ket{x} + \sin(\theta)\ket{y}]e^{i\alpha}
	\end{align*}
	On $\alpha=\alpha_x=\alpha_y$.
	\item \textbf{Circular:}
	Quan els angles $\alpha_x$, $\alpha_y$ son separats per exactament $\frac{\pi}{2}$ i la amplitud per les dos bases és la mateixa:
	\begin{align*}
		\ket{\nearrow} &= \frac{1}{\sqrt{2}}\cos(\theta)e^{i\alpha_x}\ket{x} \pm i\frac{1}{\sqrt{2}}\sin(\theta)e^{i\alpha_y}\ket{y} \\
					   &= [\cos(\theta)e^{i\alpha_x}\ket{x} \pm i\sin(\theta)e^{i\alpha_y}\ket{y}]\frac{1}{\sqrt{2}}
	\end{align*}
	On el signe $\pm$ indica la diferencia entre la diferencia entre la polarització circular cap a la dreta o la esquerra, amb $+$ i $-$, respectivament.
	\item \textbf{El·líptica:}
	On els angles $\alpha_x$, $\alpha_y$ son diferents per una quantitat arbitraria\footnote{Però que no sigui la quantitat que dona a terme la polarització circular.}:
	$$
	\ket{\nearrow} = \cos(\theta)e^{i\alpha_x}\ket{x} + \sin(\theta)e^{i\alpha_y}\ket{y}
	$$
	Aquest és el cas més general.
	
\end{enumerate}

\chapter{Complexitat i algoritmes quàntics}
\label{complexity}
En ciència de la computació existeix el concepte de \textit{Big-O Notation}, una forma d'expressar lo eficients que són els algoritmes per fer certes tasques, en altres paraules la complexitat dels algoritmes. Bàsicament es una forma de classificar-los segon la rapidesa que tenen en ver la tasca que els correspon, aquesta rapidesa no és mesura en segons, degut a que aquesta mètrica pot variar d'ordinador a ordinador per les diferencies en hardware que aquest poden tindre. En canvi es mesure en nombre d'operacions o temps directament, però sense unitats. 

La \textit{Big-O Notation} consisteixes en definir el temps màxim que necessita un algoritme, es denota com $O(\cdot)$ on l'argument usualment depèn de $n$ que és la mida del input al algoritme, per exemple un algoritme de cerca ha de cercar a través de $n$ coses. Com a un exemple més concret tenim que un l'algoritme de cerca de cadenes binaries corre en un temps $O(\log_{2} n)$, on $n$ és el nombre de cadenes entre les quals ha de cercar. Recorda que la notació $O(\cdot)$ és el màxim, es a dir es \textit{upper bounded}, això significa que $\log_{2} n$ és la quantitat de temps més gran en la que es troba la cadena, també es possible que es trobi-s'hi a la primera comprovació que es va\footnote{Que la primera cadena que es cerca, és la que s'ha de trobar.}, llavors l'algoritme acabaria en un temps $O(1)$. Simplement és una manera de mirar lo eficients que són els algoritmes en relació a la mida del input que tenen.

Amb aquesta notació tenim una manera de comparar la eficiència que tenen els algoritmes quàntics amb la del clàssics que tenen la mateixa funció. Per il·lustrar aquesta avantatge en eficiència que presenten els algoritmes quàntics, presentaré a continuació els dos algoritmes quàntics més famosos, el de Grover, i el de Shor. Que serveixen per la cerca d'un element en una llista desordenada, i per la factorització en nombres prims respectivament. 

\section{Algoritme de Grover}
Al 1996, Lov Grover va presentar un algoritme quàntic per cercar en dades desordenades \cite{Grover_96} (e.g. cercar el número de telèfon d'una persona en una llista desordenada). Per aquest problema un algoritme clàssic té una complexitat de $O(N)$ cerques\footnote{Una cerca és quan es verifica si un element de la llista és l'element que es cerca.}, mentre que l'algoritme de Grover té una complexitat de $O(\sqrt{N})$, sent substancialment més eficient. En les paraules de Grover \cite{Grover_96} (adaptades), un ordinador clàssic per tindre un probabilitat de $\frac{1}{2}$ de trobar el número de telèfon d'una persona en una llista desordenada necessita mirar a un mínim de $\frac{N}{2}$ números, mentre que amb el seu s'obté el número de telèfon en només $O(\sqrt{N})$ passos\footnote{Per passos entenc que es refereix al nombre de vegades que es mira al oracle, es a dir, el nombre que de vegades que es verifica si s'ha trobar l'element que es cerca.}.

L'algoritme funciona de la següent manera:
Agafar un sistema de $n$ qubits en l'estat $\ket{0}$ que resulten en una combinació de $N = 2^n$ estats. Aplicar una distribució uniforme als qubits, es a dir, aplicar portes Hadamard a tots els qubits, tenint al final un estat resultant $\ket{s}$:
\begin{equation*}
	\ket{s} = \frac{1}{\sqrt{N}} \sum_{x=0}^{2^n-1} \ket{x}
\end{equation*}
Repetir $\approx \frac{\pi}{4}\sqrt{N}$ vegades, el operador de Grover $G$, que consisteix en el següent conjunt d'instruccions:
\begin{algorithmic}[1]
	\State{Aplicar el \textit{Oracle} $U_{\omega}$}
	\State{Aplicar portes Hadamard a tots els qubits}
	\State{Aplicar el \textit{Grover diffusion operator} $U_{s} = 2\ket{s}\bra{s} - I$}
	\State{Aplicar una altre vegada les portes Hadamard}
\end{algorithmic}


El \textit{oracle} és un tipus de funció que s'utilitza en els algoritmes de cerca, té la finalitat de reconèixer si un element és l'element que s'està cercant. En els algoritmes de cerca es contrueixen al voltant de l'\textit{oracle}, diversos algoritmes tenen diverses formes de consultar al oracle. A més a més, la quantitat de consultes a l'\textit{oracle} serveixen per quantificar la complexitat del algoritme, a més consultes, més ineficient és l'algoritme. 

Al veure el procediment de l'algoritme es pot veure l\textit{oracle} es consulta aproximadament $\frac{\pi}{4}\sqrt{N}$, vegades, d'aquesta manera resultat en una complexitat aproximada de $O(\sqrt{N})$, on $N=2^{n}$ per $n$ qubits, com ja he esmentat anteriorment. 

No entraré més en profunditat sobre aquest algoritme, perquè sinó hauré d'introduir més conceptes de computació quàntica com ara la fase d'un estat, o començar a utilitzar notació més complicada que també hauré d'explicar. Em sap greu, perquè és un algoritme que funciona d'una manera bonica de veure. També em sembla el millor algoritme per poder explicar una de les millors característiques de la computació quàntica, tenir algoritmes que et descarten automàticament els estats dolents dintre d'una combinació d'estats, d'aquesta manera lliurant un estat concret desitjat de entre la combinació. En el cas de l'algoritme de Grover aquest estat desitjat, és l'estat que està cercant l'\textit{oracle}. 

\section{Algoritme de Shor}
No explicaré la funcionalitat d'aquest algoritme degut a que és molt complex, no surt ni en els llibres de texts als quals tinc accés. Simplement en limitaré a explicar quina és la seva funció i P



\chapter{Codi}
En l'apèndix actual presentaré el codi que he utilitzat al llarg del treball. Està organitzat segons el moment en el qual he referenciat el codi en el text. 

IMPORTANT: Si hi ha text en català dintre del codi (n'hi ha en forma de comentaris), no porta accents perquè els paquet que utilitzo per formateixar el codi d'aquesta manera dintre del document no els accepta. Però si es mira el codi en el \href{https://github.com/tomiock/qGAN}{repositori del treball} es pot veure que si que té accents. No em deixa posar ni accents, ni la ce trencada, ni la ele geminada. L'error diu que no són caràcters que estan en Unicode UTF-8, quan si que estan allà, per fet que és un codi que suposadament està internacionalitzat per tots el llenguatges que s'utilitzen 'moderadament'. 

\section{Part I}
\subsection{Capítol 3}

\paragraph{Regressió lienal}
\label{lst:linear_regression}
Codi per efectuar una regressió lineal a dades que es generen al atzar en el mateix arxiu, l'utilitzo per poder generar una gràfic per il·lustrar un exemple de regressió lienal. Aquest tros de codi l'he tret de GitHub\footnote{Gist realitzat per l'usuari \textit{jimimvp}: \href{https://gist.github.com/jimimvp/05ece11fec25d5c8c009af9ba469d6c2}{link}}.

\begin{lstlisting}[language=Python, caption=Regressió lineal]
	import numpy as np
	from matplotlib import pyplot as plt
	import matplotlib
	
	font = {'family' : 'Helvetica',
		'size'   : 18}
	
	matplotlib.rc('font', **font)
	
	# generate the data
	np.random.seed(222)
	X = np.random.normal(0,1, (200, 1))
	w_target = np.random.normal(0,1, (1,1))
	# data + white noise
	y = X@w_target + np.random.normal(0, 1, (200,1))
	
	# least squares
	w_estimate = np.linalg.inv(X.T@X)@X.T@y
	y_estimate = X@w_estimate
	
	# plot the data
	plt.figure(figsize=(15,10))
	plt.scatter(X.flat, y_estimate.flat, label="Prediccio")
	plt.scatter(X.flat, y.flat, color='red', alpha=0.4, label="Dades")
	plt.tight_layout()
	plt.title("Regressio per diferencia de quadrats")
	plt.legend()
	plt.savefig("least_squares.png")
	plt.show()
\end{lstlisting}

\section{Part II}
\subsection{Capítol 6}

\paragraph{Codi original per la xarxa neuronal clàssica}
\label{lst:disc_original}
Està extret del \href{https://github.com/mnielsen/neural-networks-and-deep-learning}{repositori de GitHub de Micheal Nielsen}, concretament del arxiu \code{network.py}.


\begin{lstlisting}[language=Python, caption=Codi original per la xarxa neuronal clàssica]
"""
network.py
~~~~~~~~~~
A module to implement the stochastic gradient descent learning
algorithm for a feedforward neural network.  Gradients are calculated
using backpropagation.  Note that I have focused on making the code
simple, easily readable, and easily modifiable.  It is not optimized,
and omits many desirable features.
"""

#### Libraries
# Standard library
import random

# Third-party libraries
import numpy as np

class Network(object):

	def __init__(self, sizes):
		"""The list ``sizes`` contains the number of neurons in the
		respective layers of the network.  For example, if the list
		was [2, 3, 1] then it would be a three-layer network, with the
		first layer containing 2 neurons, the second layer 3 neurons,
		and the third layer 1 neuron.  The biases and weights for the
		network are initialized randomly, using a Gaussian
		distribution with mean 0, and variance 1.  Note that the first
		layer is assumed to be an input layer, and by convention we
		won't set any biases for those neurons, since biases are only
		ever used in computing the outputs from later layers."""
		self.num_layers = len(sizes)
		self.sizes = sizes
		self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
		self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]

	def feedforward(self, a):
		"""Return the output of the network if ``a`` is input."""
		for b, w in zip(self.biases, self.weights):
			a = sigmoid(np.dot(w, a)+b)
		return a

	def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):
		"""Train the neural network using mini-batch stochastic
		gradient descent.  The ``training_data`` is a list of tuples
		``(x, y)`` representing the training inputs and the desired
		outputs.  The other non-optional parameters are
		self-explanatory.  If ``test_data`` is provided then the
		network will be evaluated against the test data after each
		epoch, and partial progress printed out.  This is useful for
		tracking progress, but slows things down substantially."""
		if test_data: n_test = len(test_data)
		n = len(training_data)
		for j in xrange(epochs):
			random.shuffle(training_data)
			mini_batches = [training_data[k:k+mini_batch_size] for k in xrange(0, n, mini_batch_size)]
			for mini_batch in mini_batches:
				self.update_mini_batch(mini_batch, eta)
			if test_data:
				print "Epoch {0}: {1} / {2}".format(j, self.evaluate(test_data), n_test)
			else:
				print "Epoch {0} complete".format(j)

	def update_mini_batch(self, mini_batch, eta):
		"""Update the network's weights and biases by applying
		gradient descent using backpropagation to a single mini batch.
		The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``
		is the learning rate."""
		nabla_b = [np.zeros(b.shape) for b in self.biases]
		nabla_w = [np.zeros(w.shape) for w in self.weights]
		for x, y in mini_batch:
			delta_nabla_b, delta_nabla_w = self.backprop(x, y)
			nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
			nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
		self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]
		self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]

	def backprop(self, x, y):
		"""Return a tuple ``(nabla_b, nabla_w)`` representing the
		gradient for the cost function C_x.  ``nabla_b`` and
		``nabla_w`` are layer-by-layer lists of numpy arrays, similar
		to ``self.biases`` and ``self.weights``."""
		
		nabla_b = [np.zeros(b.shape) for b in self.biases]
		nabla_w = [np.zeros(w.shape) for w in self.weights]
		# feedforward
		activation = x
		activations = [x] # list to store all the activations, layer by layer
		zs = [] # list to store all the z vectors, layer by layer
		for b, w in zip(self.biases, self.weights):
			z = np.dot(w, activation)+b
			zs.append(z)
			activation = sigmoid(z)
			activations.append(activation)
			
		# backward pass
		delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])
		nabla_b[-1] = delta
		nabla_w[-1] = np.dot(delta, activations[-2].transpose())
		# Note that the variable l in the loop below is used a little
		# differently to the notation in Chapter 2 of the book.  Here,
		# l = 1 means the last layer of neurons, l = 2 is the
		# second-last layer, and so on.  It's a renumbering of the
		# scheme in the book, used here to take advantage of the fact
		# that Python can use negative indices in lists.
		for l in xrange(2, self.num_layers):
			z = zs[-l]
			sp = sigmoid_prime(z)
			delta = np.dot(self.weights[-l+1].transpose(), delta) * sp
			nabla_b[-l] = delta
			nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())
		return (nabla_b, nabla_w)

	def evaluate(self, test_data):
		"""Return the number of test inputs for which the neural
		network outputs the correct result. Note that the neural
		network's output is assumed to be the index of whichever
		neuron in the final layer has the highest activation."""
		test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]
		return sum(int(x == y) for (x, y) in test_results)

	def cost_derivative(self, output_activations, y):
		"""Return the vector of partial derivatives \partial C_x /
		\partial a for the output activations."""
		return (output_activations-y)

#### Miscellaneous functions
def sigmoid(z):
	"""The sigmoid function."""
	return 1.0/(1.0+np.exp(-z))

def sigmoid_prime(z):
	"""Derivative of the sigmoid function."""
	return sigmoid(z)*(1-sigmoid(z))
\end{lstlisting}

\paragraph{Codi final per el discriminador}
\label{lst:disc_final}
Aquest codi es pot trobar al \href{https://github.com/tomiock/qGAN}{repositori del treball} en l'arxiu \code{discriminator.py}. 

\begin{lstlisting}[language=Python, caption=Codi final pel discriminador]
"""DISCRIMINATOR"""
import json
from typing import Dict, List

import numpy as np

from quantumGAN.functions import BCE_derivative, minimax_derivative_fake, minimax_derivative_real, sigmoid, sigmoid_prime


def load(filename):
	f = open(filename, "r")
	data = json.load(f)
	f.close()
	# cost = getattr(sys.modules[__name__], data["cost"])
	net = ClassicalDiscriminator_that_works(data["sizes"], data["loss"])
	net.weights = [np.array(w) for w in data["weights"]]
	net.biases = [np.array(b) for b in data["biases"]]
	return net


class ClassicalDiscriminator_that_works:

	def __init__(	self,
					sizes: List[int],
					type_loss: str) -> None:

		self.num_layers = len(sizes)
		self.sizes = sizes
		self.type_loss = type_loss
		self.data_loss = {"real": [], "fake": []}
		self.ret: Dict[str, any] = {"loss": [],
			"label real": [],
			"label fake": [],
			"label fake time": [],
			"label real time": []}
		self.biases = [np.random.randn(y, ) for y in sizes[1:]]
		self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]

	def feedforward(self, a):
		"""Return the output of the network if ``a`` is input."""
		for b, w in zip(self.biases, self.weights):
a = sigmoid(np.dot(w, a) + b)
return a

def predict(self, x):
# feedforward
activation = x
zs = []  # list to store all the z vectors, layer by layer
for b, w in zip(self.biases, self.weights):
z = np.dot(w, activation) + b
zs.append(z)
activation = sigmoid(z)
return activation

def evaluate(self, test_data):

test_results = [(np.argmax(self.feedforward(x)), y)
for (x, y) in test_data]
return sum(int(x == y) for (x, y) in test_results)


def forwardprop(self, x: np.ndarray):
activation = x
activations = [x]  # list to store all the activations, layer by layer
zs = []  # list to store all the z vectors, layer by layer
for b, w in zip(self.biases, self.weights):
z = np.dot(w, activation) + b
zs.append(z)
activation = sigmoid(z)
activations.append(activation)
return activation, activations, zs

def backprop_bce(self, image, label):
"""Return a tuple ``(nabla_b, nabla_w)`` representing the
gradient for the cost function C_x.  ``nabla_b`` and
``nabla_w`` are layer-by-layer lists of numpy arrays, similar
to ``self.biases`` and ``self.weights``."""
nabla_b = [np.zeros(b.shape) for b in self.biases]
nabla_w = [np.zeros(w.shape) for w in self.weights]

# feedforward and back error calculation depending on type of image
activation, activations, zs = self.forwardprop(image)
delta = BCE_derivative(activations[-1], label) * sigmoid_prime(zs[-1])

# backward pass
nabla_b[-1] = delta
nabla_w[-1] = np.dot(delta, activations[-2].reshape(1, activations[-2].shape[0]))

for l in range(2, self.num_layers):
z = zs[-l]
delta = np.dot(self.weights[-l + 1].transpose(), delta) * sigmoid_prime(z)
nabla_b[-l] = delta
nabla_w[-l] = np.dot(delta.reshape(delta.shape[0], 1), activations[-l - 1].reshape(1, activations[-l - 1].shape[0]))
return nabla_b, nabla_w, activations[-1]

def backprop_minimax(self, real_image, fake_image, is_real):
"""Return a tuple ``(nabla_b, nabla_w)`` representing the
gradient for the cost function C_x.  ``nabla_b`` and
``nabla_w`` are layer-by-layer lists of numpy arrays, similar
to ``self.biases`` and ``self.weights``."""
nabla_b = [np.zeros(b.shape) for b in self.biases]
nabla_w = [np.zeros(w.shape) for w in self.weights]

# feedforward and back error calculation depending on type of image
activation_real, activations_real, zs_real = self.forwardprop(real_image)
activation_fake, activations_fake, zs_fake = self.forwardprop(fake_image)

if is_real:
delta = minimax_derivative_real(activations_real[-1]) * sigmoid_prime(zs_real[-1])
activations, zs = activations_real, zs_real
else:
delta = minimax_derivative_fake(activations_fake[-1]) * sigmoid_prime(zs_fake[-1])
activations, zs = activations_fake, zs_fake

# backward pass
nabla_b[-1] = delta
nabla_w[-1] = np.dot(delta, activations[-2].reshape(1, activations[-2].shape[0]))

for l in range(2, self.num_layers):
z = zs[-l]
delta = np.dot(self.weights[-l + 1].transpose(), delta) * sigmoid_prime(z)
nabla_b[-l] = delta
nabla_w[-l] = np.dot(delta.reshape(delta.shape[0], 1),
activations[-l - 1].reshape(1, activations[-l - 1].shape[0]))
return nabla_b, nabla_w, activations[-1]

def train_mini_batch(self, mini_batch, learning_rate):
global label_real, label_fake
nabla_b = [np.zeros(b.shape) for b in self.biases]
nabla_w = [np.zeros(w.shape) for w in self.weights]

if self.type_loss == "binary cross entropy":
for real_image, fake_image in mini_batch:
delta_nabla_b, delta_nabla_w, label_real = self.backprop_bce(real_image, np.array([1.]))
nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]

delta_nabla_b, delta_nabla_w, label_fake = self.backprop_bce(fake_image, np.array([0.]))
nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]

elif self.type_loss == "minimax":
for real_image, fake_image in mini_batch:
delta_nabla_b, delta_nabla_w, label_real = self.backprop_minimax(real_image, fake_image, True)
nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]

delta_nabla_b, delta_nabla_w, label_fake = self.backprop_minimax(real_image, fake_image, False)
nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
else:
raise Exception("type of loss function not valid")

# gradient descent
# nabla_w and nabla_b are multiplied by the learning rate
# and taken the mean of (dividing by the mini batch size)
self.weights = [w - (learning_rate / len(mini_batch)) * nw
for w, nw in zip(self.weights, nabla_w)]
self.biases = [b - (learning_rate / len(mini_batch)) * nb
for b, nb in zip(self.biases, nabla_b)]
\end{lstlisting}

\paragraph{Codi per el generador}
\label{lst:gen_final}
Aquest codi es pot trobar al \href{https://github.com/tomiock/qGAN}{repositori del treball} en l'arxiu \code{generador.py}. 

\begin{lstlisting}[language=Python, caption=Codi final pel generador]
"""QUANTUM GENERATOR"""

from typing import Any, Dict, Optional, cast

import numpy as np
import qiskit
from qiskit import ClassicalRegister, QuantumRegister
from qiskit.circuit import QuantumCircuit
from qiskit.circuit.library import TwoLocal
from qiskit.providers.aer import AerSimulator

from quantumGAN.functions import create_entangler_map, create_real_keys, minimax_generator


class QuantumGenerator:

	def __init__(
			self,
			shots: int,
			num_qubits: int,
			num_qubits_ancilla: int,
			generator_circuit: Optional[QuantumCircuit] = None,
			snapshot_dir: Optional[str] = None
	) -> None:

		super().__init__()
		# passar els arguments de la classe a metodes en de classes
		# d'aquesta manera son accessibles per qualsevol funcio dintre de la classe
		self.num_qubits_total = num_qubits
		self.num_qubits_ancilla = num_qubits_ancilla
		self.generator_circuit = generator_circuit
		self.snapshot_dir = snapshot_dir
		self.shots = shots
		self.discriminator = None
		self.ret: Dict[str, Any] = {"loss": []}
		self.simulator = AerSimulator()

	def init_parameters(self):
		"""Inicia els parametres inicial i crea el circuit al qual se li posen els parametres"""
		# iniciacia dels parametres inicials i dels circuits al qual posar aquests parametres
		self.generator_circuit = self.construct_circuit(latent_space_noise=None, to_measure=False)
		# s'ha de crear primer el circuit perque d'aquesta manera es pot saber el nombre de parametres que es necessiten
		self.parameter_values = np.random.normal(np.pi / 2, .1, self.generator_circuit.num_parameters)

	def construct_circuit(self,
						latent_space_noise,
						to_measure: bool):
		"""Crea el circuit quantic des de zero a partir de diversos registres de qubits"""
		if self.num_qubits_ancilla is 0:
			qr = QuantumRegister(self.num_qubits_total, 'q')
			cr = ClassicalRegister(self.num_qubits_total, 'c')
			qc = QuantumCircuit(qr, cr)
		else:
			qr = QuantumRegister(self.num_qubits_total - self.num_qubits_ancilla, 'q')
			anc = QuantumRegister(self.num_qubits_ancilla, 'ancilla')
			cr = ClassicalRegister(self.num_qubits_total - self.num_qubits_ancilla, 'c')
			qc = QuantumCircuit(anc, qr, cr)

		# creacia de la part del circuit que conte la implantacio dels parametres d'input. En cas que no es donin aquests parametres es creen automaticament
		if latent_space_noise is None:
			randoms = np.random.normal(-np.pi * .01, np.pi * .01, self.num_qubits_total)
			init_dist = qiskit.QuantumCircuit(self.num_qubits_total)

			# es col.loca una porta RY en cada qubits i amb un parametre diferent cadascuna
			for index in range(self.num_qubits_total):
				init_dist.ry(randoms[index], index)
		else:
			init_dist = qiskit.QuantumCircuit(self.num_qubits_total)

			for index in range(self.num_qubits_total):
				init_dist.ry(latent_space_noise[index], index)

		# la funcio create_entagler_map crea les parelles de qubits a les qual col.locar les portes CZ
		# en funcio del nombre de qubits
		if self.num_qubits_ancilla is 0:
			entangler_map = create_entangler_map(self.num_qubits_total)
		else:
			entangler_map = create_entangler_map(self.num_qubits_total - self.num_qubits_ancilla)

		# creacio final dels circuits a partir una funcio integrada a Qiskit que va repetint les operacions
		# que se li especifiquen
		ansatz = TwoLocal(int(self.num_qubits_total), 'ry', 'cz', entanglement=entangler_map, reps=1, insert_barriers=True)

		# aqui s'ajunten el circuit que funciona com a input amb el circuit que consisteix en la repeticio
		# de les portes RY i CZ
		qc = qc.compose(init_dist, front=True)
		qc = qc.compose(ansatz, front=False)

		if to_measure:
			qc.measure(qr, cr)

		return qc

	def set_discriminator(self, discriminator) -> None:
		self.discriminator = discriminator

	def get_output(
					self,
					latent_space_noise,
					parameters: Optional[np.ndarray] = None
					):
		"""Retorna un output del generador quan se li dona un estat d'input i opcionalment uns parametres en especific. Els pixels estan compostos per la probabilitat que un qubit resulti en ket_0 en cada base. Per tant, els pixels de l'imatge estan normalitzats amb la norma l-1."""
		real_keys_set, real_keys_list = create_real_keys(self.num_qubits_total - self.num_qubits_ancilla)

		# en cas de que no es donin parametres com a input, es treuen els parametres de la variable
		# self.parameter_values. Es a dir els parametres que es creen automaticament al principi i que es van
		# actualitzant al mateix temps que el model s'optimitza
		if parameters is None:
			parameters = cast(np.ndarray, self.parameter_values)

		qc = self.construct_circuit(latent_space_noise, True)

		parameter_binds = {parameter_id: parameter_value for parameter_id, parameter_value in zip(qc.parameters, parameters)}

		# el metode bind_parametres del circuit quantic
		qc = qc.bind_parameters(parameter_binds)

		# Simulacio dels circuits mitjancant el simulador Aer de Qiskit. El nivell d'optimitzacio es zero, perque al ser circuits petits i que simulen una vegada, no es necessari. Al optimitzar el proces acaba sent mes lent.
		result_ideal = qiskit.execute(experiments=qc,
									  backend=self.simulator,
									  shots=self.shots,
									  optimization_level=0).result()
		counts = result_ideal.get_counts()

		try:
			# creacio de l'imatge resultant
			pixels = np.array([counts[index] for index in list(real_keys_list)])

		except KeyError:
			# aquesta excepcio sorgeix quan en el diccionari dels resultats no estan totes les keys pel fet que qiskit, en cas de que no hi hagi un mesurament en una base, no inclou aquesta base en el diccionari
			keys = counts.keys()
			missing_keys = real_keys_set.difference(keys)
			# s'utilitza un la resta entre dos sets per poder veure quina es la key que falta en el diccionari
			for key_missing in missing_keys:
				counts[key_missing] = 0

			# una vegada es troba les keys que faltaven es crea l'imatge resultant
			pixels = np.array([counts[index] for index in list(real_keys_list)])

		pixels = pixels / self.shots
		return pixels

	def get_output_pixels(
					 	  self,
						  latent_space_noise,
						  params: Optional[np.ndarray] = None
						  ):
		"""Retorna un output del generador quan se li dona un estat d'input i opcionalment uns parametres en especific. Cada pixel es la probabilitat de que un qubits resulti en l'estat ket_0, per tant, els valors cada pixel (que son independents entre si) es troba en l'interval (0, 1) """
		qc = QuantumCircuit(self.num_qubits_total)

		init_dist = qiskit.QuantumCircuit(self.num_qubits_total)
		assert latent_space_noise.shape[0] == self.num_qubits_total

		for num_qubit in range(self.num_qubits_total):
			init_dist.ry(latent_space_noise[num_qubit], num_qubit)

		if params is None:
			params = cast(np.ndarray, self.parameter_values)

		qc.assign_parameters(params)

		# comptes de simular els valors que donara cada qubits, es simula l'estat final del circuit i d'aquest s'extreuen els valors que es mesuraran per a cada qubit
		state_vector = qiskit.quantum_info.Statevector.from_instruction(qc)
		pixels = []
		for qubit in range(self.num_qubits_total):
			# per treure la probabilitat s'utilitza una funcio implementada en Qiskit
			pixels.append(state_vector.probabilities([qubit])[0])

		# creacio de l'imatge resultada a partir de la llista que conte el valor per a cada pixel
		generated_samples = np.array(pixels)
		generated_samples.flatten()

		return generated_samples

	def train_mini_batch(self, mini_batch, learning_rate):
		"""Optimitzacio del generador per una batch d'imatges. Retorna una batch de les imatges generades amb unes imatges reals que poder donar com a input al generador. """
	 	nabla_theta = np.zero_like(self.parameter_values.shape)
		new_images = []

		for _, noise in mini_batch:
			for index, _ in enumerate(self.parameter_values):
				perturbation_vector = np.zeros_like(self.parameter_values)
				perturbation_vector[index] = 1

				# Creacio dels parametres per generar les dues imatges
				pos_params = self.parameter_values + (np.pi / 4) * perturbation_vector
				neg_params = self.parameter_values - (np.pi / 4) * perturbation_vector

				pos_result = self.get_output(noise, parameters=pos_params)  # Generacio imatges
				neg_result = self.get_output(noise, parameters=neg_params)

				pos_result = self.discriminator.predict(pos_result)  # Assignacio de les etiquetes
				neg_result = self.discriminator.predict(neg_result)

				# Diferencia entre les avaluacions de la funcio de perdua entre les dues etiquetes
				gradient = minimax_generator(pos_result) - minimax_generator(neg_result)
				nabla_theta[index] += gradient  # Afegir derivada d'un parametre al gradient
			new_images.append(self.get_output(noise))

		for index, _ in enumerate(self.parameter_values):
			# Actualitzacio dels parametres a traves del gradient
			self.parameter_values[index] += (learning_rate / len(mini_batch)) * nabla_theta[index]

		# Creacio de la batch d'imatges a retornar
		mini_batch = [(datapoint[0], fake_image) for datapoint, fake_image in zip(mini_batch, new_images)]
		return mini_batch
\end{lstlisting}

\paragraph{Execució del model}
\label{lst:main}
Aquest codi es pot trobar al \href{https://github.com/tomiock/qGAN}{repositori del treball} en l'arxiu \code{main.py}. 

\begin{lstlisting}[language=Python, caption=Codi final pel generador]
import numpy as np

from quantumGAN.discriminator import ClassicalDiscriminator
from quantumGAN.qgan import Quantum_GAN
from quantumGAN.quantum_generator import QuantumGenerator

num_qubits: int = 3

# Set number of training epochs
num_epochs = 150
# Batch size
batch_size = 10

train_data = []
for _ in range(800):
	x2 = np.random.uniform(.55, .46, (2,))
	fake_datapoint = np.random.uniform(-np.pi * .01, np.pi * .01, (num_qubits,))
	real_datapoint = np.array([x2[0], 0, x2[0], 0])
	train_data.append((real_datapoint, fake_datapoint))

discriminator = ClassicalDiscriminator(sizes=[4, 16, 8, 1],
									   type_loss="minimax")
generator = QuantumGenerator(num_qubits=num_qubits,
							 generator_circuit=None,
							 num_qubits_ancilla=1,
							 shots=4096)

quantum_gan = Quantum_GAN(generator, discriminator)
print(quantum_gan)
print(num_epochs)
quantum_gan.generator.get_output(fake_datapoint[0], None)
quantum_gan.train(num_epochs, train_data, batch_size, .1, .1, True)

quantum_gan.plot()
quantum_gan.create_gif()
quantum_gan.save()

\end{lstlisting}


\paragraph{Multiprocessament}
Per poder ser més eficient alhora de executar els models, vaig decidir utilitzar una característica molt útil de Python, el \textit{multiprocessing}. Usualment una instancia de Python, es a dir; un arxiu executant-se, només utilitza un nucli del processador a la vegada, però amb \textit{multiprocessing} pots utilitzar múltiples nuclis amb un mateix arxiu, d'aquesta manera executant diverses línies de codi al mateix temps. Llavors vaig 


