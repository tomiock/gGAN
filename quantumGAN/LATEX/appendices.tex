\chapter{Més àlgebra lienal}
Ja he escrit bastants pàgines sobre àlgebra lineal, però aparentment no eren les suficients perquè estic content amb el treball\footnote{Hi han moltes coses guays i interessants que vull explicar.}, així que aquí hi ha més àlgebra lineal.


\section{Procediment de Gram–Schmidt}\label{gram}
The Gram–Schmidt procedure is a method used to produce orthonormal basis for a vectors space \cite{QCandQI:GramSchmidt}. For a $d$-dimensional inner product vector space $V$ with a basis vectors set $\ket{v_1}, \cdots, \ket{v_d}$, we can define a new orthonormal basis set $\{\ket{u}\}$. The first element of that set is $\ket{u_1} = \ket{v_1}/\norm{\ket{v_1}}$, with the following element $\ket{v_{k+1}}$  being:
$$
\ket{u_{k+1}} = \frac{\ket{v_{k+1}} - \sum_{i=1}^{k} \bra{u_i}\ket{v_{k+1}}\ket{u_i}}{\norm{\ket{v_{k+1}} - \sum_{i=1}^{k} \bra{u_i}\ket{v_{k+1}}\ket{u_i}}}
$$
For $k$ in the interval $1 \leq k \leq d-1$.

If we follow the above for each $k$ in $1 \leq k \leq d-1$, we obtain the new vector set $\ket{u_1}, \cdots , \ket{u_d}$ that is a valid orthonormal basis for the space $V$. The created vector set must have the same span\footnote{The span of a set of vectors is all the possible linear combinations that come from those vectors.} as the previous one to be a valid basis for the space $V$:
$$
\text{span}(\{\ket{v}\}) = \text{span}(\{\ket{v}\}) = V
$$
Note that the span of the basis set is the definition of the space. In other words, every vector in $V$ can be represented as a linear combination of the basis vectors. 

The proof that is an orthonormal basis is quite simple: 
We can see immediately that the components of $\{ \ket{u} \}$ are unit vectors because they are normalized (the vectors $\ket{v_{k+1}} - \sum_{i=1}^{k} \bra{u_i}\ket{v_{k+1}}\ket{u_i}$ are divided by their norm). And we can se that they are orthogonal by checking that the inner product of non-equal vectors in the set is 0: 

For $k=1$:
$$
\ket{u_2} = \frac{\ket{v_2} - \bra{u_1}\ket{v_2}\ket{u_1}}{\norm{\ket{v_2} - \bra{u_1}\ket{v_2}\ket{u_1}}}
$$
Then the inner product with $\ket{v_1}$ is:
$$
\begin{aligned}
\bra{u_1}\ket{u_2} 
&= \bra{u_1} \left( \frac{\ket{v_2} - \bra{u_1}\ket{v_2}\ket{u_1}}{\norm{\ket{v_2} - \bra{u_1}\ket{v_2}\ket{u_1}}} \right) \\
&= \frac{\bra{u_1}\ket{v_2} - \bra{u_1}\ket{v_2}\bra{u_1}\ket{u_1}}{\norm{\ket{v_2} - \bra{u_1}\ket{v_2}\ket{u_1}}} \\
&= 0
\end{aligned}
$$
By induction we can see that for $j \leq d$, with $d$ being the dimension of the vector space:
$$
\begin{aligned}
	\bra{u_j}\ket{u_{n+1}} 
	& = \bra{u_j} \left( \frac{\ket{v_{n+1}} - \sum_{i=1}^{n} \bra{u_i}\ket{v_{n+1}}\ket{u_i}}{\norm{\ket{v_{n+1}} - \sum_{i=1}^{n} \bra{u_i}\ket{v_{n+1}}\ket{u_i}}} \right) \\
	& = \frac{\bra{u_j}\ket{v_{n+1}} - \sum_{i=1}^{n} \bra{u_i}\ket{v_{n+1}} \bra{u_j}\ket{u_i}}{\norm{\ket{v_{n+1}} - \sum_{i=1}^{n} \bra{u_i}\ket{v_{n+1}} \ket{u_i}}} \\
	& = \frac{\bra{u_j}\ket{v_{n+1}} - \sum_{i=1}^{n} \bra{u_i}\ket{v_{n+1}} \delta_{ij}}{\norm{\ket{v_{n+1}} - \sum_{i=1}^{n} \bra{u_i}\ket{v_{n+1}} \ket{u_i}}} \\
	& = \frac{\bra{u_j}\ket{v_{n+1}} - \bra{u_j}\ket{v_{n+1}}}{\norm{\ket{v_{n+1}} - \sum_{i=1}^{n} \bra{u_i}\ket{v_{n+1}} \ket{u_i}}} \\
	& = 0	\\
\end{aligned}
$$
All of this doesn't seem straightforward at first but remember that the inner product of two orthogonal vectors is zero, and the inner product between the same unit vector is one. 

\section{Dirac Notation Crash Course}

On the following table there is a quick summary important mathematical concepts of linear algebra \cite{QCandQI:dirac_notation} expressed in Dirac Notation\footnote{The notation used for the complex vector spaces and complex number space are not standard Dirac Notation, but I included them in the table to explain what they mean.}. 
%Along the text you may encounter more complicated use of this notation. 

\begin{tabular}{ p{2cm}|p{12cm} }
	\hline
	Notation & Description \\
	\hline
	\hline
	$z$ & Complex number    \\
	$z^{*}$ & Complex conjugate of the a complex number $z$. $(a+ bi)^{*} = (a -bi)$\\
	$\ket{\psi} $ & Vector with label $\psi$. Known as \textit{ket}\\
	$\ket{\psi}^T$ & Transpose of vector $\ket{\psi}$ \\
	$\ket{\psi}^\dag $ &  Hermitian conjugate of vector. $\ket{\psi}^\dag = (\ket{\psi}^T)^* $\\
	$\bra{\psi} $ & Dual vector to $\ket{\psi}$. $ \ket{\psi} = \bra{\psi}^{\dag}$ and $\bra{\psi} = \ket{\psi}^\dag$. Known as \textit{bra}\\
	$ \bra{\varphi}\ket{\psi} $ & Inner product of vectors $\bra{\varphi}$ and $\ket{\psi}$ \\
	$ \ket{\varphi}\bra{\psi} $ & Outer product of vectors $\bra{\varphi}$ and $\ket{\psi}$ \\
	$ \ket{\psi}\otimes\ket{\varphi}$ & Tensor product of vectors $\ket{\varphi}$ and $\ket{\psi}$ \\
	$ 0 $ & Zero vector and zero operator \\
	$ \mathbb{I}_n $ & Identity matrix of dimension $n$ \\
	$ \mathbb{C}_n $ & Complex vector space of dimension $n$ \\
	$ \mathbb{C}_1$ or $\mathbb{C} $ & Complex number space \\
	
\end{tabular}
\section{More on the Partial Trace}

\chapter{Quantum Computation vs Quantum Mechanics}
On the introduction I mentioned that one of the reasons for with I started learning and researching quantum computing is because it is easy, on this appendix I am going to present why this is the case with a practical example.

On quantum mechanics the more general way to represent quantum state, like the orbitals of an atom of hydrogen, are wavefunctions, not statevectors. Wavefunctions are extremely useful, however, working with them adds a hole new level of complexity because they are continuous functions that depend on time. Compare them with vectors, which are discrete packets of information that evolve through discrete amounts of time.

\section{Normalizing}
Because the probabilistic interpretation of both statevectors and wavefunctions, these two objects have to be normalize upon measurement, thus making the total sum of probabilities $1$. How to normalize these objects is a perfect example to illustrate the difference in complexity that I see between quantum mechanics and quantum computation. 

 For a wavefunction $\Psi$ that represents a particle, the probability of finding the particle in a point $x$ is $\abs{\Psi(x,t)}^2$. Then, the wavefunction has to be normalized like the following:
\begin{equation}
\int_{-\infty}^{+\infty}\abs{\Psi(x,t)}^2\, dx\ = 1
\label{eq:norm_wave}
\end{equation}

Since the wavefunctions evolves over time with the Schrödinger equation, see Fig.\ref{fig:schro}, any solution this equation has to be normalized. Luckily, if the wavefunction is normalized at time $t=0$ it stays this way, in other words, the Schrödinger equations preserves the normalization of the wavefunction \cite{IntroQM:normalizing}. 

\begin{figure}
	$$
	i\hslash \pdv{\Psi}{t} = -\frac{\hslash^2}{2m}\pdv[2]{\Psi}{x}+V\Psi
	$$
	\caption{\textbf{Schrödinger's Equation}. Where $\hslash$ is $h/2\pi$, and $V$ is the potential energy function.}
	\label{fig:schro}
\end{figure}

We can prove that the equation, preserves \ref{eq:norm_wave}, starting from the trivial equality:
$$
\dv{t} \int_{-\infty}^{+\infty}\abs{\Psi(x,t)}^2\, dx\ = \pdv{t} \int_{-\infty}^{+\infty}\abs{\Psi(x,t)}^2\, dx
$$
By the product rule we have that\footnote{From now on I am going to write $\Psi(x,t)$ as simply $\Psi$ to not make the equations to complicated.}:
$$
\pdv{t}\abs{\Psi}^2 = \pdv{t}(\Psi\Psi^*) = \Psi^*\pdv{\Psi}{t} + \pdv{\Psi^*}{t}\Psi
$$
Now the Schrödinger equation says that 
$$
\pdv{\Psi}{t} = \frac{i\hslash}{2m}\pdv[2]{\Psi}{x} -  \frac{i}{\hslash}V\Psi
$$
then by taking the complex conjugate we have that 
$$
\pdv{\Psi^*}{t} = - \frac{i\hslash}{2m}\pdv[2]{\Psi^*}{x} + \frac{i}{\hslash}V\Psi^*
$$
so
$$
\pdv{t}\abs{\Psi}^2 = \frac{i\hslash}{2m} \left( \Psi^*\pdv[2]{\Psi}{x} - \pdv[2]{\Psi^*}{x}\Psi \right) = \pdv{x} \left[ \frac{i\hslash}{2m} \left(\Psi^*\pdv{\Psi}{x} - \pdv{\Psi^*}{x}\Psi\right)\right]
$$
finally we can evaluate the integral from the start:
$$
\dv{t} \int_{-\infty}^{+\infty}\abs{\Psi(x,t)}^2\, dx\ = \frac{i\hslash}{2m} \left( \Psi^*\pdv{\Psi}{x} - \pdv{\Psi^*}{x}\Psi \right) \Big|_{-\infty}^{+\infty}
$$
Because $\Psi(x,t)$ has to go to zero when x goes to either infinity, is true that:
$$
\dv{t} \int_{-\infty}^{+\infty}\abs{\Psi(x,t)}^2\, dx\ = 0
$$
Thus the integral is constant and went $\Psi$ is normalized at $t=0$, it stays that way for any $t$ (positive of course). \todo{falta la prueba para la normalización de un statevector}

\chapter{Polarització d'un fotó}
\label{appendix:optics}
En l'equació \ref{eq:photon_state} he exclusit el concepte de fase, que determina el tipus de polarització que té un fotó. Hi han 3 tipus:
\begin{enumerate}
	\item \textbf{Linear:}
	Un fotó té polarització lineal quan els angles de la fase $\alpha_x$, $\alpha_y$ en els estats base $\ket{x},\ket{y}$ són iguals:
	\begin{align*}
		\ket{\nearrow} &= \cos(\theta) e^{i\alpha_x}\ket{x} + \cos(\theta)e^{i\alpha_y}\ket{y} \\
		&= [\cos(\theta)\ket{x} + \sin(\theta)\ket{y}]e^{i\alpha}
	\end{align*}
	On $\alpha=\alpha_x=\alpha_y$.
	
	\item \textbf{Circular:}
	Quan els angles $\alpha_x$, $\alpha_y$ son separats per exactament $\frac{\pi}{2}$ i la amplitud per les dos bases és la mateixa:
	\begin{align*}
		\ket{\nearrow} &= \frac{1}{\sqrt{2}}\cos(\theta)e^{i\alpha_x}\ket{x} \pm i\frac{1}{\sqrt{2}}\sin(\theta)e^{i\alpha_y}\ket{y} \\
					   &= [\cos(\theta)e^{i\alpha_x}\ket{x} \pm i\sin(\theta)e^{i\alpha_y}\ket{y}]\frac{1}{\sqrt{2}}
	\end{align*}
On el signe $\pm$ indica la diferencia entre la diferencia entre la polarització circular cap a la dreta o la esquerra, amb $+$ i $-$, respectivament.

	\item \textbf{El·líptica:}
	On els angles $\alpha_x$, $\alpha_y$ son diferents per una quantitat arbitraria\footnote{Però que no sigui la quantitat que dona a terme la polarització circular.}:
	$$
	\ket{\nearrow} = \cos(\theta)e^{i\alpha_x}\ket{x} + \sin(\theta)e^{i\alpha_y}\ket{y}
	$$
	Aquest és el cas més general, és a dir la equació que s'utilitza per expressar qualsevol fase.
	
\end{enumerate}

\chapter{Complexitat i algoritmes quàntics}
\label{complexity}
En ciència de la computació existeix el concepte de \textit{Big-O Notation}, una forma d'expressar lo eficients que són els algoritmes per fer certes tasques, en altres paraules la complexitat dels algoritmes. Bàsicament es una forma de classificar-los segon la rapidesa que tenen en ver la tasca que els correspon, aquesta rapidesa no és mesura en segons, degut a que aquesta mètrica pot variar d'ordinador a ordinador per les diferencies en hardware que aquest poden tindre. En canvi es mesure en nombre d'operacions o temps directament, però sense unitats. 

La \textit{Big-O Notation} consisteixes en definir el temps màxim que necessita un algoritme, es denota com $O(\cdot)$ on l'argument usualment depèn de $n$ que és la mida del input al algoritme, per exemple un algoritme de cerca ha de cercar a través de $n$ coses. Com a un exemple més concret tenim que un l'algoritme de cerca de cadenes binaries corre en un temps $O(\log_{2} n)$, on $n$ és el nombre de cadenes entre les quals ha de cercar. Recorda que la notació $O(\cdot)$ és el màxim, es a dir es \textit{upper bounded}, això significa que $\log_{2} n$ és la quantitat de temps més gran en la que es troba la cadena, també es possible que es trobi-s'hi a la primera comprovació que es va\footnote{Que la primera cadena que es cerca, és la que s'ha de trobar.}, llavors l'algoritme acabaria en un temps $O(1)$. Simplement és una manera de mirar lo eficients que són els algoritmes en relació a la mida del input que tenen.

Amb aquesta notació tenim una manera de comparar la eficiència que tenen els algoritmes quàntics amb la del clàssics que tenen la mateixa funció. 

\section{Algoritme de Grover}
Al 1996, Lov Grover va presentar un algoritme quàntic per cercar en dades desordenades \cite{Grover_96} (e.g. cercar el número de telèfon d'una persona en una llista desordenada). Per aquest problema un algoritme clàssic té una complexitat de $O(N)$ cerques\footnote{Una cerca és quan es verifica si un element de la llista és l'element que es cerca.}, mentre que l'algoritme de Grover té una complexitat de $O(\sqrt{N})$, sent substancialment més eficient. En les paraules de Grover \cite{Grover_96} (adaptades), un ordinador clàssic per tindre un probabilitat de $\frac{1}{2}$ de trobar el número de telèfon d'una persona en una llista desordenada necessita mirar a un mínim de $\frac{N}{2}$ números, mentre que amb el seu s'obté el número de telèfon en només $O(\sqrt{N})$ passos\footnote{Per passos entenc que es refereix al nombre de vegades que es mira al oracle, es a dir el nombre que de vegades que es verifica si s'ha trobar el que es cerca.}.

L'algoritme funciona de la següent manera: 


