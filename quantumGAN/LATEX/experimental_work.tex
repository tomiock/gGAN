\chapter{Plantejament de l'hipòtesi}

Al ser l'implementació de les funcions no-lineal un assumpte lleugerament conflictiu entre els diversos models de xarxes neuronals quàntiques havia decidit des de un principi centrar-me en aquest assumpte en concret. Podria haver anat per al altres vies com la generació d'imatges amb color o l'implementació d'un d'una porta $X$ en una part especifica del circuit quàntic que genera les imatges, que al posar-la o no, el model generes dos tipus d'imatge a través del mateix circuit. Tanmateix, les dues propostes requerien de desenvolupar nous conceptes, llavors al no tindre el coneixements necessaris per poder desenvolupar-les vaig descartar aquestes opcions. 

La pregunta a investigar és la següent: \\
L'incorporació d'una funció no-lineal en el circuit quàntic del model, a partir d'una mesura parcial, causa que el model arribi més ràpidament al punt òptim?

En altres paraules, volia veure si la mesura parcial repercutia positivament en la eficiència del model, fent que la generació de les imatges desitjades es dones a terme en un menor temps. 

Sembla una qüestió senzilla, però la dificultat del experiment radica en crear la xarxa neuronal en si, amb totes les seves parts accessibles per poder fer els canvis que siguin necessaris. L'única manera de fer-ho seria programant el model. 

\chapter{Programació del model}

Posteriorment a començar a programar el model, jo ja sabia que ho havia de realitzar en Python, ja creat petits algoritmes abans de començar aquest treball i tenia experiència construint i executant circuits quàntics amb Cirq, una eina desenvolupada per Google. A més a més, sabia de l'existència de TensorFlow Quantum \cite{tfq}, una altra eina desenvolupada per Google destinada a la creació de xarxes neuronals quàntiques i algoritmes de \textit{quantum mechine learning} en general. També tenia experiència en aquest \textit{framework}. Llavors TensorFlow Quantum va ser la meva primera opció, tenia pensat basar el meu codi en el tutorial de TensorFlow sobre un xarxa convolucional generativa adverbial. Havia de canviar el generador per un circuit quàntic que s'obtimitza a partir d'un diferenciador automàtic provinent de TensorFlow Quantum. Els canvis que corresponien al discriminador simplement havien de ser un canvi d'arquitectura, passar d'una xarxa més complexa a una de més simple que només consistia en unes poques capes totalment connectades. 

El primer problema que em vaig trobar va ser la creació del \textit{data set} que alimentar a la xarxa discriminativa. A causa de la peculiaritat de les imatges que volia generar\footnote{Usualment les imatges que componen els \textit{datasets} utilitzats en \textit{deep machine learning} són extretes de bancs d'informació amb mides enormes. En el meu cas, havien de ser generades per mi, per tant havia de convertir matrius de Numpy en \textit{datasets} per TensorFlow.}. Recordo que en va costar arribar a tindre la solució a aquest problema. 

Una vegada ja tenia fet el \textit{dataset} em vaig posar a fer el model. En el tutorial per una \href{https://www.tensorflow.org/tutorials/generative/dcgan}{DCGAN (GAN conolucional)} els dos models eren entrenats per la funció \code{train\_step}. En aquesta es crida a la funció \code{tf.GradientTape} per guardar el diferenciador automàtic. El problema que vaig dintre amb aquesta funció es que directament no funcionava amb el discriminador, aquest no era optimitzat. Després de intentar solucionar l'error pels meus propis medis, mirant la causa d'aquest i de buscar a forums la solució o causa, em vaig rendir. Ja havia estat uns quants mesos intentant desenvolupant el model amb TensorFlow i TensorFlow Quantum. Havia creat les capes del generador quàntic manualment, també ho havia fet amb el optimitzador. Tenia el model gairebé acabat, però al no poder optimitzar el discriminador em vaig veure obligat a canviar d'estrateiga. 

Existeixen dos grans \textit{frameworks} per crear i executar circuits quàntics: Cirq \cite{cirq}, desenvolupat per Google i Qiskit \cite{qiskit}, creat per IBM. Una vegada vaig decidir no continuar amb Cirq, havia de provar amb Qiskit. La veritat, havia d'haber començar amb Qiskit des de el principi, és més útil (té moltes més característiques), i el més important, té una gran comunitat, per tant, es més fàcil trobar solucions als error que pots tenir i és més fàcil trobar a persones disposades a ajudar-te.

Al igual que Cirq té un \textit{framework} per poder desenvolupar xarxes neuronals, TensorFlow Quantum, Qiskit també té el seu PyTorch \cite{pytorch_2019}, no obstant no té una integració tan directa, ja que no estan desenvolupats per el mateix equip, ni la mateixa companyia. 

Per tant, al canviar de Cirq a Qiskit, també havia de canviar de TensorFlow a PyTorch, no obstant, no tenia res d'experiencia amb PyTorch, sabia que seria més complicat, i tenia raó. No vaig ni aconseguir crear el \textit{dataset} que contenia les imatges que alimentar al discriminador. 

Després d'intetar-lo amb TensorFlow Quantum i amb PyTorch, vaig decidir prescindir de \textit{frameworks} per crear models de \textit{machine learning}, el discriminador, al ser una xarxa tan simple, la podia crear des de zero. Llavors vaig començar a buscar codi, d'una xarxa neuronal que havia estat feta amb Numpy, una llibreria de Python per fer càlculs amb vectors i matrius que havia fer servir des de que vaig començar amb Python.

Després de provar dues opcions que més o menys m'agradaven\footnote{Busca codi estructurat d'una forma en concret, que estigui dissenyat amb \textit{Object Oriented Programming}, una forma d'escriure codi en el qual tot s'implementa en un objecte.}, va aparèixer un repositori\footnote{\href{https://github.com/mnielsen/neural-networks-and-deep-learning}{link del respositori}} de Michael Nielsen, un dels autors de \textit{Quantum Computation and Quantum Information} \cite{QCandQI} que em va salvar. 

Era codi que estava estructurat d'una forma que m'agradaba i encara més important, que l'entenia. Inclús tènia diverses versions d'una xarxa neuronal, amb un nivell de complexitat diferent. Llavors, a partir del model més simple que havia en el repositori, veure el codi original al apèndix \ref{lst:disc_original}, vaig començar a desenvolupar el discriminador. 

Com es pot veure al codi final per el discriminador, he fet bastants canvis, però ho he canviat l'estructura o el funcionament teòric, la majoria són per tindre més funcionalitat, com per exemple l'enmagatzematge de les dades per poder al final veure-les en gràfic. Però el canvi més important és que inclou les dues formes d'optimitzar el model, amb dues funcions de pèrdua que funcionen de manera diferents però que són el mateix, la \textit{Binary Cross Entropy} i la \textit{MinMax}. Ja he parlat d'aquestes dues funcions en la part teòrica del treball, però a mode de recordatori, amb la primera s'agafa una imatge falsa o una real, i s'altera la funció, mentre que amb la \textit{MinMax} depenent de si es una imatge falsa o una real, s'utilitza una funció diferent. Això en el codi està materialitzat en dues funcions\footnote{Funcions de Python.} diferents, \code{backprop\_bce()} i \code{backprop\_minimax()}. No fa falta entrar en detall sobre que va cadascuna de les funcions, degut a que, fan el mateix i no tenen ninguna diferencia en termes de eficàcia o rapidesa. Les vaig fer tan sols per comprovar que no hi hauria ninguna diferencia. El codi final del discriminadors es pot veure al apèndix \ref{lst:disc_final}. També en el repositori del treball hi ha un altre arxiu anomenat \code{discriminator.py}, que conté una altre versió en la qual intentava implementar diverses funcions d'activació en el model, però no ho vaig aconseguir, tanmateix, no em preocupa perquè no es una part vital del treball, no passa res per tindre el discriminador només amb la sigmoide. 

El desenvolupament de l'altre part del model, el generador, va ser molt diferent. Després de provar a fer-ho amb TensorFlow, sense resultats, no tenia altra opció que fer-lo tot manualment i jo mateix\footnote{No em vaig ni plantejar buscar codi per internet perquè pensava que els autors dels paper que vaig llegir, les úniques persones que sabia que podien tindre el codi, no el penjarien.}, no obstant, ja tenia experiència en fer pseudo-xarxes neuronals quàntics. Sabia perfectament que tenia que fer, i com ho havia de fer. Implementar el mètode de \textit{parameter shift} en els circuits quàntics de l'article en el que vaig basar el treball \cite{QGAN_exp}. 






 
\chapter{Realització del experiment} 
 