\chapter{Àlgebra Lineal}\label{algebra}
Quan vaig començar a buscar informació sobre computació quàntica, en vaig ràpidament donar compte que necessitava molt més coneixement matemàtic, degut a que no entenia gairebé res dels llibres sobre computació quàntica. Arran aquell temps, una serie de vídeos sobre àlgebra lineal en va captar l’atenció, que es justament la branca de les matemàtiques sobre la qual es basa la computació quàntica. Els vídeos son les lliçons que dona el Professor Gilbert Strang al Institut Tecnològic de Massachusetts (MIT en anglès) \cite{LA_OCW_strang, LA2_OCW_strang}. Una vegada havia vist gairebé tots els vídeos, ja tenia bastants conceptes apresos. 

Aquelles lliçons es van ajudar a entendre les matemàtiques de \textit{Quantum Computation and Quantum Information} \cite{QCandQI} i \textit{Quantum Computing: A Gentle Introduction}. A poc a poc, vaig anar aprenent els fundaments matemàtics de la computació quàntica i mecànica quàntica.

En aquesta secció aniré explicant els conceptes bàsics de l'àlgebra lienal, per formar els coneixements en matemàtiques necessaris per poder comprendre aquest treball. 

\section{Vectors i Espais Vectorials}
Els objectes fonamentals de l'àlgebra lineal són els espais vectorials. Un espai vectorial es el conjunt de tots els vectors que tenen les mateixes dimensions. Per exemple  $\mathbb{R}^{3}$ seria el espai vectorial de tots els vectors de 3 dimensions, aquests vectors normalment s'utilitzen per representar punts en un espai tridimensional. En computació quàntica un tipus d'espais vectorials en concret són utilitzats: Els espais de Hilbert, en altres paraules, un espai vectorial amb un producte interior \cite{QCandQI:GramSchmidt}. Els espais de Hilbert segueixen un conjunt de productes i compleixen unes certes normes, en aquest capítol presentaré una part d'aquestes normes i productes, la quantitat que és necessària. S'ha de tenir en compte que els espais de Hilbert són molt més complicats que el que es representa en aquest treball, també que d'aquí en endavant, quan mencioni espai vectorial hem referiré a un espai de Hilbert, d'ha no ser que s'especifiqui el contrari. 

Els espais vectorial estan definits per les seves bases, un set de vectors $B = \{\ket{v_1}, \dots, \ket{v_n}\}$ es una base vàlida per l'espai $V$, si cada vector $\ket{v}$ en l'espai es pot escriure com $\ket{v} = \sum_i a_i \ket{v_i}$ per $\ket{v_i} \in B$. Els vectors en $B$ són linealment independent entre ells.

La notació estàndard pels conceptes de  àlgebra lienal en mecànica quàntica es la notació de Dirac, en la qual es representa un vector com $ \ket{\psi} $. On $\psi$ es la etiqueta del vector. Un vector $ \ket{\psi} $ amb $n$ dimensions també pot ser representat com una matriu columna que te la forma: 
$$
\ket{\psi} = 
\begin{bmatrix}
	z_{1} \\
	z_{2} \\
	\vdots \\
	z_{n-1} \\
	z_{n}
\end{bmatrix}
$$

On els nombres complexes $(z_{1}, z_{2}, \dots , z_{n-1}, z_{n} )$ són els seus elements. Un vector escrit com a $\ket{\psi}$ també s'anomena \textit{ket}.

La adició d'un par de vectors en un espai de Hilbert es definida per \footnote{Els vectors d'aquesta definició tenen els seus elements representats per la seva etiqueta i un subscrit e.g. el vector $\ket{\psi}$ te un element qualsevol $\psi_{1}$ i el seu primer element es $\psi_{1}$. Aquesta notació es seguirà utilitzant al llarg del treball.}:
$$
\ket{\psi} + \ket{\varphi} = \begin{bmatrix} \psi_{1} \\ \vdots \\ \psi_{n} \end{bmatrix} +
\begin{bmatrix} \varphi_{1} \\ \vdots \\ \varphi_{n} \end{bmatrix}
$$

A més a més, hi ha una multiplicació per un escalar\footnote{Un numero qualsevol en $\mathbb{R}$.} definida per:
$$
  z\ket{\psi} = z\begin{bmatrix} \psi_{1} \\ \vdots \\ \psi_{n} \end{bmatrix} = \begin{bmatrix} z \psi_{1} \\ \vdots \\ z \psi_{n} \end{bmatrix}
$$
On $z$ es un escalar i $\ket{\psi}$ un vector. Cal que notar que cada element del vector es multiplicar per el escalar.

Degut a que els espais de Hilbert son complexos tenen un conjugat complex definit per escalar com a: Per un escalar complex $z=a +bi$, el seu conjugat $z^*$ es igual a $a-bi$. 

Aquesta noció pot ampliar per a vectors i matrius, agafant el conjugat de totes les seves entrades/elements:
$$
\ket{\psi}^{*} = 
	\begin{bmatrix} \psi_{1} \\ \vdots \\ \psi_{n} \end{bmatrix}* = \begin{bmatrix} \psi_{1}^* \\ \vdots \\ \psi_{n}^* \end{bmatrix}
$$
$$
A^{*} = 
	\begin{bmatrix} 
	A_{11} & \cdots & A_{1n}\\ 
	\vdots & \ddots & \vdots \\ 
	A_{m1} & \cdots & A_mn
\end{bmatrix}^* 
= \begin{bmatrix} 
	A_{11}^* & \dots & A_{1n}^*\\ 
	\vdots & \ddots & \vdots \\ 
	A_{m1}^* & \cdots & A_mn^*
\end{bmatrix}
$$
Amb $\ket{\psi}$ sent un vector de dimensions $n$, i $A$ sent una matriu de dimensions $m \times n$.

Un altre concepte important es la transposada, representada per el supercrit $T$ que "rota" un vector o una matriu. Un vector columna amb una dimensió $n,1$ es transforma amb un vector fila amb una dimensió $1,n$\footnote{En realitat els vectors columna son matrius amb dimensió $n,1$ però he estat ometent el 1. Quan hem refereixo a les dimensions de un vector qualsevol, només diré un numero, no obstant, especificaré si és un vector columna o un vector fila.}:
$$
\ket{\psi}^{T} = 
	\begin{bmatrix} \psi_{1} \\ \vdots \\ \psi_{n} \end{bmatrix}^T = \begin{bmatrix} \psi_{1} & \dots & \psi_{n} \end{bmatrix}
$$

El mateix és veritat per les matrius, una matriu $m\times n$ transposada es converteix en una matriu $n \times m$. Per exemple:
$$
A^T = \begin{bmatrix}
	 2 & 3 \\
	 6 & 4 \\
	 2 & 5 
\end{bmatrix}^T = \begin{bmatrix}
 2 & 6 & 2 \\
 3 & 4 & 5
\end{bmatrix}
$$

La combinació de un conjugat complex i la transposada s'anomena el conjugat Hermitià, la seva notació es una $\dagger$ supercrita. Per un vector $\ket{\psi}$ el seu conjugat Hermitià $\ket{\psi}^\dagger$ és:
$$
\ket{\psi}^\dagger = (\ket{\psi}^*)^T =  \begin{bmatrix} \psi_{1}^* & \dots & \psi_{n}^* \end{bmatrix} = \bra{\psi}
$$
El conjugat Hermitià compleix que $\ket{\psi}^\dagger = \bra{\psi}$ i $\bra{\psi}^\dagger = \ket{\psi}$.

El conjugat Hermitià de un vector columna $\ket{\psi}$ s'anomena \textit{bra} o vector dual. En la notació de Dirac un vector dual s'escriu com $\bra{\psi}$.

\section{Operadors Lineals}
Per poder operar amb vectors i fer operacions amb ells, s'utilitzen les matrius, que també son denominades mapes lineal o operadors lineals, que són noms que descriuen millor com funcionen aquests objectes. La definició formal de un operador lineal pot ser bastant complicada, per aquesta raó, utilitzaré termes més informals al en aquesta secció. 

Bàsicament, un operador lineal transforma un vector en un altre vector, aquest vector poden o no ser de espais diferents \cite{LR_done_right:linear_map}. Més formalment, per un vector $\ket{v}$ en un espai $V$ i un vector $\ket{w}$ en un espai $W$, un operador lienal $A$ entre els vectors, fa l'acció:

$$
A \ket{v} = \ket{w}
$$
En altres paraules, l'operador mapa un element del espai vectorial $V$ cap a un espai vectorial $W$.
Els operadors lineals han de complir les següents operacions:
\begin{enumerate}
	\item Adició de Vectors: \\
	Per els vectors $\ket{\psi}$ i $\ket{\varphi}$ en un mateix espai vectorial, i un operador lienal $A$:
	$$ A(\ket{\psi} + \ket{\varphi}) = A\ket{\psi} + A\ket{\varphi}$$ 
	\item Multiplicació Escalar:\\
	Per el vector $\ket{\psi}$, el escalar $z$ i el operador lienal $A$:
	$$ A(z\ket{\psi}) = z A \ket{\psi}$$
\end{enumerate}

Aquestes afirmacions tenen que ser veritat per tots els vectors i tots els escalars en els espais on els operadors actuen. Cal notar que un operador lineal no te perquè ser una matriu necessàriament, fer exemple, les derivades i les integrals son operadors lienals, això es pot provar fàcilment al veure que compleixen els criteris especificats posteriorment. No obstant, les derivades i les integrals usualment no s'apliquen a vectors, sinó a les funcions, però es possible aplicar-les a vectors \footnote{No et preocupis, que es clar que les aplicaré a vectors :D.}.

Les matrius només son la representació matricial del operadors lienals \cite{LR_done_right:matrix}.

	\subsection{Tipus d'Operadors Lineals}
En la secció actual, exposaré els tipus bàsics d'operadors lienals que són indispensables en la teoria presentada en aquest capítol i la rest del treball.

\begin{enumerate}
	\item \textbf{Operador Zero} \\
	Qualsevol espai vectorial té un vector zero expressat en notació de Dirac com a $0$, degut a que $\ket{0}$ es un altre concepte totalment diferent en CQ i IQ\footnote{Computació Quàntica i Informació Quàntica.}. El vector zero es aquell vector que per qualsevol vector $\ket{\psi}$ i qualsevol escalar $z$, es compleix que:	
	$\ket{\psi} + 0 = \ket{\psi} $ i $z0 = 0$. \\
	El operador zero també s'escriu com a $0$ i es defineix com l'operador que mapa qualsevol vector al vector zero: $0\ket{\psi} = 0 $.  
	
	\item \textbf{Matriu inversa} \\
	Un matriu quadrada\footnote{Una matriu quadrada és una matriu amb dimensions $n\times n$, on $n \in \mathbb{N}$.} $A$ és invertible si existeix una matriu $A^{-1}$ de manera que $AA^{-1}=A^{-1}A$. $A^{-1}$ es la matriu inversa de $A$. La manera més ràpida de saber si una matriu es invertible es veient si el seu determinant no és zero.

	\item \textbf{Operador Identitat} \\
	Per a qualsevol espai vectorial $V$ existeix un operador identitat $I$ que es definit com $I\ket{\psi} = \ket{\psi}$, aquest operador no fa cap canvi al vectors als quals opera. Cal notar també que per qualsevol matriu $A$ i la seva inversa és veritat que $AA^{-1} = I$
	
	\item \textbf{Operador Unitari} \\
	Un operador unitari es qualsevol operador que no altera la norma dels vectors al quals es aplicat, per tant, una matriu es unitària si $AA^\dagger = I$
	Per convertir qualsevol operador en unitari, es divideix les seves entrades entre la norma del operador. 
	
	\item \textbf{Operadors Hermitians} \\
	Un operador Hermitià o \textit{self-adjoint operator} en anglès, es qualsevol operador que el seu conjugat Hermitià es ell mateix: $A = A^\dagger$ \\
	Una altre cosa a tenir en compte es que existeix un operador únic $A$ en un espai de Hilbert, de manera que per qualsevol vectors $\ket{\psi}$ i $\ket{\varphi}$, es compleix que:
	$$
	\bra{\psi} (A \ket{\varphi}) = (A^\dagger\bra{\psi})\ket{\varphi}
	$$ 
	Aquest operador es conegut com el \textit{adjoint} o conjugat Hermitià de $A$.
	
\end{enumerate}

\section{Producte Interior i Producte Exterior}

\subsection{Producte Interior}
Un vector dual $\bra{\psi}$ i un vector $\ket{\varphi}$ combinats formen el producte interior $\bra{\psi}\ket{\varphi}$, el qual efectua una operació que agafa els dos vectors com a input i produeix un nombre complex com a output:
$$
\bra{a}\ket{b} = a_1 b_1 + a_2 b_2 + ... + a_{n-1} b_{n-1} + a_n b_n = z
$$
Amb $z, a_i, b_i \in \mathbb{C}$. Quan hem refereixo a un producte interior, normalment diré "el producte interior de dos vectors", quan en realitat es una operació entre un vector dual i un vector.

El equivalent d'aquest producte en un espai real de dos dimensions $\mathbb{R}^2$ es el producte escalar, que es expressat com a :
\begin{equation}
	\bra{a}\ket{b} = \norm{\ket{a}}_2 \cdot \norm{\ket{b}}_2 \cos \theta 
	\label{eq:dot_product}
\end{equation}


Amb $\norm{\cdot}_2$ sent la norma $\ell^2$  definida com a $\norm{\ket{\psi}}_2 = \sqrt{\psi_{1}^2 + \cdots + \psi_{n}^2}$ amb $\theta$ sent l'angle entre els vectors $\ket{a}$ i $\ket{b}$. Com he dit l'equació \eqref{eq:dot_product} és equivalent al producte interior, no obstant, segons el que he vist, no es usada àmpliament ja que interpretar $\theta$ com un angle entre vectors de dimensions altes no té molt de sentit. En contrast, he vist aquest producte presentat en la seva interpretació geomètrica\footnote{Els detalls exactes de l'interpretació geomètrica estan fora del domini d'aquest treball, malgrat que m'agradaria molt parlar sobre el tema.} com el producte entre un vector fila i un vector columna: 
% The Geometry of Linear Equations: https://www.youtube.com/watch?v=ZK3O402wf1c&list=PL3CCC70370BD3FD48&index=1
% The four Fundamental Subspaces: https://www.youtube.com/watch?v=nHlE7EgJFds
$$
\bra{a}\ket{b} = \begin{bmatrix}a_1  \cdots  a_n \end{bmatrix} \begin{bmatrix} b_{1} \\ \vdots \\ b_{n} \end{bmatrix}
$$

Ja he definit la norma 
I have already defined the $\ell_2$ norm as the square root of the sum of the squared entries of a vector: 
$$
\norm{\ket{a}}_2 = \sqrt{\sum_{i} \abs{a_{i}}^2}
$$
Nonetheless, the more common definition is based upon the inner product. As you can see the inner product of a vector by itself is the sum of the squared entries: 
$$
\bra{a}\ket{a} = a_1 a_1 + \cdots + a_n a_n = a_1^2 + \cdots +  a_n^2 \\= \sum_{i} a_i^2
$$
Thus, the norm can be defined as the square root of the inner product of a vector:
\begin{equation}
\norm{\ket{a}}_2 = \sqrt{\bra{a}\ket{a}}
\label{eq:norm}
\end{equation}
When the norm is applied to a 2 dimensional vector you can see that is the same as the length of that vector, that is because norm and length are the same concepts, however, the norm is the generalized length that can be applied to a vector of any dimension. 


From what I understand some properties of the length of a two dimensional vector do not hold with the norm of a vector that has more than 2 dimensions. In other words, the norm behaves in similar ways like the distance from the origin (which is the length), thus they are not the exact same thing. Moreover, there are different types of norm\footnote{But not different types of length, that I know of at least.} that are used in different types of scenarios. That is why I am referring to a $\ell^2$ norm, a specific type of norm that is also named Euclidean norm which is used to define the $\ell^2$ distance or Euclidean distance, widely used to measure the distance of two points in a 2D space or a 3D space in high school \cite{wolfram:2norm}.

\subsubsection{Properties of the Inner Product}
The basic properties of the inner product are as follows:
\begin{enumerate}
	\item Is linear in the second argument $ (z_1\bra{a}+ z_2\bra{c})\ket{b} = z_1\bra{a}\ket{b} + z_2\bra{c}\ket{b}$
	\item Conjugate symmetry $\bra{a}\ket{b} = (\bra{b}\ket{a})^*$
	\item $\bra{a}\ket{a}$ is non-negative and real, except in the case of $\bra{a}\ket{a} = 0 \Leftrightarrow \ket{a} = 0$
\end{enumerate}

 
\subsection{Orthonormal and orthogonal vectors}
From the concept of norm comes the concepts a pair of orthogonal vectors and a pair of orthonormal vectors\footnote{Funny note, when I encountered these two terms for the first time in QC and QI \tocite, I taught they were the same thing and a week passed until I realized. It was such a difficult mess to understand everything else with these two terms confused.}. \\
Looking at the equation \eqref{eq:norm} we can see that if the inner product of a vector is one, the norm of this vector is also one. A vector that has norm one is named a unit vector. Therefore, if the inner product of a vector is one, that vector is a unit vector.

A pair of non-zero vectors are orthogonal if their inner product is zero. For two non-zero 2 dimensional vectors, if their inner product is equal to zero, you can see that they are perpendicular to each other by looking at equation \eqref{eq:dot_product}:
\begin{multline*}
	 \text{For }\ket{a} \text{and} \ket{b}\neq 0 : \\
	\text{If }\bra{a}\ket{b} = 0 \text{ then: } \norm{\ket{a}}_2 \cdot \norm{\ket{b}}_2 \cos \theta = 0 \\
	 \text{Because $\ket{a}$ and $\ket{b}$ are non-zero vectors, their norms can't be zero.} \\
	 \text{Thus the remainder term } \cos\theta \text{ is equal to zero.} \\
	 \text{Therefore, the angle $\theta$ as to be } \frac{\pi}{2}. \\
\end{multline*}
However, thinking that perpendicularity and orthogonality are the same concepts is a mistake, since, it only holds when looking at 2 dimensional vectors. As with norm and length, orthogonality is the generalized concept of perpendicularity that works for high dimensional vectors.

When we mix the concepts of unit vector and orthogonal vectors we arrive at the term orthonormality \cite{QCandQI:GramSchmidt}. A pair of non-zero vectors are orthonormal when both are unit vectors and there are orthogonal to each other: 
$$
\ket{a} \text{and } \ket{b} \text{ are othornormal if} \left\{
	\begin{array}{ll}
		\bra{a}\ket{b} = 0 \\
		\bra{a}\ket{a} = 1 \\
		\bra{b}\ket{b} = 1 \\
	\end{array}
\right.
$$
Orthonormal vectors are important, they are broadly used in quantum computation as well as quantum mechanics because they form the basis for the vector spaces on which the quantum states are located. \todo{maybe introduce them before if you talk about basis later on} \\
One thing to point out is that I have been talking about a pair of vectors when referring to orthonormal vectors, however, orthonormality can be extended to a set of vectors. If a set has all unit vectors and the vectors are orthogonal to each other, the set is orthonormal. The set of vectors $ B = \{\ket{\beta_1}, \ket{\beta_2}, ..., \ket{\beta_{n-1}} \ket{\beta_n}\} $ is
orthonormal if $\bra{\beta_i}\ket{\beta_i} = \delta_{ij}  \forall i, j$ \cite{QCandQI:GramSchmidt} where $\delta_{ij}$ is the Kronecker delta defined as 
% a gentle intro to QC
: 
$$
\delta_{ij} =
\begin{cases}
	1 & \text{if } i=j\\
	0 & \text{if } i\neq j
\end{cases}
$$

\subsection{Outer product}
The outer product is a function that takes two vectors -expressed as $\ket{a}\bra{b}$, with $\ket{a}$ and $\ket{b}$ being vectors- and produces an linear operator as output. Unlike the inner product, there is no analog for the outer product on the mathematics taught in high school\footnote{The analog of the inner product would be the dot product.}, and it is a bit difficult to understand it as it can take two vectors from different spaces as input. It is defined as follows:


%For a vector $\ket{v}$ and $\ket{v'}$ in the Hilbert space $V$ and a vector $\ket{w}$ in the Hilbert space $W$. The output is a linear operator of dimensions $m \times n$ in the space $M_{m\times n}$:
%$$\ket{v}\bra{w} : V,W \mapsto M_{m\times n} \\ $$
For a vector $\ket{v}$ and $\ket{v'}$ of dimensions $m$ and a vector $\ket{w}$ of dimension $n$. The output is a linear operator $A$ of dimensions $m \times n$ in the space $M_{m\times n}$:
$$
\ket{v}\bra{w} = A \text{ with } A\in \mathrm{Mat}_{m\times n} .
$$ 
%In other words, that takes a vector from $\mathbb{C}_i$ to $\mathbb{C}_j$. 
Whose action is defined by: 
\begin{equation}
	(\ket{v}\bra{w})\ket{v'} \equiv \ket{w}\bra{v}\ket{v'} = \bra{v}\ket{v'}\ket{w}
	\label{eq:outer_def}
\end{equation}


From equation \eqref{eq:outer_def} the usefulness and meaning of the outer product are hard to comprehend, so I will look at the way to compute it next to clarify how it works. For two vectors $\ket{a}$ and $\ket{b}$ of dimensions $m$ and $n$ respectively, their outer product is computed multiplying each element of $\ket{a}$ by each element of $\ket{b}$ forming a matrix of size $ m \times n$:
$$
\ket{a}\bra{b} = \begin{bmatrix}
	a_1 b_1 & a_1 b_2 & \cdots & a_1 b_n \\
	a_2 b_1 & a_2 b_2 & \cdots & a_2 b_n \\
	\vdots  & \vdots  & \ddots& \vdots  \\
	a_m b_1 & a_m b_2 & \cdots & a_m b_n \\
\end{bmatrix}
$$
The usefulness of the outer product will be shown in future sections.

\section{Tensor product}
The last product to mention is the tensor product, represented with the symbol $\otimes$. This product is used to create larger vector spaces by combining smaller vector spaces. The formal explanation of this concept is quite difficult, so I will focus on explaining the way to compute it by using the matrix representation of this product, named the Kronecker product. 

For a $m \times n$ matrix  $A$ and a $p \times q$ matrix  $B$ the output of their Kronecker product \cite{QCandQI:kronecker} is a $pm \times qn$ matrix:
\begin{multline*}
A \otimes B = \begin{bmatrix}
	a_{11} B & a_{12} B & \cdots & a_{1n}B \\
	a_{21} B & a_{22} B & \cdots & a_{2n}B \\
	\vdots & \vdots & \ddots &           \vdots \\
	a_{m1} B & a_{m2} B & \cdots & a_{mn} B
\end{bmatrix} \\
= \begin{bmatrix}
a_{11} b_{11} & a_{11} b_{12} & \cdots & a_{11} b_{1q} &
\cdots & \cdots & a_{1n} b_{11} & a_{1n} b_{12} & \cdots & a_{1n} b_{1q} \\
a_{11} b_{21} & a_{11} b_{22} & \cdots & a_{11} b_{2q} &
\cdots & \cdots & a_{1n} b_{21} & a_{1n} b_{22} & \cdots & a_{1n} b_{2q} \\
\vdots & \vdots & \ddots & \vdots & & & \vdots & \vdots & \ddots & \vdots \\
a_{11} b_{p1} & a_{11} b_{p2} & \cdots & a_{11} b_{pq} &
\cdots & \cdots & a_{1n} b_{p1} & a_{1n} b_{p2} & \cdots & a_{1n} b_{pq} \\
\vdots & \vdots & & \vdots & \ddots & & \vdots & \vdots & & \vdots \\
\vdots & \vdots & & \vdots & & \ddots & \vdots & \vdots & & \vdots \\
a_{m1} b_{11} & a_{m1} b_{12} & \cdots & a_{m1} b_{1q} &
\cdots & \cdots & a_{mn} b_{11} & a_{mn} b_{12} & \cdots & a_{mn} b_{1q} \\
a_{m1} b_{21} & a_{m1} b_{22} & \cdots & a_{m1} b_{2q} &
\cdots & \cdots & a_{mn} b_{21} & a_{mn} b_{22} & \cdots & a_{mn} b_{2q} \\
\vdots & \vdots & \ddots & \vdots & & & \vdots & \vdots & \ddots & \vdots \\
a_{m1} b_{p1} & a_{m1} b_{p2} & \cdots & a_{m1} b_{pq} &
\cdots & \cdots & a_{mn} b_{p1} & a_{mn} b_{p2} & \cdots & a_{mn} b_{pq}
\end{bmatrix}
\end{multline*}

Note that $a_{ij}B$ is a scalar multiplication by a matrix, with $a_{ij}$ being the scalar and $B$ being the matrix.

Here is a clearer example with two $2\times2$ matrices, note that each entry of the first matrix is multiplied by the second matrix:
\begin{equation*}
\begin{bmatrix}
	1 & 2 \\
	3 & 4 \\
\end{bmatrix} \otimes
\begin{bmatrix}
	0 & 5 \\
	6 & 7 \\
\end{bmatrix} =
\begin{bmatrix}
1 \begin{bmatrix}
	0 & 5 \\
	6 & 7 \\
\end{bmatrix} & 
2 \begin{bmatrix}
	0 & 5 \\
	6 & 7 \\
\end{bmatrix} \vspace{6pt} \\ 

3 \begin{bmatrix}
	0 & 5 \\
	6 & 7 \\
\end{bmatrix} & 
4 \begin{bmatrix}
	0 & 5 \\
	6 & 7 \\
\end{bmatrix}
\end{bmatrix}
\end{equation*}

\begin{equation*}
	= 
\begin{bmatrix}
1\times 0 & 1\times 5 & 2\times 0 & 2\times 5 \\
1\times 6 & 1\times 7 & 2\times 6 & 2\times 7 \\
3\times 0 & 3\times 5 & 4\times 0 & 4\times 5 \\
3\times 6 & 3\times 7 & 4\times 6 & 4\times 7 \\
\end{bmatrix} \\
= 
\begin{bmatrix}
0 &  5 &  0 & 10 \\
6 &  7 & 12 & 14 \\
0 & 15 &  0 & 20 \\
18 & 21 & 24 & 28
\end{bmatrix}
\end{equation*}

One important piece of notation to take into account is $\bigotimes$, used to represent the equivalent of the sum (noted with $\sum$), but instead of addition the Kronecker product is used. In other words $\bigotimes$ denotes the Kronecker product of a finite number of terms. To clarify here's an example with the identity matrix:


With $\mathbb{I}$ as the matrix $\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$ and $n$ as a power of $2$:
$$
\mathbb{I}_n = \bigotimes^{\log_{2} n} \mathbb{I}  
$$
	
Here is the case for $n=8$: 
$$\mathbb{I}_8 =  \bigotimes^{\log_{2} 8} \mathbb{I} =\bigotimes^3 \mathbb{I} = \mathbb{I} \otimes \mathbb{I} \otimes \mathbb{I} =
	\begin{bmatrix}
		1 &0 &0 &0 &0 &0 &0 &0 \\
		0 &1 &0 &0 &0 &0 &0 &0 \\
		0 &0 &1 &0 &0 &0 &0 &0 \\
		0 &0 &0 &1 &0 &0 &0 &0 \\
		0 &0 &0 &0 &1 &0 &0 &0 \\
		0 &0 &0 &0 &0 &1 &0 &0 \\
		0 &0 &0 &0 &0 &0 &1 &0 \\
		0 &0 &0 &0 &0 &0 &0 &1 \\
	\end{bmatrix} 
$$


The Kronecker product also works for vectors in the same way, with a scalar-vector multiplication:

For the vectors $\ket{\psi}$ and $\ket{\varphi}$ of dimensions $n$ and $m$ respectively:
$$
\ket{\psi} \otimes \ket{\varphi} = \begin{bmatrix}
	\psi_{1} \ket{\varphi} \\
	\psi_2 \ket{\varphi} \\
	\vdots \\
	\psi_{m} \ket{\varphi}
\end{bmatrix} = 
\begin{bmatrix}
\psi_{1} \varphi_1 \\
\psi_{1} \varphi_{2} \\
\vdots \\
\psi_{1} \varphi_{m} \\
\vdots \\
\vdots \\
\psi_{n} \varphi_1 \\
\psi_{n} \varphi_{2} \\
\vdots \\
\psi_{n} \varphi_{m} \\
\end{bmatrix}
$$

Note that the Kronecker product can also be taken between a vector and a matrix or vise-versa, however this form isn't as common as the other two.
%With $V$ and $W$ being vector spaces of dimension $m$ and $n$ respectively, the tensor product between them $V \otimes W$ is a vector spaces of dimension $mn$. The elements on the space $V \otimes W$ are 
%For the orthonormal basis $\ket{i}$ and $\ket{j}$ of $V$ and $W$ respectively the orthonormal basis for $V\otimes W$ would be $\ket{i}\otimes\ket{j}$.


\subsection{Properties of the Tensor Product}
The basic properties of the tensor product are as follows \cite{QCandQI:tensor_product, wiki:tensor_product}:
\begin{enumerate}
	\item Associativity: \\ $A \otimes (B + C) = A \otimes B + A \otimes C$ \\
	$ (zA) \otimes B = A \otimes (zB) = z(A \otimes B)$\\
	$ (A \otimes B) \otimes C = A \otimes (B\otimes C)$ \\
	$A \otimes 0 = 0 \otimes A = 0 $
	\item Non-commutative \footnote{One cool thing is that $A \otimes B$ and $B \otimes A$ are permutation equivalent: \\ $\exists P, Q \Rightarrow A \otimes B = P (  B \otimes A )Q$ where $P$ and $Q$ are permutation matrices.  }: \\
	$A \otimes B \neq B \otimes A $
\end{enumerate}



%$$
%f: \mathbb{R} \rightarrow \mathbb{R} \text{ or } \mathbb{R} \stackrel{f}{\to} \mathbb{R} 
%$$
%Where $\mathbb{R}$ is the space of all real number, and $\rightarrow$ is 


\section{Trace}
The trace of a matrix is just the sum of the elements on the main diagonal, the one that goes from top to bottom and left to right.

Here's a matrix $A$ with its main diagonal marked:
$$A = 
	\begin{bmatrix}
		\tikzmark{top}{1} & \tikzmark{top2}{1} & 0 \\
		0 & 1 & \tikzmark{bottom2}{1} \\
		\tikzmark{end}{1} & 0 & \tikzmark{bottom}{1}\\
	\end{bmatrix}
$$
\begin{tikzpicture}[overlay,remember picture]
	\draw[opacity=.4,line width=3mm,line cap=round] (top.center) -- (bottom.center);
\end{tikzpicture}

And its trace, denoted by $\Tr[A]$ is:
$$
\Tr[A] = 1 + 1 + 1 = 3
$$
More formally, the trace of a $n$-dimensional squared matrix $A$ is:
$$
\Tr[A] = \sum_{i=1}^{n} a_{ii} = a_{11} + a_{22} + \cdots + a_{nn}
$$


The trace of a matrix as the following properties:
\begin{enumerate}
	\item Linear operator: \\
	Because the trace is a linear mapping, it follows that:\\
	$\Tr[A+B] = \Tr[A] + \Tr[B]$ and $\Tr[zA] = z \Tr[A]$, for all squared matrices $A$ and $B$ and all scalars $z$.
	\item Trace of a Kronecker product: \\
	$\Tr[A \otimes B] = \Tr[A]\Tr[B]$
	\item Transpose has the same trace: \\
	$\Tr[A]=\Tr[A^T]$
	\item Trace of a product is cyclic: \\
	For a $m \times n$ matrix $A$ and a $n \times m$ matrix $B$:\\
	$\Tr[AB]=\Tr[BA]$
\end{enumerate}

One very useful way to compute the trace of an operator is through the Gram-Schmidt procedure\footnote{See \ref{gram} for the definition of the Gram-Schmidt procedure.} and an outer product. 
Using Gram-Schmidt to represent the unit vector $\ket{\psi}$ with an orthonormal basis $\ket{i}$ which includes $\ket{\psi}$ as the first element, is true that:
$$
\Tr[A\ket{\psi}\bra{\psi}] = \sum_i \bra{i}A\ket{\psi}\bra{\psi}\ket{i} = \bra{\psi}A\ket{\psi}
$$

\chapter{Quantum Computation}
After some amount of math theory, it is time to start talking about quantum mechanics, in this chapter I am going to introduce the basic concepts of quantum computation and quantum information.

Quantum mechanics is a mathematical framework or rather a set of theories used to describe and explain the physical properties of atoms, molecules, and subatomic particles. It is the framework of all quantum physics including quantum information science. The right way of presenting quantum computation is through the formal quantum mechanics postulates because, with them, the statements made in quantum computation do not seem to come from anywhere \cite{QCandQI:QM_postulates}. However, to not complicate this section more than it is, I will do my best to explain the concepts and math of quantum computing just on their own, without presenting more generalized concepts from quantum mechanics, unless it is totally necessary to do so.

\section{Quantum State and Superpositions}
To describe how physical systems evolution through time, you need to represent them in some way. On quantum computing those quantum physical systems are represented through quantum states, which are some form of probably distribution for the possible outcomes of a measurement on a quantum system \cite{QT_concepts:q_systems}.

Imagine that you have a pen, however, you do not know which color is it, but you know that it can be red or blue. To figure out the color, you would do a measurement, in other words, write something to see the color of the ink. You also know that there is a 50\% change of being blue, and a 50\% of being red\footnote{This is starting to sound a lot like US politics.}. So far you have your quantum system (the pen), a way to measured it (writing), and a list of all the possible outcomes (50\% blue and 50\% red). Now you need your quantum state, the way to represent everything mathematically. Let's try to store the information that we know about the pen in a vector.

If we put the probability of each possible outcome in an entry, we have the following vector:
$$
\begin{bmatrix}
	0.5 \\
	0.5
\end{bmatrix}
$$
Where the first entry is the probability of painting blue and the second one of painting red, to make it more easy let me color the numbers:
$$
\begin{bmatrix}
	\textcolor{blue}{0.5} \\
	\textcolor{red}{0.5}
\end{bmatrix}
$$
Note that this vector is normalized went using the $\ell_1$ norm, defined as the sum of the entries of a vectors\footnote{With $\ket{a}$ being a vector, the $\ell_1$ norm, denoted as $\norm{\cdot}_1$ is $\norm{\ket{a}}_1 = \sum_i a_i$.}, in other words, the $\ell_1$ norm of this vector is $1$.

So, how can we extract the probability about a possible outcome from the vectors, by a mathematical operation; I mean. We how that we need to get a real number out of this operation, so let's try with an inner product.
As we know a vector in a space is a linear combination of space basis:
$$
\textcolor{blue}{0.5}\ket{0} + \textcolor{red}{0.5}\ket{1} = \begin{bmatrix}
	\textcolor{blue}{0.5} \\
	\textcolor{red}{0.5}
\end{bmatrix}
$$
Where $\ket{0}$ is the vector $\begin{bmatrix} 1 \\ 0\end{bmatrix}$, and, $\ket{1}$ is the vector $\begin{bmatrix} 0 \\ 1 \end{bmatrix}$. Thus to get the scalar, you take the inner product between the basis corresponding to that scalar and the vector, like so:
$$
\bra{0}\ket{v} =
\begin{bmatrix}
	1 &
	0
\end{bmatrix}
\begin{bmatrix}
	\textcolor{blue}{0.5} \\
	\textcolor{red}{0.5}
\end{bmatrix}
= \textcolor{blue}{0.5}
$$
$$
\bra{1}\ket{v} =
	\begin{bmatrix}
		0 &
		1
	\end{bmatrix}
	\begin{bmatrix}
		\textcolor{blue}{0.5} \\
		\textcolor{red}{0.5}
	\end{bmatrix}
	= \textcolor{red}{0.5}
$$
You can see that this is quite easy. For example, to represent a pen with 6 colors with a random possibility of writing in a specific color, a valid vector to represent this pen would be\footnote{The entries in the vector are colored for clarity, but don't take it to seriously.}:
$$
\ket{w} =
\begin{bmatrix}
	\textcolor{red}{0.25} \\
	\textcolor{blue}{0.3 }\\
	\textcolor{green}{0.1 }\\
	\textcolor{magenta}{0.1 }\\
	\textcolor{black}{0.1 }\\
	\textcolor{brown}{0.05}
\end{bmatrix}
$$
Then we can measure the state with an inner product:
$$
\begin{bmatrix}
	0 & 0 & 1 & 0 & 0 & 0\\
\end{bmatrix}
\begin{bmatrix}
	\textcolor{red}{0.25} \\
	\textcolor{blue}{0.3 }\\
	\textcolor{green}{0.1 }\\
	\textcolor{magenta}{0.1 }\\
	\textcolor{black}{0.1 }\\
	\textcolor{brown}{0.05}
\end{bmatrix} = \textcolor{green}{0.1 }
$$
Note that we use the corresponding basis for the 3th element, the green number.

Leaving the pens aside, now we are going to substitute the pens by quantum systems, for example, with a photon. Photons have certain properties that can be measured, like polarization\footnote{More concretely, its a property of traverse waves, which electromagnetic waves are.}, when looking at photon as waves in the electromagnetic field, the polarization is the geometrical orientation of the oscillation of the wave. This can be interpreted as an angle respect to the propagation direction. \todo{insert figure here}

We can set the basis states of the polarization as vertical and horizontal, denoted with the vectors $\ket{\text{\textrightarrow}}$ and $\ket{\text{\textuparrow}}$, respectively. And we can set a state that is a superposition of both, denoted as $\ket{\nearrow}$ \cite{QC_intro:photon}. 
The superposed state will be: 
$$
\ket{\nearrow} = \alpha\ket{\text{\textrightarrow}} + \beta\ket{\text{\textuparrow}}
$$
Where $\alpha$ and $\beta$ are complex numbers.
The superpose state simply is an angle that is not $0$ nor $\frac{\pi}{2}$. Note that we do not have to worry about the exact math of all these vectors\footnote{The mathematical description of the polarization of the photon is not necessary to know went talking about quantum computation, unless you are working with a quantum computer that represents the quantum states as photons (using their polarization or something else.). Then you would be talking about quantum optics which is itself a complex part of quantum mechanics, that I do not want to learn about yet.}, because we are only worry about the information that they carry. Which is taken through the measurements, like with the pen. To know\footnote{Kind of, you will see later.} the state of polarization, e.i. to measure it, you need to pass the photon through several polarizing filters, which let a photon pass completely through it or absorbed completely on a probabilistic matter. 

Let me explain, what the filter does is to collapse the photon in two possible states of polarization in either the state on which the filter is oriented or on the state that is perpendicular to it. For example, a filter oriented in the horizontal direction will either pass a photon oriented vertically or block entirely.  The matter on which the collapse is decided is probabilistic, a photon oriented on the horizontal direction has a 100\% probability of being absorbed, while a photon polarized vertically has a 0\% chance of being absorbed, and finally, a photon that has a $\frac{\pi}{4}$ angle of polarization respect to the filter, meaning that is exactly in between these two perpendicular states, has a 50\% probability of being absorbed by the filter, thus it has a 50\% chance of passing through. 

However, the polarization of the photons are a concrete physical system, in quantum information everyone works with qubits, instead of physical systems\footnote{Unless you are working with a concrete quantum system, and you want to represent the information in the system's terms.}. Went presenting algorithms for example, researchers use qubits. 

\section{Qubits}
Modern computers represent information through strings of zeros and ones named bits. Everything, from images to letters, characters and instructions. For example, the letter t is represented by the bitstring $01110100$. Everything you do on a computer is on binary code.

Since we are so used to binary code, in the field of quantum computing is also used, however, instead of bits, qubits are used. A qubit is the analog of a bit, in other words the information unit used by quantum computers. We can apply quantum superposition and quantum entanglement to qubits. If a bit has two states $0$ and $1$, a qubit has a state that is a linear combination of the two fundamental states $\ket{0}$ and $\ket{1}$:
$$
\ket{\psi} = \alpha\ket{0} + \beta\ket{1}
$$
Where $\alpha$ and $\beta$ are complex numbers and $\ket{\psi}$ is a vector in a 2-dimensional Hilbert space\footnote{A space with an inner product.}.
The vectors $\ket{0}$ and $\ket{1}$ are called the computation basis vectors. They are represented as follows:
$$
\ket{0} = \begin{bmatrix}
	1 \\ 0
\end{bmatrix} \;
\ket{1} = \begin{bmatrix}
	0 \\ 1
\end{bmatrix}
$$
Thus the vector $\ket{\psi}$ is:
$$
\ket{\psi} 
= \alpha 
\begin{bmatrix}
	1 \\ 0
\end{bmatrix} + \beta 
\begin{bmatrix}
0 \\ 1
\end{bmatrix}= 
\begin{bmatrix}
	\alpha \\ \beta
\end{bmatrix}
$$
This vector is a valid quantum state to represent a qubit, however, there is an important factor to take into account. The vector has to be normalized according to the $\ell_2$ norm, therefore $\alpha$ and $\beta$ can not be any complex number, they need to form a vector that a norm of $1$:
$$
\norm{\ket{\psi}} = 1 
$$
Therefore:
\begin{align*}
	\norm{\psi} &= \sqrt{\abs{\alpha}^2 + \abs{\beta}^2} = 1 \\
	&\Rightarrow \abs{\alpha}^2 + \abs{\beta}^2 = 1
\end{align*}

Saying that a qubit is "is a linear combination of the two fundamental states" is not that meaningful, so let me elaborate more on that:
A $n$-bitstring can represent just one possible combination at a time, while a $n$-qubit system can represent a combination of all the possible combinations. Consider a qubit as a mixture of the possible states, each coefficient on the linear combination is the number that indicates how much a state is part of that mixture. For example, if you want an equal mixture of the states $\ket{0}$ and $\ket{1}$ 




Something interesting happens as you increase the number of qubits, the "amount of information" increases exponentially. When you go from a $7$-bitstring to a $8$-string the amount of numbers\footnote{Binary numbers.} that can be represent increases by one, from $7$ to $8$.
But $n$ qubits can represent $2^n$ complex numbers. Note that the qubit represented before represents two complex numbers. All this is because the computational basis used by the qubits, it grows exponentially with the number of qubits. 

Two qubits are represented by the vector:
\begin{align*}
	\ket{\psi} &= \alpha_1\ket{00} + \alpha_2\ket{01} + \alpha_3\ket{10} + \alpha_4\ket{11}\\
			&= \alpha_1\begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \end{bmatrix} + \alpha_2 \begin{bmatrix} 0 \\ 1 \\ 0 \\ 0 \end{bmatrix} + \alpha_3 \begin{bmatrix} 0 \\ 0 \\ 1 \\ 0 \end{bmatrix} + \alpha_4\begin{bmatrix} 0 \\ 0 \\ 0 \\ 1 \end{bmatrix} 
			= \begin{bmatrix} \alpha_1 \\ \alpha_2 \\ \alpha_3 \\ \alpha_4 \end{bmatrix}
\end{align*}
Note that the computational basis vectors are the columns of an identity matrix of dimensions $2^n\times 2^n$, where $n$ is the number of qubits.

The quantity of information that a qubit has is the information of all possible combination that the qubit has. One qubit has two possible combination $\ket{0}$ and $\ket{1}$, meanwhile two qubits have four possible combinations $\ket{00}, \ket{01}, \ket{10}, \ket{11}$, and four qubits have $2^4$ possible combinations, that is a total of $16$. And the information about each combination is a complex number, represent in the examples above has $\alpha_i$. The complex numbers are used to specify the quantum superposition that the system has. 



\section{Quantum Measurement}
Turning back to the photons and their polarization, to predict on which basis the photon is going to collapse, we need to employ quantum measurements. On this case to know the probability of the collapse to a specific basis is enough. 
Where the vector $\ket{\text{\textrightarrow}}$ expresses the polarization of the filter and $\ket{\text{\textuparrow}}$ its perpendicular complement. A photon that is polarized in the following state \cite{QC_intro:photon}:
$$
\ket{\nearrow} = \alpha\ket{\text{\textrightarrow}} + \beta\ket{\text{\textuparrow}}
$$
The probability of being absorbed is $\alpha^2$, note that $\alpha$ is the coefficient that corresponds to the basis $\ket{\text{\textrightarrow}}$, which is the polarization of the filter. In other words, $\alpha^2$ indicates the probability of the collapse of the polarization onto the state $\ket{\text{\textrightarrow}}$. 

The coefficients $\alpha$ and $\beta$ can be expressed as function of an angle $\theta$\footnote{For more information on the polarization of a photon, see appendix \ref{appendix:optics}.}:
\begin{equation}
	\ket{\nearrow} = \cos\theta\ket{\text{\textrightarrow}} + \sin\theta\ket{\text{\textuparrow}}
	\label{eq:photon_state}
\end{equation}
And the expected probabilities become $\cos^2\theta$ and  $\sin^2\theta$, respectively \cite{QC_intro:photon}. Now we can prove the previous statements. In the case of a $\pi/2$ angle we see that:
\begin{align*}
	p(\ket{\text{\textrightarrow}}) &= \cos^2\theta 
	= \cos^2 \frac{\pi}{2}
	= \left( \frac{1}{\sqrt{2}} \right) ^2 \\
	&= 0.5 \\
	p(\ket{\text{\textuparrow}}) &= \sin^2\theta 
	= \sin^2 \frac{\pi}{2}
	= \left( \frac{1}{\sqrt{2}} \right) ^2 \\
	&= 0.5 \\	
\end{align*} 
