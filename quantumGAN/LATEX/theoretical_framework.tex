\chapter{Àlgebra Lineal}\label{algebra}
Quan vaig començar a buscar informació sobre computació quàntica, en vaig ràpidament donar compte que necessitava molt més coneixement matemàtic, degut a que no entenia gairebé res dels llibres sobre computació quàntica. Arran aquell temps, una serie de vídeos sobre àlgebra lineal en va captar l’atenció, que es justament la branca de les matemàtiques sobre la qual es basa la computació quàntica. Els vídeos son les lliçons que dona el Professor Gilbert Strang al Institut Tecnològic de Massachusetts (MIT en anglès) \cite{LA_OCW_strang, LA2_OCW_strang}. Una vegada havia vist gairebé tots els vídeos, ja tenia bastants conceptes apresos. 

Aquelles lliçons es van ajudar a entendre les matemàtiques de \textit{Quantum Computation and Quantum Information} \cite{QCandQI} i \textit{Quantum Computing: A Gentle Introduction}. A poc a poc, vaig anar aprenent els fundaments matemàtics de la computació quàntica i mecànica quàntica.

En aquesta secció aniré explicant els conceptes bàsics de l'àlgebra lienal, per formar els coneixements en matemàtiques necessaris per poder comprendre aquest treball. 

\section{Vectors i Espais Vectorials}
Els objectes fonamentals de l'àlgebra lineal són els espais vectorials. Un espai vectorial es el conjunt de tots els vectors que tenen les mateixes dimensions. Per exemple  $\mathbb{R}^{3}$ seria el espai vectorial de tots els vectors de 3 dimensions, aquests vectors normalment s'utilitzen per representar punts en un espai tridimensional. En computació quàntica un tipus d'espais vectorials en concret són utilitzats: Els espais de Hilbert, en altres paraules, un espai vectorial amb un producte interior \cite{QCandQI:GramSchmidt}. Els espais de Hilbert segueixen un conjunt de productes i compleixen unes certes normes, en aquest capítol presentaré una part d'aquestes normes i productes, la quantitat que és necessària. S'ha de tenir en compte que els espais de Hilbert són molt més complicats que el que es representa en aquest treball, també que d'aquí en endavant, quan mencioni espai vectorial hem referiré a un espai de Hilbert, d'ha no ser que s'especifiqui el contrari. 

Els espais vectorial estan definits per les seves bases, un set de vectors $B = \{\ket{v_1}, \dots, \ket{v_n}\}$ es una base vàlida per l'espai $V$, si cada vector $\ket{v}$ en l'espai es pot escriure com $\ket{v} = \sum_i a_i \ket{v_i}$ per $\ket{v_i} \in B$. Els vectors en $B$ són linealment independent entre ells.

La notació estàndard pels conceptes de  àlgebra lienal en mecànica quàntica es la notació de Dirac, en la qual es representa un vector com $ \ket{\psi} $. On $\psi$ es la etiqueta del vector. Un vector $ \ket{\psi} $ amb $n$ dimensions també pot ser representat com una matriu columna que te la forma: 
$$
\ket{\psi} = 
\begin{bmatrix}
	z_{1} \\
	z_{2} \\
	\vdots \\
	z_{n-1} \\
	z_{n}
\end{bmatrix}
$$

On els nombres complexes $(z_{1}, z_{2}, \dots , z_{n-1}, z_{n} )$ són els seus elements. Un vector escrit com a $\ket{\psi}$ també s'anomena \textit{ket}.

La adició d'un par de vectors en un espai de Hilbert es definida per \footnote{Els vectors d'aquesta definició tenen els seus elements representats per la seva etiqueta i un subscrit e.g. el vector $\ket{\psi}$ te un element qualsevol $\psi_{1}$ i el seu primer element es $\psi_{1}$. Aquesta notació es seguirà utilitzant al llarg del treball.}:
$$
\ket{\psi} + \ket{\varphi} = \begin{bmatrix} \psi_{1} \\ \vdots \\ \psi_{n} \end{bmatrix} +
\begin{bmatrix} \varphi_{1} \\ \vdots \\ \varphi_{n} \end{bmatrix}
$$

A més a més, hi ha una multiplicació per un escalar\footnote{Un numero qualsevol en $\mathbb{R}$.} definida per:
$$
  z\ket{\psi} = z\begin{bmatrix} \psi_{1} \\ \vdots \\ \psi_{n} \end{bmatrix} = \begin{bmatrix} z \psi_{1} \\ \vdots \\ z \psi_{n} \end{bmatrix}
$$
On $z$ es un escalar i $\ket{\psi}$ un vector. Cal que notar que cada element del vector es multiplicar per el escalar.

Degut a que els espais de Hilbert son complexos tenen un conjugat complex definit per escalar com a: Per un escalar complex $z=a +bi$, el seu conjugat $z^*$ es igual a $a-bi$. 

Aquesta noció pot ampliar per a vectors i matrius, agafant el conjugat de totes les seves entrades/elements:
$$
\ket{\psi}^{*} = 
	\begin{bmatrix} \psi_{1} \\ \vdots \\ \psi_{n} \end{bmatrix}* = \begin{bmatrix} \psi_{1}^* \\ \vdots \\ \psi_{n}^* \end{bmatrix}
$$
$$
A^{*} = 
	\begin{bmatrix} 
	A_{11} & \cdots & A_{1n}\\ 
	\vdots & \ddots & \vdots \\ 
	A_{m1} & \cdots & A_mn
\end{bmatrix}^* 
= \begin{bmatrix} 
	A_{11}^* & \dots & A_{1n}^*\\ 
	\vdots & \ddots & \vdots \\ 
	A_{m1}^* & \cdots & A_mn^*
\end{bmatrix}
$$
Amb $\ket{\psi}$ sent un vector de dimensions $n$, i $A$ sent una matriu de dimensions $m \times n$.

Un altre concepte important es la transposada, representada per el supercrit $T$ que "rota" un vector o una matriu. Un vector columna amb una dimensió $n,1$ es transforma amb un vector fila amb una dimensió $1,n$\footnote{En realitat els vectors columna son matrius amb dimensió $n,1$ però he estat ometent el 1. Quan hem refereixo a les dimensions de un vector qualsevol, només diré un numero, no obstant, especificaré si és un vector columna o un vector fila.}:
$$
\ket{\psi}^{T} = 
	\begin{bmatrix} \psi_{1} \\ \vdots \\ \psi_{n} \end{bmatrix}^T = \begin{bmatrix} \psi_{1} & \dots & \psi_{n} \end{bmatrix}
$$

El mateix és veritat per les matrius, una matriu $m\times n$ transposada es converteix en una matriu $n \times m$. Per exemple:
$$
A^T = \begin{bmatrix}
	 2 & 3 \\
	 6 & 4 \\
	 2 & 5 
\end{bmatrix}^T = \begin{bmatrix}
 2 & 6 & 2 \\
 3 & 4 & 5
\end{bmatrix}
$$

La combinació de un conjugat complex i la transposada s'anomena el conjugat Hermitià, la seva notació es una $\dagger$ supercrita. Per un vector $\ket{\psi}$ el seu conjugat Hermitià $\ket{\psi}^\dagger$ és:
$$
\ket{\psi}^\dagger = (\ket{\psi}^*)^T =  \begin{bmatrix} \psi_{1}^* & \dots & \psi_{n}^* \end{bmatrix} = \bra{\psi}
$$
El conjugat Hermitià compleix que $\ket{\psi}^\dagger = \bra{\psi}$ i $\bra{\psi}^\dagger = \ket{\psi}$.

El conjugat Hermitià de un vector columna $\ket{\psi}$ s'anomena \textit{bra} o vector dual. En la notació de Dirac un vector dual s'escriu com $\bra{\psi}$.

\section{Operadors Lineals}
Per poder operar amb vectors i fer operacions amb ells, s'utilitzen les matrius, que també son denominades mapes lineal o operadors lineals, que són noms que descriuen millor com funcionen aquests objectes. La definició formal de un operador lineal pot ser bastant complicada, per aquesta raó, utilitzaré termes més informals al en aquesta secció. 

Bàsicament, un operador lineal transforma un vector en un altre vector, aquest vector poden o no ser de espais diferents \cite{LR_done_right:linear_map}. Més formalment, per un vector $\ket{v}$ en un espai $V$ i un vector $\ket{w}$ en un espai $W$, un operador lienal $A$ entre els vectors, fa l'acció:

$$
A \ket{v} = \ket{w}
$$
En altres paraules, l'operador mapa un element del espai vectorial $V$ cap a un espai vectorial $W$.
Els operadors lineals han de complir les següents operacions:
\begin{enumerate}
	\item Adició de Vectors: \\
	Per els vectors $\ket{\psi}$ i $\ket{\varphi}$ en un mateix espai vectorial, i un operador lienal $A$:
	$$ A(\ket{\psi} + \ket{\varphi}) = A\ket{\psi} + A\ket{\varphi}$$ 
	\item Multiplicació Escalar:\\
	Per el vector $\ket{\psi}$, el escalar $z$ i el operador lienal $A$:
	$$ A(z\ket{\psi}) = z A \ket{\psi}$$
\end{enumerate}

Aquestes afirmacions tenen que ser veritat per tots els vectors i tots els escalars en els espais on els operadors actuen. Cal notar que un operador lineal no te perquè ser una matriu necessàriament, fer exemple, les derivades i les integrals son operadors lienals, això es pot provar fàcilment al veure que compleixen els criteris especificats posteriorment. No obstant, les derivades i les integrals usualment no s'apliquen a vectors, sinó a les funcions, però es possible aplicar-les a vectors \footnote{No et preocupis, que es clar que les aplicaré a vectors :D.}.

Les matrius només son la representació matricial del operadors lienals \cite{LR_done_right:matrix}.

	\subsection{Tipus d'Operadors Lineals}
En la secció actual, exposaré els tipus bàsics d'operadors lienals que són indispensables en la teoria presentada en aquest capítol i la rest del treball.

\begin{enumerate}
	\item \textbf{Operador Zero} \\
	Qualsevol espai vectorial té un vector zero expressat en notació de Dirac com a $0$, degut a que $\ket{0}$ es un altre concepte totalment diferent en CQ i IQ\footnote{Computació Quàntica i Informació Quàntica.}. El vector zero es aquell vector que per qualsevol vector $\ket{\psi}$ i qualsevol escalar $z$, es compleix que:	
	$\ket{\psi} + 0 = \ket{\psi} $ i $z0 = 0$. \\
	El operador zero també s'escriu com a $0$ i es defineix com l'operador que mapa qualsevol vector al vector zero: $0\ket{\psi} = 0 $.  
	
	\item \textbf{Matriu inversa} \\
	Un matriu quadrada\footnote{Una matriu quadrada és una matriu amb dimensions $n\times n$, on $n \in \mathbb{N}$.} $A$ és invertible si existeix una matriu $A^{-1}$ de manera que $AA^{-1}=A^{-1}A$. $A^{-1}$ es la matriu inversa de $A$. La manera més ràpida de saber si una matriu es invertible es veient si el seu determinant no és zero.

	\item \textbf{Operador Identitat} \\
	Per a qualsevol espai vectorial $V$ existeix un operador identitat $I$ que es definit com $I\ket{\psi} = \ket{\psi}$, aquest operador no fa cap canvi al vectors als quals opera. Cal notar també que per qualsevol matriu $A$ i la seva inversa és veritat que $AA^{-1} = I$
	
	\item \textbf{Operador Unitari} \\
	Un operador unitari es qualsevol operador que no altera la norma dels vectors al quals es aplicat, per tant, una matriu es unitària si $AA^\dagger = I$
	Per convertir qualsevol operador en unitari, es divideix les seves entrades entre la norma del operador. 
	
	\item \textbf{Operadors Hermitians} \\
	Un operador Hermitià o \textit{self-adjoint operator} en anglès, es qualsevol operador que el seu conjugat Hermitià es ell mateix: $A = A^\dagger$ \\
	Una altre cosa a tenir en compte es que existeix un operador únic $A$ en un espai de Hilbert, de manera que per qualsevol vectors $\ket{\psi}$ i $\ket{\varphi}$, es compleix que:
	$$
	\bra{\psi} (A \ket{\varphi}) = (A^\dagger\bra{\psi})\ket{\varphi}
	$$ 
	Aquest operador es conegut com el \textit{adjoint} o conjugat Hermitià de $A$.
	
\end{enumerate}

\section{Producte Interior i Producte Exterior}

\subsection{Producte Interior}
Un vector dual $\bra{\psi}$ i un vector $\ket{\varphi}$ combinats formen el producte interior $\bra{\psi}\ket{\varphi}$, el qual efectua una operació que agafa els dos vectors com a input i produeix un nombre complex com a output:
$$
\bra{a}\ket{b} = a_1 b_1 + a_2 b_2 + ... + a_{n-1} b_{n-1} + a_n b_n = z
$$
Amb $z, a_i, b_i \in \mathbb{C}$. Quan hem refereixo a un producte interior, normalment diré "el producte interior de dos vectors", quan en realitat es una operació entre un vector dual i un vector.

El equivalent d'aquest producte en un espai real de dos dimensions $\mathbb{R}^2$ es el producte escalar, que es expressat com a :
\begin{equation}
	\bra{a}\ket{b} = \norm{\ket{a}}_2 \cdot \norm{\ket{b}}_2 \cos \theta 
	\label{eq:dot_product}
\end{equation}


Amb $\norm{\cdot}_2$ sent la norma $\ell^2$  definida com a $\norm{\ket{\psi}}_2 = \sqrt{\psi_{1}^2 + \cdots + \psi_{n}^2}$ amb $\theta$ sent l'angle entre els vectors $\ket{a}$ i $\ket{b}$. Com he dit l'equació \eqref{eq:dot_product} és equivalent al producte interior, no obstant, segons el que he vist, no es usada àmpliament ja que interpretar $\theta$ com un angle entre vectors de dimensions altes no té molt de sentit. En contrast, he vist aquest producte presentat en la seva interpretació geomètrica\footnote{Els detalls exactes de l'interpretació geomètrica estan fora del domini d'aquest treball, malgrat que m'agradaria molt parlar sobre el tema.} com el producte entre un vector fila i un vector columna: 
% The Geometry of Linear Equations: https://www.youtube.com/watch?v=ZK3O402wf1c&list=PL3CCC70370BD3FD48&index=1
% The four Fundamental Subspaces: https://www.youtube.com/watch?v=nHlE7EgJFds
$$
\bra{a}\ket{b} = \begin{bmatrix}a_1  \cdots  a_n \end{bmatrix} \begin{bmatrix} b_{1} \\ \vdots \\ b_{n} \end{bmatrix}
$$

Ja he definit la norma 
I have already defined the $\ell_2$ norm as the square root of the sum of the squared entries of a vector: 
$$
\norm{\ket{a}}_2 = \sqrt{\sum_{i} \abs{a_{i}}^2}
$$
Nonetheless, the more common definition is based upon the inner product. As you can see the inner product of a vector by itself is the sum of the squared entries: 
$$
\bra{a}\ket{a} = a_1 a_1 + \cdots + a_n a_n = a_1^2 + \cdots +  a_n^2 \\= \sum_{i} a_i^2
$$
Thus, the norm can be defined as the square root of the inner product of a vector:
\begin{equation}
\norm{\ket{a}}_2 = \sqrt{\bra{a}\ket{a}}
\label{eq:norm}
\end{equation}
When the norm is applied to a 2 dimensional vector you can see that is the same as the length of that vector, that is because norm and length are the same concepts, however, the norm is the generalized length that can be applied to a vector of any dimension. 


From what I understand some properties of the length of a two dimensional vector do not hold with the norm of a vector that has more than 2 dimensions. In other words, the norm behaves in similar ways like the distance from the origin (which is the length), thus they are not the exact same thing. Moreover, there are different types of norm\footnote{But not different types of length, that I know of at least.} that are used in different types of scenarios. That is why I am referring to a $\ell^2$ norm, a specific type of norm that is also named Euclidean norm which is used to define the $\ell^2$ distance or Euclidean distance, widely used to measure the distance of two points in a 2D space or a 3D space in high school \cite{wolfram:2norm}.

\subsubsection{Properties of the Inner Product}
The basic properties of the inner product are as follows:
\begin{enumerate}
	\item Is linear in the second argument $ (z_1\bra{a}+ z_2\bra{c})\ket{b} = z_1\bra{a}\ket{b} + z_2\bra{c}\ket{b}$
	\item Conjugate symmetry $\bra{a}\ket{b} = (\bra{b}\ket{a})^*$
	\item $\bra{a}\ket{a}$ is non-negative and real, except in the case of $\bra{a}\ket{a} = 0 \Leftrightarrow \ket{a} = 0$
\end{enumerate}

 
\subsection{Orthonormal and orthogonal vectors}
From the concept of norm comes the concepts a pair of orthogonal vectors and a pair of orthonormal vectors\footnote{Funny note, when I encountered these two terms for the first time in QC and QI \tocite, I taught they were the same thing and a week passed until I realized. It was such a difficult mess to understand everything else with these two terms confused.}. \\
Looking at the equation \eqref{eq:norm} we can see that if the inner product of a vector is one, the norm of this vector is also one. A vector that has norm one is named a unit vector. Therefore, if the inner product of a vector is one, that vector is a unit vector.

A pair of non-zero vectors are orthogonal if their inner product is zero. For two non-zero 2 dimensional vectors, if their inner product is equal to zero, you can see that they are perpendicular to each other by looking at equation \eqref{eq:dot_product}:
\begin{multline*}
	 \text{For }\ket{a} \text{and} \ket{b}\neq 0 : \\
	\text{If }\bra{a}\ket{b} = 0 \text{ then: } \norm{\ket{a}}_2 \cdot \norm{\ket{b}}_2 \cos \theta = 0 \\
	 \text{Because $\ket{a}$ and $\ket{b}$ are non-zero vectors, their norms can't be zero.} \\
	 \text{Thus the remainder term } \cos\theta \text{ is equal to zero.} \\
	 \text{Therefore, the angle $\theta$ as to be } \frac{\pi}{2}. \\
\end{multline*}
However, thinking that perpendicularity and orthogonality are the same concepts is a mistake, since, it only holds when looking at 2 dimensional vectors. As with norm and length, orthogonality is the generalized concept of perpendicularity that works for high dimensional vectors.

When we mix the concepts of unit vector and orthogonal vectors we arrive at the term orthonormality \cite{QCandQI:GramSchmidt}. A pair of non-zero vectors are orthonormal when both are unit vectors and there are orthogonal to each other: 
$$
\ket{a} \text{and } \ket{b} \text{ are othornormal if} \left\{
	\begin{array}{ll}
		\bra{a}\ket{b} = 0 \\
		\bra{a}\ket{a} = 1 \\
		\bra{b}\ket{b} = 1 \\
	\end{array}
\right.
$$
Orthonormal vectors are important, they are broadly used in quantum computation as well as quantum mechanics because they form the basis for the vector spaces on which the quantum states are located. \todo{maybe introduce them before if you talk about basis later on} \\
One thing to point out is that I have been talking about a pair of vectors when referring to orthonormal vectors, however, orthonormality can be extended to a set of vectors. If a set has all unit vectors and the vectors are orthogonal to each other, the set is orthonormal. The set of vectors $ B = \{\ket{\beta_1}, \ket{\beta_2}, ..., \ket{\beta_{n-1}} \ket{\beta_n}\} $ is
orthonormal if $\bra{\beta_i}\ket{\beta_i} = \delta_{ij}  \forall i, j$ \cite{QCandQI:GramSchmidt} where $\delta_{ij}$ is the Kronecker delta defined as 
% a gentle intro to QC
: 
$$
\delta_{ij} =
\begin{cases}
	1 & \text{if } i=j\\
	0 & \text{if } i\neq j
\end{cases}
$$

\subsection{Outer product}
The outer product is a function that takes two vectors -expressed as $\ket{a}\bra{b}$, with $\ket{a}$ and $\ket{b}$ being vectors- and produces an linear operator as output. Unlike the inner product, there is no analog for the outer product on the mathematics taught in high school\footnote{The analog of the inner product would be the dot product.}, and it is a bit difficult to understand it as it can take two vectors from different spaces as input. It is defined as follows:


%For a vector $\ket{v}$ and $\ket{v'}$ in the Hilbert space $V$ and a vector $\ket{w}$ in the Hilbert space $W$. The output is a linear operator of dimensions $m \times n$ in the space $M_{m\times n}$:
%$$\ket{v}\bra{w} : V,W \mapsto M_{m\times n} \\ $$
For a vector $\ket{v}$ and $\ket{v'}$ of dimensions $m$ and a vector $\ket{w}$ of dimension $n$. The output is a linear operator $A$ of dimensions $m \times n$ in the space $M_{m\times n}$:
$$
\ket{v}\bra{w} = A \text{ with } A\in \mathrm{Mat}_{m\times n} .
$$ 
%In other words, that takes a vector from $\mathbb{C}_i$ to $\mathbb{C}_j$. 
Whose action is defined by: 
\begin{equation}
	(\ket{v}\bra{w})\ket{v'} \equiv \ket{w}\bra{v}\ket{v'} = \bra{v}\ket{v'}\ket{w}
	\label{eq:outer_def}
\end{equation}


From equation \eqref{eq:outer_def} the usefulness and meaning of the outer product are hard to comprehend, so I will look at the way to compute it next to clarify how it works. For two vectors $\ket{a}$ and $\ket{b}$ of dimensions $m$ and $n$ respectively, their outer product is computed multiplying each element of $\ket{a}$ by each element of $\ket{b}$ forming a matrix of size $ m \times n$:
$$
\ket{a}\bra{b} = \begin{bmatrix}
	a_1 b_1 & a_1 b_2 & \cdots & a_1 b_n \\
	a_2 b_1 & a_2 b_2 & \cdots & a_2 b_n \\
	\vdots  & \vdots  & \ddots& \vdots  \\
	a_m b_1 & a_m b_2 & \cdots & a_m b_n \\
\end{bmatrix}
$$
The usefulness of the outer product will be shown in future sections.

\section{Tensor product}
The last product to mention is the tensor product, represented with the symbol $\otimes$. This product is used to create larger vector spaces by combining smaller vector spaces. The formal explanation of this concept is quite difficult, so I will focus on explaining the way to compute it by using the matrix representation of this product, named the Kronecker product. 

For a $m \times n$ matrix  $A$ and a $p \times q$ matrix  $B$ the output of their Kronecker product \cite{QCandQI:kronecker} is a $pm \times qn$ matrix:
\begin{multline*}
A \otimes B = \begin{bmatrix}
	a_{11} B & a_{12} B & \cdots & a_{1n}B \\
	a_{21} B & a_{22} B & \cdots & a_{2n}B \\
	\vdots & \vdots & \ddots &           \vdots \\
	a_{m1} B & a_{m2} B & \cdots & a_{mn} B
\end{bmatrix} \\
= \begin{bmatrix}
a_{11} b_{11} & a_{11} b_{12} & \cdots & a_{11} b_{1q} &
\cdots & \cdots & a_{1n} b_{11} & a_{1n} b_{12} & \cdots & a_{1n} b_{1q} \\
a_{11} b_{21} & a_{11} b_{22} & \cdots & a_{11} b_{2q} &
\cdots & \cdots & a_{1n} b_{21} & a_{1n} b_{22} & \cdots & a_{1n} b_{2q} \\
\vdots & \vdots & \ddots & \vdots & & & \vdots & \vdots & \ddots & \vdots \\
a_{11} b_{p1} & a_{11} b_{p2} & \cdots & a_{11} b_{pq} &
\cdots & \cdots & a_{1n} b_{p1} & a_{1n} b_{p2} & \cdots & a_{1n} b_{pq} \\
\vdots & \vdots & & \vdots & \ddots & & \vdots & \vdots & & \vdots \\
\vdots & \vdots & & \vdots & & \ddots & \vdots & \vdots & & \vdots \\
a_{m1} b_{11} & a_{m1} b_{12} & \cdots & a_{m1} b_{1q} &
\cdots & \cdots & a_{mn} b_{11} & a_{mn} b_{12} & \cdots & a_{mn} b_{1q} \\
a_{m1} b_{21} & a_{m1} b_{22} & \cdots & a_{m1} b_{2q} &
\cdots & \cdots & a_{mn} b_{21} & a_{mn} b_{22} & \cdots & a_{mn} b_{2q} \\
\vdots & \vdots & \ddots & \vdots & & & \vdots & \vdots & \ddots & \vdots \\
a_{m1} b_{p1} & a_{m1} b_{p2} & \cdots & a_{m1} b_{pq} &
\cdots & \cdots & a_{mn} b_{p1} & a_{mn} b_{p2} & \cdots & a_{mn} b_{pq}
\end{bmatrix}
\end{multline*}

Note that $a_{ij}B$ is a scalar multiplication by a matrix, with $a_{ij}$ being the scalar and $B$ being the matrix.

Here is a clearer example with two $2\times2$ matrices, note that each entry of the first matrix is multiplied by the second matrix:
\begin{equation*}
\begin{bmatrix}
	1 & 2 \\
	3 & 4 \\
\end{bmatrix} \otimes
\begin{bmatrix}
	0 & 5 \\
	6 & 7 \\
\end{bmatrix} =
\begin{bmatrix}
1 \begin{bmatrix}
	0 & 5 \\
	6 & 7 \\
\end{bmatrix} & 
2 \begin{bmatrix}
	0 & 5 \\
	6 & 7 \\
\end{bmatrix} \vspace{6pt} \\ 

3 \begin{bmatrix}
	0 & 5 \\
	6 & 7 \\
\end{bmatrix} & 
4 \begin{bmatrix}
	0 & 5 \\
	6 & 7 \\
\end{bmatrix}
\end{bmatrix}
\end{equation*}

\begin{equation*}
	= 
\begin{bmatrix}
1\times 0 & 1\times 5 & 2\times 0 & 2\times 5 \\
1\times 6 & 1\times 7 & 2\times 6 & 2\times 7 \\
3\times 0 & 3\times 5 & 4\times 0 & 4\times 5 \\
3\times 6 & 3\times 7 & 4\times 6 & 4\times 7 \\
\end{bmatrix} \\
= 
\begin{bmatrix}
0 &  5 &  0 & 10 \\
6 &  7 & 12 & 14 \\
0 & 15 &  0 & 20 \\
18 & 21 & 24 & 28
\end{bmatrix}
\end{equation*}

One important piece of notation to take into account is $\bigotimes$, used to represent the equivalent of the sum (noted with $\sum$), but instead of addition the Kronecker product is used. In other words $\bigotimes$ denotes the Kronecker product of a finite number of terms. To clarify here's an example with the identity matrix:


With $\mathbb{I}$ as the matrix $\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$ and $n$ as a power of $2$:
$$
\mathbb{I}_n = \bigotimes^{\log_{2} n} \mathbb{I}  
$$
	
Here is the case for $n=8$: 
$$\mathbb{I}_8 =  \bigotimes^{\log_{2} 8} \mathbb{I} =\bigotimes^3 \mathbb{I} = \mathbb{I} \otimes \mathbb{I} \otimes \mathbb{I} =
	\begin{bmatrix}
		1 &0 &0 &0 &0 &0 &0 &0 \\
		0 &1 &0 &0 &0 &0 &0 &0 \\
		0 &0 &1 &0 &0 &0 &0 &0 \\
		0 &0 &0 &1 &0 &0 &0 &0 \\
		0 &0 &0 &0 &1 &0 &0 &0 \\
		0 &0 &0 &0 &0 &1 &0 &0 \\
		0 &0 &0 &0 &0 &0 &1 &0 \\
		0 &0 &0 &0 &0 &0 &0 &1 \\
	\end{bmatrix} 
$$


The Kronecker product also works for vectors in the same way, with a scalar-vector multiplication:

For the vectors $\ket{\psi}$ and $\ket{\varphi}$ of dimensions $n$ and $m$ respectively:
$$
\ket{\psi} \otimes \ket{\varphi} = \begin{bmatrix}
	\psi_{1} \ket{\varphi} \\
	\psi_2 \ket{\varphi} \\
	\vdots \\
	\psi_{m} \ket{\varphi}
\end{bmatrix} = 
\begin{bmatrix}
\psi_{1} \varphi_1 \\
\psi_{1} \varphi_{2} \\
\vdots \\
\psi_{1} \varphi_{m} \\
\vdots \\
\vdots \\
\psi_{n} \varphi_1 \\
\psi_{n} \varphi_{2} \\
\vdots \\
\psi_{n} \varphi_{m} \\
\end{bmatrix}
$$

Note that the Kronecker product can also be taken between a vector and a matrix or vise-versa, however this form isn't as common as the other two.
%With $V$ and $W$ being vector spaces of dimension $m$ and $n$ respectively, the tensor product between them $V \otimes W$ is a vector spaces of dimension $mn$. The elements on the space $V \otimes W$ are 
%For the orthonormal basis $\ket{i}$ and $\ket{j}$ of $V$ and $W$ respectively the orthonormal basis for $V\otimes W$ would be $\ket{i}\otimes\ket{j}$.


\subsection{Properties of the Tensor Product}
The basic properties of the tensor product are as follows \cite{QCandQI:tensor_product, wiki:tensor_product}:
\begin{enumerate}
	\item Associativity: \\ $A \otimes (B + C) = A \otimes B + A \otimes C$ \\
	$ (zA) \otimes B = A \otimes (zB) = z(A \otimes B)$\\
	$ (A \otimes B) \otimes C = A \otimes (B\otimes C)$ \\
	$A \otimes 0 = 0 \otimes A = 0 $
	\item Non-commutative \footnote{One cool thing is that $A \otimes B$ and $B \otimes A$ are permutation equivalent: \\ $\exists P, Q \Rightarrow A \otimes B = P (  B \otimes A )Q$ where $P$ and $Q$ are permutation matrices.  }: \\
	$A \otimes B \neq B \otimes A $
\end{enumerate}



%$$
%f: \mathbb{R} \rightarrow \mathbb{R} \text{ or } \mathbb{R} \stackrel{f}{\to} \mathbb{R} 
%$$
%Where $\mathbb{R}$ is the space of all real number, and $\rightarrow$ is 


\section{Trace}
The trace of a matrix is just the sum of the elements on the main diagonal, the one that goes from top to bottom and left to right.

Here's a matrix $A$ with its main diagonal marked:
$$A = 
	\begin{bmatrix}
		\tikzmark{top}{1} & \tikzmark{top2}{1} & 0 \\
		0 & 1 & \tikzmark{bottom2}{1} \\
		\tikzmark{end}{1} & 0 & \tikzmark{bottom}{1}\\
	\end{bmatrix}
$$
\begin{tikzpicture}[overlay,remember picture]
	\draw[opacity=.4,line width=3mm,line cap=round] (top.center) -- (bottom.center);
\end{tikzpicture}

And its trace, denoted by $\Tr[A]$ is:
$$
\Tr[A] = 1 + 1 + 1 = 3
$$
More formally, the trace of a $n$-dimensional squared matrix $A$ is:
$$
\Tr[A] = \sum_{i=1}^{n} a_{ii} = a_{11} + a_{22} + \cdots + a_{nn}
$$


The trace of a matrix as the following properties:
\begin{enumerate}
	\item Linear operator: \\
	Because the trace is a linear mapping, it follows that:\\
	$\Tr[A+B] = \Tr[A] + \Tr[B]$ and $\Tr[zA] = z \Tr[A]$, for all squared matrices $A$ and $B$ and all scalars $z$.
	\item Trace of a Kronecker product: \\
	$\Tr[A \otimes B] = \Tr[A]\Tr[B]$
	\item Transpose has the same trace: \\
	$\Tr[A]=\Tr[A^T]$
	\item Trace of a product is cyclic: \\
	For a $m \times n$ matrix $A$ and a $n \times m$ matrix $B$:\\
	$\Tr[AB]=\Tr[BA]$
\end{enumerate}

One very useful way to compute the trace of an operator is through the Gram-Schmidt procedure\footnote{See \ref{gram} for the definition of the Gram-Schmidt procedure.} and an outer product. 
Using Gram-Schmidt to represent the unit vector $\ket{\psi}$ with an orthonormal basis $\ket{i}$ which includes $\ket{\psi}$ as the first element, is true that:
$$
\Tr[A\ket{\psi}\bra{\psi}] = \sum_i \bra{i}A\ket{\psi}\bra{\psi}\ket{i} = \bra{\psi}A\ket{\psi}
$$

\chapter{Quantum Computation}
After some amount of math theory, it is time to start talking about quantum mechanics, in this chapter I am going to introduce the basic concepts of quantum computation and quantum information.

Quantum mechanics is a mathematical framework or rather a set of theories used to describe and explain the physical properties of atoms, molecules, and subatomic particles. It is the framework of all quantum physics including quantum information science. The right way of presenting quantum computation is through the formal quantum mechanics postulates because, with them, the statements made in quantum computation do not seem to come from anywhere \cite{QCandQI:QM_postulates}. However, to not complicate this section more than it is, I will do my best to explain the concepts and math of quantum computing just on their own, without presenting more generalized concepts from quantum mechanics, unless it is totally necessary to do so.

\section{Quantum State and Superpositions}
Per descriure com evolucionen els sistemes físics a través del temps, es necessita representar els sistemes d'alguna manera. En computació quàntica és representen per estats quàntics, els quals són algun tipus de distribucions de probabilitat per els possibles resultats d'una mesura on un sistema quàntic \cite{QT_concepts:q_systems}.

Imaginat que tens un boli, però que no saps de quin color és, no obstant, saps que pot ser vermell o blau. Per esbrinar de quin color és, pots provar a escriure amb el boli per veure el color de la pinta, o en altres paraules, fer una mesura. Saps que hi ha un 50\% de probabilitat de que sigui vermell i un 50\% de que sigui blau. En aquesta situació hipotètica tindries el teu sistema quàntic (el boli), una manera de mesurar-lo (escriure alguna cosa) i una llista amb els possibles resultats (50\% vermell, 50\% blau), només et falta una manera de representar-lo tot matemàticament, el estat quàntic. Per tant, per què no intentem guardar la informació que sabem del boli en un vector?

Si posem cada probabilitat de treure un resultat en un vector tenim que:
$$
\begin{bmatrix}
	0.5 \\
	0.5
\end{bmatrix}
$$
On la primera entrada és la possibilitat de que el boli sigui vermell i la segona entrada de que sigui blau, per fer-ho més senzill aquí està en colors:
$$
\begin{bmatrix}
	\textcolor{red}{0.5} \\
	\textcolor{blue}{0.5}
\end{bmatrix}
$$
Cal remarcar que aquest vector està normalitzat amb la norma $\ell_1$, definida com la suma de les entrades d'un vector\footnote{Amb $\ket{a}$ sent un vector, la norma $\ell_1$, denotada per $\norm{\cdot}_1$ és $\norm{\ket{a}}_1 = \sum_i a_i$.}, en altres paraules la norma $\ell_1$ d'aquest vector és $1$.

Llavors, hem d'escollir una operació matemàtica per poder extreure la informació del vector, com que el output ha de ser un número, podem provar a utilitzar un producte interior. Però primer s'ha de representar el vector com una combinació lineal de les seves bases:
$$
\textcolor{red}{0.5}\ket{0} + \textcolor{blue}{0.5}\ket{1} = \begin{bmatrix}
	\textcolor{red}{0.5} \\
	\textcolor{blue}{0.5}
\end{bmatrix}
$$
On $\ket{0}$ és el vector $\begin{bmatrix} 1 \\ 0\end{bmatrix}$, i, $\ket{1}$ és el vector $\begin{bmatrix} 0 \\ 1 \end{bmatrix}$.
Per tant, per trobar la probabilitat, s'agafa el producte interior del vector amb la base corresponent a la probabilitat, com a continuació:
$$
\bra{0}\ket{v} =
\begin{bmatrix}
	1 &
	0
\end{bmatrix}
\begin{bmatrix}
	\textcolor{red}{0.5} \\
	\textcolor{blue}{0.5}
\end{bmatrix}
= \textcolor{red}{0.5}
$$
$$
\bra{1}\ket{v} =
	\begin{bmatrix}
		0 &
		1
	\end{bmatrix}
	\begin{bmatrix}
		\textcolor{red}{0.5} \\
		\textcolor{blue}{0.5}
	\end{bmatrix}
	= \textcolor{blue}{0.5}
$$
Aquest procediment és bastant senzill, com a un altre exemple, per representar un boli amb 6 colors possibles amb una possibilitat aleatòria d'escriure amb un color del 6 possibles, l'estat d'aquest boli és\footnote{Le posat colors a els elements per claredat.}:
$$
\ket{w} =
\begin{bmatrix}
	\textcolor{red}{0.25} \\
	\textcolor{blue}{0.3 }\\
	\textcolor{green}{0.1 }\\
	\textcolor{magenta}{0.1 }\\
	\textcolor{black}{0.1 }\\
	\textcolor{brown}{0.05}
\end{bmatrix}
$$
Ara veure la probabilitat de per exemple treure el color verd, utilitzant la tercera base, la que correspon al color verd:
$$
\begin{bmatrix}
	0 & 0 & 1 & 0 & 0 & 0\\
\end{bmatrix}
\begin{bmatrix}
	\textcolor{red}{0.25} \\
	\textcolor{blue}{0.3 }\\
	\textcolor{green}{0.1 }\\
	\textcolor{magenta}{0.1 }\\
	\textcolor{black}{0.1 }\\
	\textcolor{brown}{0.05}
\end{bmatrix} = \textcolor{green}{0.1 }
$$
Deixant els bolis a una banda, ara els podem substituir per un sistema físic quàntic, com per exemple un fotó. Els fotons tenen certes propietats que poden ser mesurades, com la seva polarització\footnote{Concretament, són una propietat que les ones transversals tenen, el tipus d'ona de les ones electromagnètiques, que són els fotons en realitat.}. Al mirar als fotons com ones en que oscil·len en el camps electromagnètic, la polarització és l'orientació geomètrica de l'ona. La polarització pot ser interpretada com un angle respecte a la direcció de propagació. \todo{posar una figura aquí}

Al definir les bases del estat de polarització com vertical i horitzontal, denotades per els vectors $\ket{\text{\textrightarrow}}$ i $\ket{\text{\textuparrow}}$, respectivament. Podem definir un estat de superposició entre les bases, representat com $\ket{\nearrow}$ \cite{QC_intro:photon}: 
$$
\ket{\nearrow} = \alpha\ket{\text{\textrightarrow}} + \beta\ket{\text{\textuparrow}}
$$
On $\alpha$ i $\beta$ són números complexos. 
Un estat en superposició es simplement un estat on l'angle de polarització no és $0$ ni $\frac{\pi}{2}$. No ens tenim que preocupar de la descripció matemàtica exacte de la polarització dels fotons al parlar de computació quàntica perquè el que importa és l'informació que porten aquests estat, no la física dels sistemes que representen. Aquesta informació s'ha d'agafar mitjançant mesures, com amb els bolis. Aquestes mesures en el cas de la polarització dels fotons seria passar-los per diversos filtres de polarització, els quals deixen passar el fotó o l'absorbeixen, tot això d'una manera probabilística, es a dir, dependent de quin sigui l'estat tenen certa possibilitat de ser absorbits o no.

Per clarificar, el filtre lo que fa es col·lapsar el fotó en els dos possibles estats de polarització, l'estat en el quan el filtre es orientat o l'estat perpendicular a aquest. Si col·lapsa en l'estat del filtre el fotó passa pel el filtre, en cas de col·lapsar en l'altre estat, es absorbit. La manera en la que els fotons col·lapsen és probabilística, si agafen un filtre que està orientat horitzontalment respecte al fotons que li arriben, un fotó orientat en horitzontalment té un 100\% de poder passar, mentre que un fotó polaritzat verticalment té un 0\% de probabilitat de poder passar. I si un fotó està polaritzat en un angle just entre vertical i horitzontal, es a dir a 45º, tindrà un 50\% de possibilitats de passar i un 50\% de no poder-hi. 

Aquesta és la manera amb la qual els sistemes quàntics es comporten, a través de la probabilitat, on els estats que els representen tenen la informació sobre quines són aquestes possibilitats. 
No obstant, la, polarització dels fotons són un sistema físic concret, quan es parla de computació quàntica és millor treballar amb conceptes més generals per poder expressar tants algoritmes com sigui possible i poder implementar aquests algoritmes en tants ordinadors quàntics com sigui possible. Per saqueta raó, en la branca de la informació quàntica i la computació quàntica es treballa amb qubits, en comptes de diversos sistemes físics. 

\section{Qubits}
Ordinadors modern representen informació a través de cadenes de zeros i uns, anomenats bits. Tot, des de imatges a lletres o instruccions electròniques. Per exemple, la lletra t és representada per la cadena de bits $01110100$, codificat a través de codi binari. Tot el que fas en un ordinador es codifica i representa en codi binari. 

Degut a que estem molt acostumats a codi binari, en el camp de la computació quàntica també s'utilitza, no obstant, en comptes de bits s'utilitzen qubits. Un qubit es l'anàleg d'un bit, en altres paraules, és la unitat mínima d'informació utilitzada pels ordinadors quàntics. En els qubits podem aplicar propietats quàntiques com la superposició o l'entrellaçament\footnote{Ja he parlat de la primera amb la polarització del fotons, del entrellaçament parlaré més endavant.}. Si un bit pot estar en l'estat $1$ o en l'estat $1$, un qubit pot estar en una combinació d'aquests estats, en un estat enmig del $1$ o el $0$. És una combinació lienal dels vectors que representen aquests estats, $\ket{0}$ i $\ket{1}$:
$$
\ket{\psi} = \alpha\ket{0} + \beta\ket{1}
$$
On $\alpha$ i $\beta$ són nombres complexos i $\ket{\psi}$ és un vector en un bidimensional espai de Hilbert\footnote{Un espai vectorial amb un producte interior.}. Els vectors $\ket{0}$ i $\ket{1}$ són anomenats els vectors de la base computacionals, representats com:
$$
\ket{0} = \begin{bmatrix}
	1 \\ 0
\end{bmatrix} \;
\ket{1} = \begin{bmatrix}
	0 \\ 1
\end{bmatrix}
$$
Per tant el vector $\ket{\psi}$ és:
$$
\ket{\psi} 
= \alpha 
\begin{bmatrix}
	1 \\ 0
\end{bmatrix} + \beta 
\begin{bmatrix}
0 \\ 1
\end{bmatrix}= 
\begin{bmatrix}
	\alpha \\ \beta
\end{bmatrix}
$$
Aquest vector és un estat quàntic vàlid per representar un qubit, no obstant, hi ha un important factor a tenir en compte. El vector ha d'estar normalitzat amb la norma $\ell_2$, per tant, els nombres $\alpha$ i $\beta$ no poden ser qualsevol nombre, necessiten ser els coeficients que formin un vector amb una norma de $1$.   
$$
\norm{\ket{\psi}} = 1 
$$
Llavors:
\begin{align*}
	\norm{\psi} &= \sqrt{\abs{\alpha}^2 + \abs{\beta}^2} = 1 \\
	&\Rightarrow \abs{\alpha}^2 + \abs{\beta}^2 = 1
\end{align*}

Definir un qubit com una "combinació lienal dels estats fundamentals" no és de molta ajuda, per això, elaboraré més sobre aquesta definició: 
Una cadena de $n$-bit pot només representar una única combinació d'uns i zeros, mentres que una cadena de $n$-qubits representa una combinació de totes les possibles combinacions. En el cas d'un qubit, aquest és una combinació del possible estats $\ket{0}$ i $\ket{1}$. Considera un qubit com una barreja dels estats possibles, amb cada coeficient de la combinació lineal sent el nombre que indica quant d'un estat forma part de la barreja. 

Una cosa molt interessant passa quan s'augmenta el nombre qubits, la 'quantitat d'informació' creix exponencialment. Quan passes d'una cadena de $7$ qubits a una cadena de $8$ qubits la quantitat d'informació creix per només un número. Però per una cadena de $n$ qubits la quantitat d'informació que té, en altres paraules la quantitat de números que representa és $2^n$, on aquests números son els coeficients de la combinació lineal. Això es perquè quan afegeixes un qubit el nombre de combinació creix exponencialment, per tant es necessiten més coeficients per representar aquestes combinacions en la combinació lineal. El qubits al ser una combinació de possibles estats necessiten molta més informació per poder representar-los\footnote{Informació que es manifesta amb els coeficients de la combinació lineal}, no com els bits que al ser només una combinació és necessita només saber quina combinació és. No passa res si això no s'entén perfectament, lo important és saber que es necessiten $2^n$ números complexos\footnote{Són complexos degut a que els coeficients de la combinació lienal són números complexos.} per representar $n$-qubits i que es necessiten $n$ números binaris per representar $n$-bits.

Per il·lustrar tot això, 2 qubits es representen amb el vector:
\begin{align*}
	\ket{\psi} &= \alpha_1\ket{00} + \alpha_2\ket{01} + \alpha_3\ket{10} + \alpha_4\ket{11}\\
			&= \alpha_1\begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \end{bmatrix} + \alpha_2 \begin{bmatrix} 0 \\ 1 \\ 0 \\ 0 \end{bmatrix} + \alpha_3 \begin{bmatrix} 0 \\ 0 \\ 1 \\ 0 \end{bmatrix} + \alpha_4\begin{bmatrix} 0 \\ 0 \\ 0 \\ 1 \end{bmatrix} 
			= \begin{bmatrix} \alpha_1 \\ \alpha_2 \\ \alpha_3 \\ \alpha_4 \end{bmatrix}
\end{align*}
Cal remarcar que els vectors de la base computacional serien les columnes d'una matriu identitat amb dimensions $2^n\times 2^n$, on $n$ és el nombre de qubits. 



%The quantity of information that a qubit has is the information of all possible combination that the qubit has. One qubit has two possible combination $\ket{0}$ and $\ket{1}$, meanwhile two qubits have four possible combinations $\ket{00}, \ket{01}, \ket{10}, \ket{11}$, and four qubits have $2^4$ possible combinations, that is a total of $16$. And the information about each combination is a complex number, represent in the examples above has $\alpha_i$. The complex numbers are used to specify the quantum superposition that the system has. 



\section{Mesurament quàntic}
Tornant als fotons i la seva polarització, per prediu-re en quina base col·lapsa, necessitem utilitzar els mesuraments quàntics. Aquests mesuraments ens donen la probabilitat que té un fotó de col·lapsar en una certa base. 
On el vector $\ket{\text{\textrightarrow}}$ expressa la polarització d'un filtre i $\ket{\text{\textuparrow}}$ és el seu complement perpendicular, un fotó polaritzat té el estat següent \cite{QC_intro:photon}: 
$$
\ket{\nearrow} = \alpha\ket{\text{\textrightarrow}} + \beta\ket{\text{\textuparrow}}
$$
La probabilitat de ser absorbit és $\alpha^2$, cal notar que $\alpha$ és el coeficient que correspon a la base $\ket{\text{\textrightarrow}}$, la base en la qual el filtre es orientat. En altres paraules, $\alpha^2$ indica la probabilitat del col·lapse de la polarització en l'estat $\ket{\text{\textrightarrow}}$. 

Els coeficients $\alpha$ i $\beta$ poden ser expressats com una funció del angle $\theta$\footnote{Per més informació sobre la polarització dels fotons, veure l'apèndix \ref{appendix:optics}.}
\begin{equation}
	\ket{\nearrow} = \cos\theta\ket{\text{\textrightarrow}} + \sin\theta\ket{\text{\textuparrow}}
	\label{eq:photon_state}
\end{equation}
I les probabilitats expectades es converteixen en $\cos^2\theta$ i $\sin^2\theta$, respectivament \cite{QC_intro:photon}. Ara podem mirar que els coeficients són la probabilitat, al veure el cas per $\theta = \pi/2$:
\begin{align*}
	p(\ket{\text{\textrightarrow}}) &= \cos^2\theta 
	= \cos^2 \frac{\pi}{2}
	= \left( \frac{1}{\sqrt{2}} \right) ^2 \\
	&= 0.5 \\
	p(\ket{\text{\textuparrow}}) &= \sin^2\theta 
	= \sin^2 \frac{\pi}{2}
	= \left( \frac{1}{\sqrt{2}} \right) ^2 \\
	&= 0.5 \\	
\end{align*} 
